{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awjans/CopilotForPRsAdoption/blob/main/scripts/AIDev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ03WzKRQ1bS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b7c674"
      },
      "source": [
        "**First**, We need to define the URLs of the AIDev Parquet Files that we are intersted in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de61a39b"
      },
      "source": [
        "pull_request_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pull_request.parquet'\n",
        "pr_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_comments.parquet'\n",
        "pr_commits_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commits.parquet'\n",
        "pr_commit_details_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commit_details.parquet'\n",
        "pr_reviews_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_reviews.parquet'\n",
        "pr_review_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_review_comments.parquet'\n",
        "pr_task_type_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_task_type.parquet'\n",
        "repository_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/repository.parquet'\n",
        "user_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/user.parquet'\n",
        "\n",
        "\"\"\"\n",
        "Load the Parquet file into a Pandas DataFrame from the file URL.\n",
        "\"\"\"\n",
        "def load_data(url):\n",
        "  try:\n",
        "    # For Parquet files:\n",
        "    df = pd.read_parquet(url)\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading data: {e}\")\n",
        "      print(\"Please ensure the URL is correct and the file is publicly accessible.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second**, We need to load the data from the URLs"
      ],
      "metadata": {
        "id": "CKy9xnAeb1p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pull_request = load_data(pull_request_file_url)\n",
        "pr_comments = load_data(pr_comments_file_url)\n",
        "pr_commits = load_data(pr_commits_file_url)\n",
        "pr_commit_details = load_data(pr_commit_details_file_url)\n",
        "pr_reviews = load_data(pr_reviews_file_url)\n",
        "pr_review_comments = load_data(pr_review_comments_file_url)\n",
        "pr_task_type = load_data(pr_task_type_file_url)\n",
        "repository = load_data(repository_file_url)\n",
        "user = load_data(user_file_url)"
      ],
      "metadata": {
        "id": "YAPFfpktRJkz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third**, Gather the covariant variables\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zFclpKRkNfSv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a1b0d13",
        "outputId": "7c6c5d6b-9424-453d-a837-6081169ad2f6"
      },
      "source": [
        "max_created_at = pull_request['created_at'].max()\n",
        "min_created_at = pull_request['created_at'].min()\n",
        "\n",
        "print(f\"Maximum created_at: {max_created_at}\")\n",
        "print(f\"Minimum created_at: {min_created_at}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum created_at: 2025-07-30T19:36:13Z\n",
            "Minimum created_at: 2024-12-24T00:23:09Z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of Covariants\n",
        "\n",
        "## PR Variables\n",
        "1. **Number of added lines (num_added_LOC):** The # of added LOC by a PR\n",
        "2. **Number of deleted lines (num_deleted_LOC):** The # of deleted LOC by a PR\n",
        "3. **PR size (PR_size):** The total nunber of added and deleted LOC by a PR (additions + deletions)\n",
        "4. **Purpose (PR_purpose):** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', …).\n",
        "5. **Number of files (num_files_changed):** The # of files changed by a PR\n",
        "6. **Number of commits (num_commits):** The # of commits involved in a PR\n",
        "7. **Description length:** The length of a PR description\n",
        "8. **PR author experience (PR_author_experience):** The # of prior PRs that were submitted by the PR author (author’s prior PR count). Query the author’s PR history in the same repo and count PRs created before the current one.\n",
        "9. **Is member (is_member):** Whether or not the author is a member or outside collaborator\n",
        "10. **Number of comments (num_comments):** The # of comments left on a PR\n",
        "11. **Number of author comments (num_author_comments):** The # of comments left by the PR author\n",
        "12. **Number of reviewer comments (num_reviewer_comments):** The # of comments left by the reviewers who participate in the disucssion\n",
        "13. **Number of reviewers (num_reviewers):** The # of developers who participate in the discussion.\n",
        "14. **Repo age (repo_age):** Time interval between the repository creation time and PR creation time in days.\n",
        "\n",
        "## Project variables\n",
        "\n",
        "15. **Language (language):** The repository language that a PR belongs to, represented by the top 10 or others\n",
        "16. **Number of forks (num_forks):** The # of forks that a repository has\n",
        "17. **Number of stargazers (num_stargazers):** The # of stargazers that a repository has.\n",
        "\n",
        "## Treatment variables\n",
        "\n",
        "18. **With Copilot for PRs (with_copilot_for_PRs):** Whether or not a PR is generated by Copilot for PRs (binary)\n",
        "\n",
        "## Outcome variables\n",
        "\n",
        "19. **Review time (review_time):** Time interval between the PR creation time and closed time in hours\n",
        "20. **Is merged (is_merged):** Whether or not a PR is merged (binary)"
      ],
      "metadata": {
        "id": "a30SFFhZXgy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - Number of added lines (num_added_LOC): The # of added LOC by a PR\n",
        "# 2 - Number of deleted lines (num_deleted_LOC): The # of deleted LOC by a PR\n",
        "# 3 - PR size (PR_size): The total number of added and deleted LOC by a PR (additions + deletions)\n",
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "pull_request = pull_request.drop(columns=['num_added_LOC', 'num_deleted_LOC', 'PR_size', 'pr_id'], errors='ignore')\n",
        "\n",
        "# Get the sums of the columns we are interested in\n",
        "pr_commit_LOC = (pr_commit_details.groupby(['pr_id'])\n",
        "                                  .sum(['additions', 'deletions', 'changes'])\n",
        "                                  .reset_index())\n",
        "\n",
        "# Rename the sum columns to what we want\n",
        "pr_commit_LOC = (pr_commit_LOC.rename(columns={'additions': 'num_added_LOC'})\n",
        "                              .rename(columns={'deletions': 'num_deleted_LOC'})\n",
        "                              .rename(columns={'changes': 'PR_size'}))\n",
        "\n",
        "# Drop the extraneous columns\n",
        "pr_commit_LOC = pr_commit_LOC.drop(columns=['commit_stats_total', 'commit_stats_additions', 'commit_stats_deletions'])\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "pull_request = pd.merge(pull_request, pr_commit_LOC, left_on='id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage collect the temporary Dataframe\n",
        "pr_commit_LOC = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "pull_request['num_added_LOC'] = pull_request['num_added_LOC'].fillna(0).astype(int)\n",
        "pull_request['num_deleted_LOC'] = pull_request['num_deleted_LOC'].fillna(0).astype(int)\n",
        "pull_request['PR_size'] = pull_request['PR_size'].fillna(0).astype(int)\n",
        "\n",
        "# Drop the 'pr_id' column that was left as an artifact of the merge\n",
        "pull_request = pull_request.drop(columns=['pr_id'], errors='ignore')\n"
      ],
      "metadata": {
        "id": "nj9typbGIcI2",
        "collapsed": true
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 - Purpose (PR_purpose): The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', …).\n",
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "pull_request = pull_request.drop(columns=['PR_purpose'], errors='ignore')\n",
        "\n",
        "# Make a copy of the PR Task Type Dataframe and Drop unneeded columns\n",
        "pr_task = pr_task_type.copy().drop(columns=['agent', 'title', 'reason', 'confidence'], errors='ignore')\n",
        "\n",
        "# Group by ID and get the First Record\n",
        "pr_task = pr_task.groupby(['id']).first()\n",
        "\n",
        "# Rename the column to what we want to keep\n",
        "pr_task = pr_task.rename(columns={'type': 'PR_purpose'})\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "pull_request = pd.merge(pull_request, pr_task, left_on='id', right_on='id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_task = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "pull_request['PR_purpose'] = pull_request['PR_purpose'].fillna('other')\n"
      ],
      "metadata": {
        "id": "Hc2sYaVwFPdL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 - Number of files (num_files_changed): The # of files changed by a PR\n",
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "pull_request = pull_request.drop(columns=['num_files_changed', 'pr_id'], errors='ignore')\n",
        "\n",
        "# Count the number of Files changed and change the column name to what we want\n",
        "pr_files_changed = (pr_commit_details.groupby(['pr_id', 'filename'])\n",
        "                                     .size()\n",
        "                                     .groupby(['pr_id'])\n",
        "                                     .size()\n",
        "                                     .reset_index(name='num_files_changed'))\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "pull_request = pd.merge(pull_request, pr_files_changed, left_on='id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_files_changed = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "pull_request['num_files_changed'] = pull_request['num_files_changed'].fillna(0).astype(int)\n",
        "\n",
        "# Drop the 'pr_id' column that was left as an artifact of the merge\n",
        "pull_request = pull_request.drop(columns=['pr_id'], errors='ignore')\n"
      ],
      "metadata": {
        "id": "YwnUjWn9FPY3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 - Number of commits (num_commits): The # of commits involved in a PR\n",
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "pull_request = pull_request.drop(columns=['num_commits', 'pr_id'], errors='ignore')\n",
        "\n",
        "# Count the number of Commits for the Pull Request, name the column what we want\n",
        "pr_commits_count = pr_commits.groupby(['pr_id']).size().reset_index(name='num_commits')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "pull_request = pd.merge(pull_request, pr_commits_count, left_on='id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_commits_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "pull_request['num_commits'] = pull_request['num_commits'].fillna(0).astype(int)\n",
        "\n",
        "# Drop the 'pr_id' column that was left as an artifact of the merge\n",
        "pull_request = pull_request.drop(columns=['pr_id'], errors='ignore')\n"
      ],
      "metadata": {
        "id": "vOyzUxnHFPUc"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 - Description length: The length of a PR description\n",
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "pull_request = pull_request.drop(columns=['description_length'], errors='ignore')\n",
        "\n",
        "# Get the Length of the Body of the Pull Request\n",
        "pull_request['description_length'] = pull_request['body'].str.len()\n"
      ],
      "metadata": {
        "id": "OZYyz-mVFPPp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 - PR author experience (PR_author_experience): The # of prior PRs that were submitted by the PR author (author’s prior PR count).\n",
        "#     Query the author’s PR history in the same repo and count PRs created before the current one.\n"
      ],
      "metadata": {
        "id": "37dKywGDFPKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 - Is member (is_member): Whether or not the author is a member or outside collaborator\n"
      ],
      "metadata": {
        "id": "DW_ZiHEHFPEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 - Number of comments (num_comments): The # of comments left on a PR\n",
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "pull_request = pull_request.drop(columns=['num_comments', 'pr_id'], errors='ignore')\n",
        "\n",
        "# Count the number of Comments for the Pull Request, name the column what we want.\n",
        "pr_comments_count = pr_comments.groupby(['pr_id']).size().reset_index(name='num_comments')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "pull_request = pd.merge(pull_request, pr_comments_count, left_on='id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "pull_request['num_comments'] = pull_request['num_comments'].fillna(0).astype(int)\n",
        "\n",
        "# Drop the 'pr_id' column that was left as an artifact of the merge\n",
        "pull_request = pull_request.drop(columns=['pr_id'], errors='ignore')\n"
      ],
      "metadata": {
        "id": "2BNmsnGIFO9Y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 - Number of author comments (num_author_comments): The # of comments left by the PR author\n"
      ],
      "metadata": {
        "id": "seCVAfr1FO2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 - Number of reviewer comments (num_reviewer_comments): The # of comments left by the reviewers who participate in the disucssion\n"
      ],
      "metadata": {
        "id": "wQL4RLbzFOvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13 - Number of reviewers (num_reviewers): The # of developers who participate in the discussion.\n"
      ],
      "metadata": {
        "id": "ipbOK8tqFOnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 - Repo age (repo_age): Time interval between the repository creation time and PR creation time in days.\n"
      ],
      "metadata": {
        "id": "ZBVOYKNaFOcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 - Language (language): The repository language that a PR belongs to, represented by the top 10 or others\n",
        "# 16 - Number of forks (num_forks): The # of forks that a repository has\n",
        "# 17 - Number of stargazers (num_stargazers): The # of stargazers that a repository has.\n"
      ],
      "metadata": {
        "id": "hEoTBBXjFOVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fourth**, Bot detection and filtering employed the methodology of Golzadeh et al. (2022); simple “bot” username suffix check with a comprehensive, manually verified list of 527 bot accounts"
      ],
      "metadata": {
        "id": "V8vXCVjGW1aV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNB1iJ34WyMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}