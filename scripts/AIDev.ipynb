{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awjans/CopilotForPRsAdoption/blob/main/scripts/AIDev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection/Cleaning Overview\n",
        "1. **PR identification**\n",
        "   * Queried GitHub via GraphQL for PRs whose description contained the phrase **‚ÄúGenerated by Copilot‚Äù** or any of the marker tags:\n",
        "\n",
        "     * `copilot:summary`\n",
        "     * `copilot:walkthrough`\n",
        "     * `copilot:poem`\n",
        "     * `copilot:all`\n",
        "\n",
        "2. **Scope**\n",
        "   * Collected **18,256 PRs** from **146 early-adopter repositories** during **March 2023 ‚Äì August 2023**.\n",
        "\n",
        "3. **Control set**\n",
        "   * For the same repositories, gathered **54,188 PRs** that did **not** contain any Copilot marker.\n",
        "   * These served as the **untreated (control) group** for the **RQ2 comparison**.\n",
        "\n",
        "4. **Bot filtering**\n",
        "   * Removed PRs and comments authored by bots using the **high-precision method** of **Golzadeh et al. (2022)**, which included:\n",
        "     * (i) Usernames ending with ‚Äúbot‚Äù\n",
        "     * (ii) A curated list of **527 known bot accounts**\n",
        "\n",
        "5. **Revision extraction (RQ3)**\n",
        "   * From the **18,256 Copilot-generated PRs**, retrieved the full **edit history** of PR descriptions.\n",
        "   * Identified **1,437 revisions** where developers **edited the AI-suggested content**."
      ],
      "metadata": {
        "id": "j4dl0GKtyPQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXf5zM0UZf20",
        "outputId": "32ed254e-9e09-4fb2-8008-bb59d7bf58f5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eZ03WzKRQ1bS"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from dateutil import parser\n",
        "from google.colab import userdata\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "import random\n",
        "from itertools import cycle\n",
        "from google.colab import userdata\n",
        "from dateutil import parser\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b7c674"
      },
      "source": [
        "# **First**, Define the URLs of the AIDev Parquet Files that we are intersted in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de61a39b"
      },
      "source": [
        "pull_request_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_pull_request.parquet'\n",
        "pr_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_comments.parquet'\n",
        "pr_commits_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commits.parquet'\n",
        "pr_commit_details_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commit_details.parquet'\n",
        "pr_reviews_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_reviews.parquet'\n",
        "pr_review_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_review_comments.parquet'\n",
        "pr_task_type_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_task_type.parquet'\n",
        "repository_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_repository.parquet'\n",
        "user_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/user.parquet'\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Second**, We need to load the data from the URLs (15s)"
      ],
      "metadata": {
        "id": "CKy9xnAeb1p6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Parquet Files"
      ],
      "metadata": {
        "id": "eU0V6v6thrEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the Parquet file into a Pandas DataFrame from the file URL.\n",
        "\"\"\"\n",
        "def load_data(url: str):\n",
        "  import pandas as pd # Import pandas inside the function\n",
        "  try:\n",
        "    # For Parquet files:\n",
        "    df = pd.read_parquet(url)\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading data: {e}\")\n",
        "      print(\"Please ensure the URL is correct and the file is publicly accessible.\")\n",
        "      return None # Return None in case of an error"
      ],
      "metadata": {
        "id": "Qcu3bdoFM6UE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pull_request = load_data(pull_request_file_url)\n",
        "pr_comments = load_data(pr_comments_file_url)\n",
        "pr_commits = load_data(pr_commits_file_url)\n",
        "pr_commit_details = load_data(pr_commit_details_file_url)\n",
        "pr_reviews = load_data(pr_reviews_file_url)\n",
        "pr_review_comments = load_data(pr_review_comments_file_url)\n",
        "pr_task_type = load_data(pr_task_type_file_url)\n",
        "repository = load_data(repository_file_url)\n",
        "user = load_data(user_file_url)"
      ],
      "metadata": {
        "id": "YAPFfpktRJkz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(pr_reviews.head(25))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "C7Cc2ODrPIpY",
        "outputId": "df5d8954-b547-4460-dc3f-022320918ad4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            id       pr_id                                user user_type  \\\n",
              "0   2885691382  3107321792                   coderabbitai[bot]       Bot   \n",
              "1   2885712797  3107321792                   coderabbitai[bot]       Bot   \n",
              "2   3059587397  3234660269                                Fank      User   \n",
              "3   3059589121  3234660269                           benfdking      User   \n",
              "4   2813254905  3037457814                         wilsonccccc      User   \n",
              "5   2679908484  2915198291                   ellipsis-dev[bot]       Bot   \n",
              "6   3011938086  3223924413                           davinchia      User   \n",
              "7   3034262238  3223924413                         aaronsteers      User   \n",
              "8   2866389127  3088611576  copilot-pull-request-reviewer[bot]       Bot   \n",
              "9   2866393218  3088611576                             parruda      User   \n",
              "10  2680962240  2915784381                         nickskalkin      User   \n",
              "11  2681196091  2915784381                        olerichter00      User   \n",
              "12  2682336603  2915784381                             damassi      User   \n",
              "13  3047266705  3256172444                   coderabbitai[bot]       Bot   \n",
              "14  3049598567  3256172444                   coderabbitai[bot]       Bot   \n",
              "15  3049626708  3256172444                   coderabbitai[bot]       Bot   \n",
              "16  3054106627  3256172444                             mainred      User   \n",
              "17  3066517093  3256172444                          moshemorad      User   \n",
              "18  3066527532  3256172444                          moshemorad      User   \n",
              "19  3066566545  3256172444                          moshemorad      User   \n",
              "20  3066718478  3256172444                          moshemorad      User   \n",
              "21  3066737736  3256172444                          moshemorad      User   \n",
              "22  3066966270  3256172444                          moshemorad      User   \n",
              "23  3066980423  3256172444                          moshemorad      User   \n",
              "24  3067010451  3256172444                          moshemorad      User   \n",
              "\n",
              "        state          submitted_at  \\\n",
              "0   COMMENTED  2025-06-01T14:22:22Z   \n",
              "1   COMMENTED  2025-06-01T14:37:45Z   \n",
              "2   COMMENTED  2025-07-27T15:06:40Z   \n",
              "3   COMMENTED  2025-07-27T15:12:52Z   \n",
              "4    APPROVED  2025-05-03T15:34:42Z   \n",
              "5   COMMENTED  2025-03-12T21:24:55Z   \n",
              "6   COMMENTED  2025-07-11T20:48:38Z   \n",
              "7    APPROVED  2025-07-18T17:11:06Z   \n",
              "8   COMMENTED  2025-05-24T17:14:22Z   \n",
              "9    APPROVED  2025-05-24T17:34:15Z   \n",
              "10   APPROVED  2025-03-13T08:33:50Z   \n",
              "11   APPROVED  2025-03-13T09:44:20Z   \n",
              "12   APPROVED  2025-03-13T15:08:08Z   \n",
              "13  COMMENTED  2025-07-23T12:46:47Z   \n",
              "14  COMMENTED  2025-07-24T01:14:00Z   \n",
              "15  COMMENTED  2025-07-24T01:29:25Z   \n",
              "16  COMMENTED  2025-07-25T06:56:45Z   \n",
              "17  COMMENTED  2025-07-29T09:33:20Z   \n",
              "18  COMMENTED  2025-07-29T09:35:35Z   \n",
              "19  COMMENTED  2025-07-29T09:43:36Z   \n",
              "20  COMMENTED  2025-07-29T10:12:37Z   \n",
              "21  COMMENTED  2025-07-29T10:17:11Z   \n",
              "22  COMMENTED  2025-07-29T11:21:45Z   \n",
              "23  COMMENTED  2025-07-29T11:24:41Z   \n",
              "24  COMMENTED  2025-07-29T11:29:26Z   \n",
              "\n",
              "                                                 body  \n",
              "0   **Actionable comments posted: 2**\\n\\n<details>...  \n",
              "1   **Actionable comments posted: 1**\\n\\n<details>...  \n",
              "2                                                None  \n",
              "3                                                None  \n",
              "4                                                None  \n",
              "5   :+1: Looks good to me! Reviewed everything up ...  \n",
              "6                                                None  \n",
              "7                                                None  \n",
              "8   ## Pull Request Overview\\n\\nImplements dot‚Äênot...  \n",
              "9                                                None  \n",
              "10                                               None  \n",
              "11                                                 üåü   \n",
              "12  ‚≠ê \\r\\n\\r\\nNeed to run this over all of volts e...  \n",
              "13  **Actionable comments posted: 17**\\n\\n<details...  \n",
              "14  **Actionable comments posted: 1**\\n\\n<details>...  \n",
              "15  **Actionable comments posted: 1**\\n\\n<details>...  \n",
              "16                                               None  \n",
              "17                                               None  \n",
              "18                                               None  \n",
              "19                                               None  \n",
              "20                                               None  \n",
              "21                                               None  \n",
              "22                                               None  \n",
              "23                                               None  \n",
              "24                                               None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64b3633f-254c-482b-9afb-f6e738d88d0a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>pr_id</th>\n",
              "      <th>user</th>\n",
              "      <th>user_type</th>\n",
              "      <th>state</th>\n",
              "      <th>submitted_at</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2885691382</td>\n",
              "      <td>3107321792</td>\n",
              "      <td>coderabbitai[bot]</td>\n",
              "      <td>Bot</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-06-01T14:22:22Z</td>\n",
              "      <td>**Actionable comments posted: 2**\\n\\n&lt;details&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2885712797</td>\n",
              "      <td>3107321792</td>\n",
              "      <td>coderabbitai[bot]</td>\n",
              "      <td>Bot</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-06-01T14:37:45Z</td>\n",
              "      <td>**Actionable comments posted: 1**\\n\\n&lt;details&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3059587397</td>\n",
              "      <td>3234660269</td>\n",
              "      <td>Fank</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-27T15:06:40Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3059589121</td>\n",
              "      <td>3234660269</td>\n",
              "      <td>benfdking</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-27T15:12:52Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2813254905</td>\n",
              "      <td>3037457814</td>\n",
              "      <td>wilsonccccc</td>\n",
              "      <td>User</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-05-03T15:34:42Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2679908484</td>\n",
              "      <td>2915198291</td>\n",
              "      <td>ellipsis-dev[bot]</td>\n",
              "      <td>Bot</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-03-12T21:24:55Z</td>\n",
              "      <td>:+1: Looks good to me! Reviewed everything up ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3011938086</td>\n",
              "      <td>3223924413</td>\n",
              "      <td>davinchia</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-11T20:48:38Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3034262238</td>\n",
              "      <td>3223924413</td>\n",
              "      <td>aaronsteers</td>\n",
              "      <td>User</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-07-18T17:11:06Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2866389127</td>\n",
              "      <td>3088611576</td>\n",
              "      <td>copilot-pull-request-reviewer[bot]</td>\n",
              "      <td>Bot</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-05-24T17:14:22Z</td>\n",
              "      <td>## Pull Request Overview\\n\\nImplements dot‚Äênot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2866393218</td>\n",
              "      <td>3088611576</td>\n",
              "      <td>parruda</td>\n",
              "      <td>User</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-05-24T17:34:15Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2680962240</td>\n",
              "      <td>2915784381</td>\n",
              "      <td>nickskalkin</td>\n",
              "      <td>User</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-03-13T08:33:50Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2681196091</td>\n",
              "      <td>2915784381</td>\n",
              "      <td>olerichter00</td>\n",
              "      <td>User</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-03-13T09:44:20Z</td>\n",
              "      <td>üåü</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2682336603</td>\n",
              "      <td>2915784381</td>\n",
              "      <td>damassi</td>\n",
              "      <td>User</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-03-13T15:08:08Z</td>\n",
              "      <td>‚≠ê \\r\\n\\r\\nNeed to run this over all of volts e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3047266705</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>coderabbitai[bot]</td>\n",
              "      <td>Bot</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-23T12:46:47Z</td>\n",
              "      <td>**Actionable comments posted: 17**\\n\\n&lt;details...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3049598567</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>coderabbitai[bot]</td>\n",
              "      <td>Bot</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-24T01:14:00Z</td>\n",
              "      <td>**Actionable comments posted: 1**\\n\\n&lt;details&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3049626708</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>coderabbitai[bot]</td>\n",
              "      <td>Bot</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-24T01:29:25Z</td>\n",
              "      <td>**Actionable comments posted: 1**\\n\\n&lt;details&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3054106627</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>mainred</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-25T06:56:45Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3066517093</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T09:33:20Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3066527532</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T09:35:35Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3066566545</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T09:43:36Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3066718478</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T10:12:37Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3066737736</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T10:17:11Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3066966270</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T11:21:45Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3066980423</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T11:24:41Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3067010451</td>\n",
              "      <td>3256172444</td>\n",
              "      <td>moshemorad</td>\n",
              "      <td>User</td>\n",
              "      <td>COMMENTED</td>\n",
              "      <td>2025-07-29T11:29:26Z</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64b3633f-254c-482b-9afb-f6e738d88d0a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-64b3633f-254c-482b-9afb-f6e738d88d0a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-64b3633f-254c-482b-9afb-f6e738d88d0a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e802d745-12d3-4378-aaac-722182bdf8cf\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e802d745-12d3-4378-aaac-722182bdf8cf')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e802d745-12d3-4378-aaac-722182bdf8cf button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pr_reviews\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 146181515,\n        \"min\": 2679908484,\n        \"max\": 3067010451,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          2866389127,\n          3054106627,\n          2885691382\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pr_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 129042174,\n        \"min\": 2915198291,\n        \"max\": 3256172444,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3234660269,\n          3088611576,\n          3107321792\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"nickskalkin\",\n          \"damassi\",\n          \"coderabbitai[bot]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"User\",\n          \"Bot\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"APPROVED\",\n          \"COMMENTED\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"submitted_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"2025-05-24T17:14:22Z\",\n          \"2025-07-25T06:56:45Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"**Actionable comments posted: 1**\\n\\n<details>\\n<summary>\\u267b\\ufe0f Duplicate comments (14)</summary><blockquote>\\n\\n<details>\\n<summary>holmes/plugins/toolsets/aks-node-health.yaml (1)</summary><blockquote>\\n\\n`10-10`: **Remove trailing spaces.**\\n\\nLine 10 contains only trailing spaces that should be removed.\\n\\n</blockquote></details>\\n<details>\\n<summary>holmes/plugins/toolsets/aks.yaml (1)</summary><blockquote>\\n\\n`10-10`: **Remove trailing spaces.**\\n\\nLine 10 contains only trailing spaces that should be removed.\\n\\n</blockquote></details>\\n<details>\\n<summary>tests/integration/test_kubernetes_transformer_execution.py (1)</summary><blockquote>\\n\\n`509-509`: **Remove unused variable assignment.**\\n\\nThe `result` variable is assigned but never used.\\n\\n</blockquote></details>\\n<details>\\n<summary>tests/integration/test_tool_execution_pipeline.py (5)</summary><blockquote>\\n\\n`39-41`: **Avoid accessing private registry attributes.**\\n\\nAccessing the private `_transformers` attribute breaks encapsulation and could fail if the registry implementation changes.\\n\\n\\n\\n\\nConsider storing the transformer class reference using public methods:\\n\\n```diff\\n-            # Store the original transformer class for restoration\\n-            self._original_llm_summarize = registry._transformers[\\\"llm_summarize\\\"]\\n+            # Store the original transformer class for restoration\\n+            # Could use registry.create_transformer to verify it exists\\n+            try:\\n+                test_instance = registry.create_transformer(\\\"llm_summarize\\\", {})\\n+                self._original_llm_summarize = test_instance.__class__\\n+            except Exception:\\n+                self._original_llm_summarize = None\\n```\\n\\n---\\n\\n`123-126`: **Fix the original size calculation to match test data.**\\n\\nThe original size calculation doesn't match the actual test data structure (7 different entries \\u00d7 20 repetitions).\\n\\n\\n\\n\\n```diff\\n-        original_size = len(\\n-            \\\"\\\\n\\\".join([\\\"2024-01-01 10:00:00 INFO Starting application\\\"] * 140)\\n-        )\\n+        # Calculate actual original size based on test data\\n+        log_entries = [\\n+            \\\"2024-01-01 10:00:00 INFO Starting application\\\",\\n+            \\\"2024-01-01 10:00:01 INFO Loading configuration\\\",\\n+            \\\"2024-01-01 10:00:02 DEBUG Database connection established\\\",\\n+            \\\"2024-01-01 10:00:03 INFO Server listening on port 8080\\\",\\n+            \\\"2024-01-01 10:01:00 ERROR Failed to process request: timeout\\\",\\n+            \\\"2024-01-01 10:01:01 WARN Retrying request\\\",\\n+            \\\"2024-01-01 10:01:02 INFO Request processed successfully\\\",\\n+        ] * 20\\n+        original_size = len(\\\"\\\\n\\\".join(log_entries))\\n```\\n\\n---\\n\\n`236-237`: **Make timing assertion more robust.**\\n\\nThe assertion `\\\"in 0.\\\"` assumes sub-second execution, which could fail on slow systems.\\n\\n\\n\\n\\n```diff\\n-                assert \\\"in 0.\\\" in performance_log  # Should show elapsed time\\n+                # Should show elapsed time in format \\\"in X.XXXs\\\"\\n+                assert \\\" in \\\" in performance_log and \\\"s\\\" in performance_log\\n```\\n\\n---\\n\\n`278-278`: **Fix incorrect length comparison.**\\n\\nThe kubectl_output was already multiplied by 5, so multiplying again in the assertion is incorrect.\\n\\n\\n\\n\\n```diff\\n-        assert len(result.data) < len(kubectl_output) * 5  # Much shorter than original\\n+        assert len(result.data) < len(kubectl_output)  # Much shorter than original\\n```\\n\\n---\\n\\n`298-300`: **Make assertion more deterministic.**\\n\\nThe OR condition makes the test less predictable. Since the threshold is 10 and the message is longer, it should always be summarized.\\n\\n\\n\\n\\n```diff\\n-        assert (\\n-            \\\"SUMMARIZED:\\\" in result.data\\n-            or \\\"Important debugging information\\\" in result.data\\n-        )\\n+        # Message is longer than threshold (10), so it should be summarized\\n+        assert \\\"SUMMARIZED:\\\" in result.data\\n+        assert \\\"Important debugging information\\\" not in result.data\\n```\\n\\n</blockquote></details>\\n<details>\\n<summary>tests/core/transformers/test_transformers.py (1)</summary><blockquote>\\n\\n`24-31`: **Simplify nested if statements in validation logic.**\\n\\nThe static analysis correctly identifies that the nested if statements can be combined for better readability.\\n\\n\\n\\n\\n```diff\\n def _validate_config(self) -> None:\\n-    if \\\"threshold\\\" in self.config:\\n-        if (\\n-            not isinstance(self.config[\\\"threshold\\\"], int)\\n-            or self.config[\\\"threshold\\\"] < 0\\n-        ):\\n-            raise ValueError(\\\"Threshold must be a non-negative integer\\\")\\n+    if \\\"threshold\\\" in self.config and (\\n+        not isinstance(self.config[\\\"threshold\\\"], int)\\n+        or self.config[\\\"threshold\\\"] < 0\\n+    ):\\n+        raise ValueError(\\\"Threshold must be a non-negative integer\\\")\\n```\\n\\n</blockquote></details>\\n<details>\\n<summary>tests/core/test_tool_transformers.py (1)</summary><blockquote>\\n\\n`655-656`: **Remove unused variable assignment.**\\n\\nThe `result` variable is assigned but never used in this test method.\\n\\n\\nApply this fix:\\n```diff\\n-        with patch(\\\"holmes.core.tools.logging\\\") as mock_logging:\\n-            result = tool.invoke({})\\n+        with patch(\\\"holmes.core.tools.logging\\\") as mock_logging:\\n+            tool.invoke({})\\n```\\n\\n</blockquote></details>\\n<details>\\n<summary>holmes/core/tools.py (2)</summary><blockquote>\\n\\n`142-154`: **Simplify nested if statements in transformer validation.**\\n\\nThe static analysis correctly identifies that the nested if statements can be combined.\\n\\n\\n```diff\\n @model_validator(mode=\\\"after\\\")\\n def validate_transformers(self):\\n     \\\"\\\"\\\"Validate transformer configurations during tool creation.\\\"\\\"\\\"\\n-    if self.transformer_configs is not None:\\n-        # Use safe validation to log warnings instead of failing\\n-        if not safe_validate_tool_transformer_configs(\\n-            self.name, self.transformer_configs\\n-        ):\\n-            # If validation fails, clear transforms to prevent runtime errors\\n-            logging.warning(f\\\"Clearing invalid transforms for tool '{self.name}'\\\")\\n-            self.transformer_configs = None\\n+    if self.transformer_configs is not None and not safe_validate_tool_transformer_configs(\\n+        self.name, self.transformer_configs\\n+    ):\\n+        # If validation fails, clear transforms to prevent runtime errors\\n+        logging.warning(f\\\"Clearing invalid transforms for tool '{self.name}'\\\")\\n+        self.transformer_configs = None\\n     return self\\n```\\n\\n---\\n\\n`236-244`: **Remove unused size_change variable.**\\n\\nThe `size_change` variable is calculated but never used. Either use it in logging or remove it.\\n\\n\\n```diff\\n # Let the transformer provide its own logging message if it wants to\\n post_transform_size = len(transformed_data)\\n-size_change = post_transform_size - pre_transform_size\\n\\n # Generic logging - transformers can override this with their own specific metrics\\n logging.info(\\n     f\\\"Applied transformer '{transformer_name}' to tool '{self.name}' output \\\"\\n-    f\\\"in {transform_elapsed:.2f}s (output size: {post_transform_size:,} characters)\\\"\\n+    f\\\"in {transform_elapsed:.2f}s (output size: {pre_transform_size:,} \\u2192 {post_transform_size:,} characters)\\\"\\n )\\n```\\n\\n</blockquote></details>\\n<details>\\n<summary>docs/transformers.md (2)</summary><blockquote>\\n\\n`119-127`: **Add language specification to fenced code blocks.**\\n\\nThe static analysis correctly identifies that these code blocks are missing language specifications. While they appear to be plain text output examples, adding appropriate language identifiers will improve readability.\\n\\n\\n```diff\\n-```\\n+```text\\n Summarize this operational data focusing on:\\n - What needs attention or immediate action\\n - Group similar entries into a single line and description\\n - Make sure to mention outliers, errors, and non-standard patterns\\n - List normal/healthy patterns as aggregate descriptions\\n - When listing problematic entries, also try to use aggregate descriptions when possible\\n - When possible, mention exact keywords, IDs, or patterns so the user can filter/search the original data and drill down on the parts they care about\\n-```\\n+```\\n```\\n\\n---\\n\\n`151-187`: **Add language specification to output example code blocks.**\\n\\nThese example output blocks should have language specifications for proper formatting.\\n\\n\\n```diff\\n **Without Transformer:**\\n-```\\n+```text\\n NAME                                READY   STATUS      RESTARTS   AGE     IP           NODE\\n pod-1                              1/1     Running     0          5d      10.1.1.1     node-1\\n pod-2                              1/1     Running     0          5d      10.1.1.2     node-1\\n pod-3                              1/1     Running     0          5d      10.1.1.3     node-2\\n pod-4                              0/1     CrashLoopBackOff  15    1h      10.1.1.4     node-2\\n [... 100 more similar pods ...]\\n-```\\n+```\\n\\n **With Transformer:**\\n-```\\n+```text\\n Found 104 pods across 2 nodes:\\n - 103 pods are healthy and running (age: 5d, on node-1 and node-2)\\n - 1 pod in CrashLoopBackOff state: pod-4 (15 restarts, 1h old, IP 10.1.1.4, node-2)\\n - Search with \\\"grep pod-4\\\" or \\\"grep CrashLoopBackOff\\\" to drill down on the problematic pod\\n-```\\n+```\\n\\n ### Log Analysis\\n\\n **Without Transformer:**\\n-```\\n+```text\\n 2024-01-15T10:30:01Z INFO Starting application...\\n 2024-01-15T10:30:02Z INFO Database connection established\\n 2024-01-15T10:30:03Z INFO Loading configuration...\\n [... 1000 similar INFO logs ...]\\n 2024-01-15T10:35:15Z ERROR Failed to connect to Redis: connection timeout\\n 2024-01-15T10:35:16Z WARN Retrying Redis connection (attempt 1/3)\\n-```\\n+```\\n\\n **With Transformer:**\\n-```\\n+```text\\n Log analysis (2024-01-15 10:30-10:35):\\n - 1000+ INFO messages showing normal application startup and operations\\n - 1 ERROR: Redis connection timeout at 10:35:15Z \\n - 1 WARN: Redis retry attempt at 10:35:16Z\\n - Search with \\\"grep ERROR\\\" or \\\"grep Redis\\\" to investigate the connection issue\\n-```\\n+```\\n```\\n\\n</blockquote></details>\\n\\n</blockquote></details>\\n\\n<details>\\n<summary>\\ud83e\\uddf9 Nitpick comments (5)</summary><blockquote>\\n\\n<details>\\n<summary>tests/core/test_transformer_backwards_compatibility.py (2)</summary><blockquote>\\n\\n`77-82`: **Combine nested `with` statements.**\\n\\n\\nThe nested `with` statements can be combined for cleaner code:\\n\\n```diff\\n-        with patch(\\\"holmes.config.load_model_from_file\\\") as mock_load:\\n-            mock_load.return_value = Config(**mock_config_content)\\n-\\n-            with patch(\\\"pathlib.Path.exists\\\") as mock_exists:\\n-                mock_exists.return_value = True\\n+        with patch(\\\"holmes.config.load_model_from_file\\\") as mock_load, \\\\\\n+             patch(\\\"pathlib.Path.exists\\\") as mock_exists:\\n+            mock_load.return_value = Config(**mock_config_content)\\n+            mock_exists.return_value = True\\n```\\n\\n---\\n\\n`94-99`: **Combine nested `with` statements.**\\n\\n\\nThe nested `with` statements can be combined:\\n\\n```diff\\n-        with patch.dict(\\\"os.environ\\\", test_env):\\n-            with patch(\\n-                \\\"holmes.config.Config._Config__get_cluster_name\\\"\\n-            ) as mock_cluster:\\n-                mock_cluster.return_value = \\\"test-cluster\\\"\\n+        with patch.dict(\\\"os.environ\\\", test_env), \\\\\\n+             patch(\\\"holmes.config.Config._Config__get_cluster_name\\\") as mock_cluster:\\n+            mock_cluster.return_value = \\\"test-cluster\\\"\\n```\\n\\n</blockquote></details>\\n<details>\\n<summary>tests/core/transformers/test_llm_summarize.py (2)</summary><blockquote>\\n\\n`15-28`: **Consider extracting duplicate `create_mock_llm` helper.**\\n\\nBoth test classes define identical `create_mock_llm` methods. Consider extracting this to a module-level function to follow DRY principles.\\n\\n\\n\\n```diff\\n+def create_mock_llm(response_content: str = \\\"Summarized content\\\"):\\n+    \\\"\\\"\\\"Create a mock LLM that returns the specified response.\\\"\\\"\\\"\\n+    mock_llm = Mock()\\n+    mock_response = Mock()\\n+    mock_choice = Mock()\\n+    mock_message = Mock()\\n+\\n+    mock_message.content = response_content\\n+    mock_choice.message = mock_message\\n+    mock_response.choices = [mock_choice]\\n+    mock_llm.completion.return_value = mock_response\\n+\\n+    return mock_llm\\n+\\n+\\n class TestLLMSummarizeTransformer:\\n     \\\"\\\"\\\"Test cases for LLMSummarizeTransformer class.\\\"\\\"\\\"\\n \\n-    def create_mock_llm(self, response_content: str = \\\"Summarized content\\\"):\\n-        \\\"\\\"\\\"Create a mock LLM that returns the specified response.\\\"\\\"\\\"\\n-        mock_llm = Mock()\\n-        mock_response = Mock()\\n-        mock_choice = Mock()\\n-        mock_message = Mock()\\n-\\n-        mock_message.content = response_content\\n-        mock_choice.message = mock_message\\n-        mock_response.choices = [mock_choice]\\n-        mock_llm.completion.return_value = mock_response\\n-\\n-        return mock_llm\\n```\\n\\n\\nAlso applies to: 324-337\\n\\n---\\n\\n`403-407`: **Use `isinstance()` for exception type checking.**\\n\\nReplace class comparison with `isinstance()` for more Pythonic and robust type checking.\\n\\n\\n\\n```diff\\n         # Verify error is properly wrapped and chained\\n         assert \\\"Failed to summarize content with fast model\\\" in str(exc_info.value)\\n-        assert exc_info.value.__cause__.__class__ == ConnectionError\\n+        assert isinstance(exc_info.value.__cause__, ConnectionError)\\n         assert \\\"Network timeout\\\" in str(exc_info.value.__cause__)\\n```\\n\\n</blockquote></details>\\n<details>\\n<summary>tests/core/transformers/test_validation.py (1)</summary><blockquote>\\n\\n`323-323`: **Add newline at end of file**\\n\\nPython files should end with a newline character.\\n\\n\\n```diff\\n         error_str = str(exc_info.value)\\n         assert \\\"required_param is missing\\\" in error_str\\n+\\n```\\n\\n</blockquote></details>\\n\\n</blockquote></details>\\n\\n<details>\\n<summary>\\ud83d\\udcdc Review details</summary>\\n\\n**Configuration used: CodeRabbit UI**\\n**Review profile: CHILL**\\n**Plan: Pro**\\n\\n\\n<details>\\n<summary>\\ud83d\\udce5 Commits</summary>\\n\\nReviewing files that changed from the base of the PR and between 565465513ed6d2eafa9f2022356d44aea42e0681 and 8a2df1931ee254688ead2509b7dbfe56b4520822.\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83d\\udcd2 Files selected for processing (29)</summary>\\n\\n* `README.md` (1 hunks)\\n* `config.example.yaml` (1 hunks)\\n* `docs/transformers.md` (1 hunks)\\n* `holmes/config.py` (6 hunks)\\n* `holmes/core/tools.py` (5 hunks)\\n* `holmes/core/toolset_manager.py` (6 hunks)\\n* `holmes/core/transformers/__init__.py` (1 hunks)\\n* `holmes/core/transformers/base.py` (1 hunks)\\n* `holmes/core/transformers/llm_summarize.py` (1 hunks)\\n* `holmes/core/transformers/registry.py` (1 hunks)\\n* `holmes/core/transformers/validation.py` (1 hunks)\\n* `holmes/main.py` (3 hunks)\\n* `holmes/plugins/toolsets/aks-node-health.yaml` (3 hunks)\\n* `holmes/plugins/toolsets/aks.yaml` (3 hunks)\\n* `holmes/plugins/toolsets/kubernetes.yaml` (3 hunks)\\n* `holmes/plugins/toolsets/kubernetes_logs.yaml` (2 hunks)\\n* `holmes/utils/config_utils.py` (1 hunks)\\n* `tests/config_class/test_config_transformers.py` (1 hunks)\\n* `tests/core/test_config_transformers.py` (1 hunks)\\n* `tests/core/test_tool_transformers.py` (1 hunks)\\n* `tests/core/test_toolset_manager.py` (1 hunks)\\n* `tests/core/test_transformer_backwards_compatibility.py` (1 hunks)\\n* `tests/core/transformers/__init__.py` (1 hunks)\\n* `tests/core/transformers/test_llm_summarize.py` (1 hunks)\\n* `tests/core/transformers/test_transformers.py` (1 hunks)\\n* `tests/core/transformers/test_validation.py` (1 hunks)\\n* `tests/integration/test_config_merging_integration.py` (1 hunks)\\n* `tests/integration/test_kubernetes_transformer_execution.py` (1 hunks)\\n* `tests/integration/test_tool_execution_pipeline.py` (1 hunks)\\n\\n</details>\\n\\n<details>\\n<summary>\\u2705 Files skipped from review due to trivial changes (2)</summary>\\n\\n* README.md\\n* holmes/core/transformers/__init__.py\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83d\\udea7 Files skipped from review as they are similar to previous changes (13)</summary>\\n\\n* tests/core/transformers/__init__.py\\n* config.example.yaml\\n* holmes/utils/config_utils.py\\n* holmes/plugins/toolsets/kubernetes_logs.yaml\\n* holmes/core/transformers/llm_summarize.py\\n* holmes/core/transformers/validation.py\\n* holmes/plugins/toolsets/kubernetes.yaml\\n* tests/integration/test_config_merging_integration.py\\n* tests/core/test_toolset_manager.py\\n* tests/config_class/test_config_transformers.py\\n* holmes/config.py\\n* holmes/main.py\\n* holmes/core/transformers/registry.py\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83e\\uddf0 Additional context used</summary>\\n\\n<details>\\n<summary>\\ud83e\\udde0 Learnings (7)</summary>\\n\\n<details>\\n<summary>tests/integration/test_tool_execution_pipeline.py (1)</summary>\\n\\nLearnt from: Sheeproid\\nPR: robusta-dev/holmesgpt#586\\nFile: tests/llm/fixtures/test_ask_holmes/03_what_is_the_command_to_port_forward/test_case.yaml:4-4\\nTimestamp: 2025-07-02T10:27:17.231Z\\nLearning: In LLM-as-judge test cases for HolmesGPT, expected outputs should be descriptive rather than prescriptive when testing for flexible responses like port numbers. Using specific values in expected outputs can cause unnecessary test failures when the AI generates different but equally valid responses.\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_llm_summarize.py (1)</summary>\\n\\nLearnt from: Sheeproid\\nPR: robusta-dev/holmesgpt#586\\nFile: tests/llm/fixtures/test_ask_holmes/03_what_is_the_command_to_port_forward/test_case.yaml:4-4\\nTimestamp: 2025-07-02T10:27:17.231Z\\nLearning: In LLM-as-judge test cases for HolmesGPT, expected outputs should be descriptive rather than prescriptive when testing for flexible responses like port numbers. Using specific values in expected outputs can cause unnecessary test failures when the AI generates different but equally valid responses.\\n\\n</details>\\n<details>\\n<summary>tests/core/test_config_transformers.py (1)</summary>\\n\\nLearnt from: nherment\\nPR: robusta-dev/holmesgpt#535\\nFile: holmes/plugins/toolsets/bash/bash_toolset.py:207-209\\nTimestamp: 2025-06-24T05:51:04.543Z\\nLearning: The init_config method in toolsets should be idempotent - safely callable multiple times without errors. self.config should maintain consistent typing (not alternate between dict and config object types) throughout the object lifecycle.\\n\\n</details>\\n<details>\\n<summary>holmes/core/tools.py (2)</summary>\\n\\nLearnt from: nherment\\nPR: robusta-dev/holmesgpt#610\\nFile: .github/workflows/llm-evaluation.yaml:39-42\\nTimestamp: 2025-07-08T08:45:41.069Z\\nLearning: The robusta-dev/holmesgpt codebase has comprehensive existing validation for Azure environment variables (AZURE_API_BASE, AZURE_API_KEY, AZURE_API_VERSION) and MODEL in tests/llm/utils/classifiers.py, tests/llm/conftest.py, and holmes/core/llm.py. Don't suggest adding redundant validation logic.\\n\\nLearnt from: nherment\\nPR: robusta-dev/holmesgpt#610\\nFile: .github/workflows/llm-evaluation.yaml:39-42\\nTimestamp: 2025-07-08T08:45:41.069Z\\nLearning: When suggesting improvements to environment variable handling in robusta-dev/holmesgpt, check first if validation logic already exists rather than reimplementing it.\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_validation.py (1)</summary>\\n\\nLearnt from: nherment\\nPR: robusta-dev/holmesgpt#610\\nFile: .github/workflows/llm-evaluation.yaml:39-42\\nTimestamp: 2025-07-08T08:45:41.069Z\\nLearning: The robusta-dev/holmesgpt codebase has comprehensive existing validation for Azure environment variables (AZURE_API_BASE, AZURE_API_KEY, AZURE_API_VERSION) and MODEL in tests/llm/utils/classifiers.py, tests/llm/conftest.py, and holmes/core/llm.py. Don't suggest adding redundant validation logic.\\n\\n</details>\\n<details>\\n<summary>holmes/core/toolset_manager.py (1)</summary>\\n\\nLearnt from: nherment\\nPR: robusta-dev/holmesgpt#535\\nFile: holmes/plugins/toolsets/bash/bash_toolset.py:207-209\\nTimestamp: 2025-06-24T05:51:04.543Z\\nLearning: The init_config method in toolsets should be idempotent - safely callable multiple times without errors. self.config should maintain consistent typing (not alternate between dict and config object types) throughout the object lifecycle.\\n\\n</details>\\n<details>\\n<summary>tests/core/test_transformer_backwards_compatibility.py (1)</summary>\\n\\nLearnt from: nherment\\nPR: robusta-dev/holmesgpt#535\\nFile: holmes/plugins/toolsets/bash/bash_toolset.py:207-209\\nTimestamp: 2025-06-24T05:51:04.543Z\\nLearning: The init_config method in toolsets should be idempotent - safely callable multiple times without errors. self.config should maintain consistent typing (not alternate between dict and config object types) throughout the object lifecycle.\\n\\n</details>\\n\\n</details><details>\\n<summary>\\ud83e\\uddec Code Graph Analysis (2)</summary>\\n\\n<details>\\n<summary>tests/integration/test_kubernetes_transformer_execution.py (7)</summary><blockquote>\\n\\n<details>\\n<summary>holmes/plugins/toolsets/__init__.py (1)</summary>\\n\\n* `load_toolsets_from_file` (48-62)\\n\\n</details>\\n<details>\\n<summary>holmes/core/tools.py (3)</summary>\\n\\n* `StructuredToolResult` (51-74)\\n* `ToolResultStatus` (29-48)\\n* `invoke` (162-184)\\n\\n</details>\\n<details>\\n<summary>holmes/core/transformers/base.py (4)</summary>\\n\\n* `BaseTransformer` (17-84)\\n* `transform` (36-49)\\n* `should_apply` (52-62)\\n* `name` (77-84)\\n\\n</details>\\n<details>\\n<summary>holmes/core/transformers/llm_summarize.py (3)</summary>\\n\\n* `transform` (109-152)\\n* `should_apply` (79-107)\\n* `name` (155-157)\\n\\n</details>\\n<details>\\n<summary>tests/core/test_tool_transformers.py (12)</summary>\\n\\n* `transform` (32-33)\\n* `transform` (465-466)\\n* `transform` (512-513)\\n* `transform` (555-556)\\n* `should_apply` (35-36)\\n* `should_apply` (468-469)\\n* `should_apply` (515-516)\\n* `should_apply` (558-560)\\n* `test_transformer_failure_handling` (460-506)\\n* `FailingTransformer` (464-469)\\n* `test_multiple_transformers_chaining` (508-549)\\n* `SecondTransformer` (511-516)\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_transformers.py (7)</summary>\\n\\n* `transform` (14-15)\\n* `transform` (32-33)\\n* `transform` (43-44)\\n* `should_apply` (17-18)\\n* `should_apply` (35-37)\\n* `should_apply` (46-47)\\n* `FailingTransformer` (40-47)\\n\\n</details>\\n<details>\\n<summary>holmes/core/transformers/registry.py (3)</summary>\\n\\n* `register` (20-39)\\n* `is_registered` (83-93)\\n* `unregister` (41-54)\\n\\n</details>\\n\\n</blockquote></details>\\n<details>\\n<summary>tests/core/transformers/test_llm_summarize.py (2)</summary><blockquote>\\n\\n<details>\\n<summary>holmes/core/transformers/llm_summarize.py (4)</summary>\\n\\n* `LLMSummarizeTransformer` (14-157)\\n* `name` (155-157)\\n* `should_apply` (79-107)\\n* `transform` (109-152)\\n\\n</details>\\n<details>\\n<summary>holmes/core/transformers/base.py (4)</summary>\\n\\n* `TransformerError` (11-14)\\n* `name` (77-84)\\n* `should_apply` (52-62)\\n* `transform` (36-49)\\n\\n</details>\\n\\n</blockquote></details>\\n\\n</details><details>\\n<summary>\\ud83e\\ude9b Ruff (0.12.2)</summary>\\n\\n<details>\\n<summary>tests/integration/test_kubernetes_transformer_execution.py</summary>\\n\\n509-509: Local variable `result` is assigned to but never used\\n\\nRemove assignment to unused variable `result`\\n\\n(F841)\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_llm_summarize.py</summary>\\n\\n405-405: Use `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\\n\\n(E721)\\n\\n</details>\\n<details>\\n<summary>tests/core/test_tool_transformers.py</summary>\\n\\n656-656: Local variable `result` is assigned to but never used\\n\\nRemove assignment to unused variable `result`\\n\\n(F841)\\n\\n</details>\\n<details>\\n<summary>holmes/core/tools.py</summary>\\n\\n145-149: Use a single `if` statement instead of nested `if` statements\\n\\n(SIM102)\\n\\n---\\n\\n238-238: Local variable `size_change` is assigned to but never used\\n\\nRemove assignment to unused variable `size_change`\\n\\n(F841)\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_transformers.py</summary>\\n\\n25-29: Use a single `if` statement instead of nested `if` statements\\n\\nCombine `if` statements using `and`\\n\\n(SIM102)\\n\\n</details>\\n<details>\\n<summary>holmes/core/toolset_manager.py</summary>\\n\\n126-126: Undefined name `enabled_toolsets`\\n\\n(F821)\\n\\n---\\n\\n129-129: Undefined name `enabled_toolsets`\\n\\n(F821)\\n\\n</details>\\n<details>\\n<summary>holmes/core/transformers/base.py</summary>\\n\\n64-74: `BaseTransformer._validate_config` is an empty method in an abstract base class, but has no abstract decorator\\n\\n(B027)\\n\\n</details>\\n<details>\\n<summary>tests/core/test_transformer_backwards_compatibility.py</summary>\\n\\n94-97: Use a single `with` statement with multiple contexts instead of nested `with` statements\\n\\nCombine `with` statements\\n\\n(SIM117)\\n\\n</details>\\n\\n</details>\\n<details>\\n<summary>\\ud83e\\ude9b markdownlint-cli2 (0.17.2)</summary>\\n\\n<details>\\n<summary>docs/transformers.md</summary>\\n\\n119-119: Fenced code blocks should have a language specified\\n\\n(MD040, fenced-code-language)\\n\\n---\\n\\n151-151: Fenced code blocks should have a language specified\\n\\n(MD040, fenced-code-language)\\n\\n---\\n\\n161-161: Fenced code blocks should have a language specified\\n\\n(MD040, fenced-code-language)\\n\\n---\\n\\n171-171: Fenced code blocks should have a language specified\\n\\n(MD040, fenced-code-language)\\n\\n---\\n\\n181-181: Fenced code blocks should have a language specified\\n\\n(MD040, fenced-code-language)\\n\\n</details>\\n\\n</details>\\n<details>\\n<summary>\\ud83e\\ude9b YAMLlint (1.37.1)</summary>\\n\\n<details>\\n<summary>holmes/plugins/toolsets/aks-node-health.yaml</summary>\\n\\n[error] 10-10: trailing spaces\\n\\n(trailing-spaces)\\n\\n</details>\\n<details>\\n<summary>holmes/plugins/toolsets/aks.yaml</summary>\\n\\n[error] 10-10: trailing spaces\\n\\n(trailing-spaces)\\n\\n</details>\\n\\n</details>\\n\\n</details>\\n\\n<details>\\n<summary>\\u23f0 Context from checks skipped due to timeout of 90000ms. You can increase the timeout in your CodeRabbit configuration to a maximum of 15 minutes (900000ms). (1)</summary>\\n\\n* GitHub Check: Pre-commit checks\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83d\\udd07 Additional comments (27)</summary><blockquote>\\n\\n<details>\\n<summary>holmes/plugins/toolsets/aks-node-health.yaml (4)</summary>\\n\\n`20-29`: **LGTM! Well-structured transformer configuration for node status.**\\n\\nThe `llm_summarize` configuration is appropriate with a reasonable threshold and a focused prompt that prioritizes problematic nodes while preserving important details like node names.\\n\\n---\\n\\n`35-45`: **LGTM! Comprehensive transformer configuration for node description.**\\n\\nThe higher threshold of 1200 characters is appropriate for verbose node descriptions, and the prompt effectively covers all critical troubleshooting aspects.\\n\\n---\\n\\n`61-71`: **LGTM! Effective transformer configuration for activity logs.**\\n\\nThe 1500 character threshold is appropriate for potentially verbose activity logs, and the prompt effectively focuses on operational issues while preserving crucial identifiers like correlation IDs.\\n\\n---\\n\\n`102-112`: **LGTM! Practical transformer configuration for VMSS commands.**\\n\\nThe configuration effectively handles shell command outputs with appropriate focus on execution status, errors, and actionable findings.\\n\\n</details>\\n<details>\\n<summary>holmes/core/transformers/base.py (2)</summary>\\n\\n`64-74`: **Consider the design choice for `_validate_config` method.**\\n\\nThe static analysis tool correctly identifies that `_validate_config` is an empty method in an abstract base class without the `@abstractmethod` decorator. However, this appears to be an intentional design choice to provide an optional validation hook that subclasses can override only when needed, rather than forcing all implementations to define it.\\n\\nThis pattern is reasonable and provides flexibility. If you want to make the intent clearer, consider adding a comment in the docstring like \\\"Subclasses may override this method to add validation logic.\\\"\\n\\n---\\n\\n`1-84`: **LGTM! Well-designed abstract base class for transformers.**\\n\\nThe `BaseTransformer` class provides a clean and extensible interface for tool output transformers with:\\n- Clear abstract methods for transformation logic\\n- Optional configuration support\\n- Sensible defaults for the name property\\n- Comprehensive docstrings\\n\\n</details>\\n<details>\\n<summary>holmes/plugins/toolsets/aks.yaml (4)</summary>\\n\\n`25-36`: **LGTM! Comprehensive transformer configuration for AKS cluster details.**\\n\\nThe 1500 character threshold is appropriate for detailed cluster configurations, and the prompt effectively covers all critical aspects including status, networking, security, and potential issues.\\n\\n---\\n\\n`42-52`: **LGTM! Effective transformer configuration for cluster listings.**\\n\\nThe configuration appropriately handles multiple clusters with focus on comparison, version consistency, and error states. The grouping approach is particularly useful for large cluster lists.\\n\\n---\\n\\n`58-69`: **LGTM! Well-structured transformer configuration for node pools.**\\n\\nThe configuration effectively summarizes node pool information with appropriate focus on scaling, resource allocation, and configuration details that matter for troubleshooting.\\n\\n---\\n\\n`126-137`: **LGTM! Security-focused transformer configuration for NSG rules.**\\n\\nThe configuration effectively prioritizes security concerns with appropriate focus on permissive rules and AKS connectivity impact. The distinction between default and custom rules is particularly useful.\\n\\n</details>\\n<details>\\n<summary>tests/core/test_transformer_backwards_compatibility.py (1)</summary>\\n\\n`1-254`: **LGTM! Comprehensive backwards compatibility test suite.**\\n\\nThis test suite effectively ensures that the new transformer feature maintains backwards compatibility with existing configurations and code. The tests cover all critical paths including Config loading, Toolset/Tool creation, and ToolsetManager initialization.\\n\\n</details>\\n<details>\\n<summary>tests/integration/test_kubernetes_transformer_execution.py (1)</summary>\\n\\n`1-528`: **LGTM! Excellent integration test coverage for transformer execution.**\\n\\nThis test suite provides comprehensive integration testing of the transformer feature with Kubernetes tools, covering:\\n- Large vs small output handling\\n- Error scenarios and graceful failure handling  \\n- Multiple transformer chaining\\n- Performance metrics logging\\n- Real YAML toolset loading and execution\\n\\nThe tests effectively validate the end-to-end transformer pipeline.\\n\\n</details>\\n<details>\\n<summary>holmes/core/toolset_manager.py (3)</summary>\\n\\n`5-5`: **LGTM! Clean implementation of global transformer config support.**\\n\\nThe import additions and constructor parameter are properly typed and follow the existing patterns in the codebase.\\n\\n\\n\\nAlso applies to: 14-14, 32-32, 36-36\\n\\n---\\n\\n`281-284`: **Good placement of global config application for CLI toolsets.**\\n\\nApplying global transformer configs to CLI custom toolsets ensures consistent behavior across all toolset types.\\n\\n---\\n\\n`448-471`: **Well-designed configuration inheritance implementation.**\\n\\nThe method correctly implements the configuration precedence hierarchy (tool > toolset > global) and handles edge cases gracefully. The two-phase merging ensures tools inherit updated toolset configs after global merge.\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_llm_summarize.py (1)</summary>\\n\\n`29-320`: **Excellent test coverage for LLMSummarizeTransformer.**\\n\\nThe tests comprehensively cover:\\n- Initialization with various configurations\\n- Config validation for all parameters\\n- Conditional application based on thresholds\\n- Success and error scenarios\\n- Prompt construction and customization\\n- Edge cases like empty/whitespace responses\\n\\n</details>\\n<details>\\n<summary>tests/core/test_config_transformers.py (2)</summary>\\n\\n`77-182`: **Well-structured tests for configuration inheritance.**\\n\\nThe tests correctly verify:\\n- Global configs are applied only when toolset configs are absent\\n- Existing toolset/tool configs are preserved (not overridden)\\n- Tool inheritance is properly delegated to `Toolset.preprocess_tools()`\\n\\nThe clear comments explaining the inheritance behavior are particularly helpful.\\n\\n---\\n\\n`214-265`: **Excellent integration test for configuration inheritance priority.**\\n\\nThe test comprehensively validates the complete inheritance chain (global \\u2192 toolset \\u2192 tool) with clear assertions for each scenario. The test structure makes the expected behavior very clear.\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_transformers.py (1)</summary>\\n\\n`50-329`: **Comprehensive test coverage for transformer infrastructure.**\\n\\nThe test suite provides excellent coverage:\\n- Abstract method enforcement\\n- Registry operations (register, unregister, create, list)\\n- Error handling and validation\\n- Instance isolation\\n- Integration scenarios\\n\\nThe mock transformers (`MockTransformer`, `ThresholdTransformer`, `FailingTransformer`) effectively test different aspects of the transformer system.\\n\\n</details>\\n<details>\\n<summary>tests/core/transformers/test_validation.py (8)</summary>\\n\\n`1-17`: **LGTM!**\\n\\nImport structure is clean and follows Python conventions with stdlib imports first, followed by project imports.\\n\\n---\\n\\n`19-43`: **Well-designed mock transformers for testing!**\\n\\nThe mock classes effectively simulate both valid and invalid configuration scenarios needed for comprehensive validation testing.\\n\\n---\\n\\n`45-53`: **LGTM!**\\n\\nSimple and effective test for the custom exception class.\\n\\n---\\n\\n`55-128`: **Comprehensive test coverage for transformer config validation!**\\n\\nExcellent use of setup/teardown methods for test isolation and thorough coverage of edge cases including type errors, empty configs, and validation failures.\\n\\n---\\n\\n`130-185`: **LGTM!**\\n\\nWell-structured tests for list validation with appropriate coverage of edge cases and error index reporting.\\n\\n---\\n\\n`187-215`: **LGTM!**\\n\\nFocused test coverage for tool-specific transformer validation with proper error context verification.\\n\\n---\\n\\n`217-263`: **Excellent testing of safe validation with logging verification!**\\n\\nGood use of mocking to verify that warnings are properly logged with relevant context when validation fails.\\n\\n---\\n\\n`265-322`: **Well-designed integration tests!**\\n\\nExcellent coverage of complex scenarios and error propagation through the validation stack.\\n\\n</details>\\n\\n</blockquote></details>\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit for review status -->\",\n          \"**Actionable comments posted: 1**\\n\\n<details>\\n<summary>\\ud83d\\udcdc Review details</summary>\\n\\n**Configuration used: CodeRabbit UI**\\n**Review profile: CHILL**\\n**Plan: Pro**\\n\\n\\n<details>\\n<summary>\\ud83d\\udce5 Commits</summary>\\n\\nReviewing files that changed from the base of the PR and between 9e6f7e72855d0e160c72dae5e6163c9d7cdfd6d0 and f62d3a576ebc65ba23b992f59e2750bfbe1a98d7.\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83d\\udcd2 Files selected for processing (6)</summary>\\n\\n* `ocode_python/tools/bash_tool.py` (2 hunks)\\n* `ocode_python/tools/git_tools.py` (4 hunks)\\n* `ocode_python/utils/path_validator.py` (1 hunks)\\n* `tests/performance/test_performance.py` (1 hunks)\\n* `tests/unit/test_security.py` (1 hunks)\\n* `tests/unit/test_windows_compatibility.py` (6 hunks)\\n\\n</details>\\n\\n<details>\\n<summary>\\u2705 Files skipped from review due to trivial changes (1)</summary>\\n\\n* ocode_python/utils/path_validator.py\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83d\\udea7 Files skipped from review as they are similar to previous changes (4)</summary>\\n\\n* ocode_python/tools/git_tools.py\\n* tests/performance/test_performance.py\\n* tests/unit/test_security.py\\n* ocode_python/tools/bash_tool.py\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83e\\uddf0 Additional context used</summary>\\n\\n<details>\\n<summary>\\ud83e\\ude9b Pylint (3.3.7)</summary>\\n\\n<details>\\n<summary>tests/unit/test_windows_compatibility.py</summary>\\n\\n[warning] 192-192: Unused argument 'mock_chmod'\\n\\n(W0613)\\n\\n</details>\\n\\n</details>\\n\\n</details>\\n\\n<details>\\n<summary>\\ud83d\\udd07 Additional comments (6)</summary><blockquote>\\n\\n<details>\\n<summary>tests/unit/test_windows_compatibility.py (6)</summary>\\n\\n`21-23`: **Refactor unused platform mock parameter naming**  \\nRenaming the `mock_platform` parameter to `_mock_platform` clearly signals it\\u2019s intentionally unused, aligning with PEP8 conventions.\\n\\n---\\n\\n`42-47`: **Align PowerShell invocation with new bypass policy and command wrapping**  \\nThe test correctly reflects the updated `-ExecutionPolicy Bypass` and wraps the PowerShell command in `& { ... }`, matching the production changes in `BashTool._prepare_shell_command`.\\n\\n---\\n\\n`54-55`: **Refactor platform mock naming in Windows execution test**  \\nRenaming the parameter to `_mock_platform` for consistency and clarity is a good update.\\n\\n---\\n\\n`75-76`: **Refactor platform mock naming in process termination test**  \\nUsing `_mock_platform` makes it clear that the platform mock is unused in the test body.\\n\\n---\\n\\n`151-152`: **Refactor platform mock naming in sanitizer test**  \\nUpdating to `_mock_platform` communicates that the platform mock isn\\u2019t used in this test.\\n\\n---\\n\\n`166-167`: **Refactor platform mock naming in script execution no-chmod test**  \\nChanging to `_mock_platform` aligns with the unused mock naming convention.\\n\\n</details>\\n\\n</blockquote></details>\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit for review status -->\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Github token cycling & safe_get functions"
      ],
      "metadata": {
        "id": "K8VxcWjshui0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "818abba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "6cb1db81-71e0-4c87-efa9-6453cd056a1c"
      },
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "GITHUB_TOKENS = [\n",
        "    os.environ.get(\"GITHUB_TOKEN\"),\n",
        "    userdata.get('GITHUB_TOKEN_2'),\n",
        "    userdata.get('GITHUB_TOKEN_3'),\n",
        "    userdata.get('GITHUB_TOKEN_4'),\n",
        "    userdata.get('GITHUB_TOKEN_5'),\n",
        "    userdata.get('GITHUB_TOKEN_6'),\n",
        "    userdata.get('GITHUB_TOKEN_7'),\n",
        "]\n",
        "# Filter out any None values if a token is not set\n",
        "GITHUB_TOKENS = [token for token in GITHUB_TOKENS if token is not None]\n",
        "\n",
        "if not GITHUB_TOKENS:\n",
        "    raise RuntimeError(\"‚ùå No GitHub tokens found. Please add your tokens to Colab Secrets.\")\n",
        "\n",
        "# Create a cycle iterator for the tokens\n",
        "token_cycle = cycle(GITHUB_TOKENS)\n",
        "\n",
        "def safe_get(url, sleep_base=1.0, retries=3):\n",
        "    \"\"\"GET with retry, error handling, and rate-limit backoff, cycling through tokens.\"\"\"\n",
        "    current_token = next(token_cycle) # Get the next token in the cycle\n",
        "    headers = {'Authorization': f'token {current_token}', # Use the current token\n",
        "               'Accept': 'application/vnd.github+json'}\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            r = requests.get(url, headers=headers)\n",
        "            # --- Rate limit exceeded ---\n",
        "            if r.status_code == 403 and 'X-RateLimit-Remaining' in r.headers and r.headers['X-RateLimit-Remaining'] == '0':\n",
        "                reset_time = int(r.headers.get('X-RateLimit-Reset', time.time()+60))\n",
        "                sleep_for = max(reset_time - time.time(), 30)\n",
        "                print(f\"[RATE-LIMIT] Sleeping {sleep_for/60:.1f} min until reset ‚Ä¶\")\n",
        "                time.sleep(sleep_for + 1)\n",
        "                current_token = next(token_cycle) # Switch to the next token\n",
        "                headers['Authorization'] = f'token {current_token}' # Update header with new token\n",
        "                continue\n",
        "\n",
        "            # --- Success / known cases ---\n",
        "            if r.status_code in (200, 204, 404):\n",
        "                return r\n",
        "            else:\n",
        "                print(f\"[WARN] {r.status_code} on {url}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"[ERROR] Request failed: {e}\")\n",
        "        # --- backoff ---\n",
        "        time.sleep(sleep_base * (2 ** attempt) + random.uniform(0, 1))\n",
        "    return None\n",
        "\n",
        "def handle_graphql_errors(data, context_info=None):\n",
        "    \"\"\"Handles GraphQL errors returned in the response body, including rate limits.\"\"\"\n",
        "    if 'errors' in data:\n",
        "        error_message = f\"GraphQL errors for {context_info or 'request'}: {data['errors']}\"\n",
        "        print(f\"[GRAPHQL ERROR] {error_message}\")\n",
        "        for error in data['errors']:\n",
        "            if error.get('type') == 'RATE_LIMITED':\n",
        "                 print(\"[RATE-LIMITED] Retrying after a delay...\")\n",
        "                 time.sleep(60) # Wait for a minute before retrying\n",
        "                 # Note: Token cycling for GraphQL is handled in the calling function (e.g., get_author_association)\n",
        "                 return 'RATE_LIMITED' # Indicate rate limit error\n",
        "        return 'ERROR' # Indicate other GraphQL error\n",
        "    return None # No GraphQL errors\n",
        "\n",
        "async def get_repo_data(repo_url: str):\n",
        "    # Make the Request using safe_get\n",
        "    print(f'Requesting: {repo_url}')\n",
        "    # Use safe_get which includes token cycling and rate limit handling\n",
        "    response = safe_get(repo_url)\n",
        "\n",
        "    if response is None:\n",
        "        print(f\"Error: Failed to retrieve data for {repo_url} after multiple retries.\")\n",
        "        return None\n",
        "\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def get_repo_created_at(repo_url: str):\n",
        "    try:\n",
        "        task = asyncio.create_task(get_repo_data(repo_url))\n",
        "        event_loop = asyncio.get_running_loop()\n",
        "        if event_loop.is_running():\n",
        "          data = event_loop.run_until_complete(task)\n",
        "        else:\n",
        "          data = asyncio.run(task)\n",
        "        if data is None:\n",
        "            return None\n",
        "\n",
        "        created_at = data['created_at']\n",
        "        print(f\"Repo: {repo_url}; Created At: {created_at}\")\n",
        "\n",
        "        if created_at:\n",
        "            return pd.to_datetime(created_at)\n",
        "        else:\n",
        "            raise Exception(f\"Error: Could not retrieve createdAt for {repo_url}. Response data: {data}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during GitHub API request for {repo_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GITHUB_TOKEN_2 does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1875095686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m GITHUB_TOKENS = [\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GITHUB_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GITHUB_TOKEN_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GITHUB_TOKEN_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GITHUB_TOKEN_4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GITHUB_TOKEN_2 does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Copy of Pull_Requests & Data Cleaning\n",
        "\n",
        "1. **Copies DataFrames:** It creates copies of the pull_request and repository DataFrames and assigns them to metrics and repos respectively. This is a good practice to avoid modifying the original loaded data.\n",
        "2. **Renames Columns:** It renames the 'id' column to 'pr_id' in the metrics DataFrame and to 'repo_id' in the repos DataFrame. This is done to prepare for merging these DataFrames later.\n",
        "3. **Filters Open Pull Requests:** It removes pull requests that are still open by filtering out rows where the 'closed_at' column has a missing value (NaN).\n",
        "4. **Converts Timestamps:** It converts the 'created_at' and 'closed_at' columns in the metrics DataFrame to datetime objects. This allows for easier time-based calculations.\n",
        "5. **Filters Repositories:** It removes repositories from the repos DataFrame that do not have any closed pull requests in the metrics DataFrame.\n",
        "6. **Gets Repository Creation Dates:** For the remaining repositories, it calls the get_repo_created_at function (defined in a previous cell) to fetch the creation date of each repository from the GitHub API and stores it in a new column 'repo_created_at'.\n",
        "7. **Filters Repositories with Creation Dates:** It removes repositories where the 'repo_created_at' could not be retrieved."
      ],
      "metadata": {
        "id": "67cBtszBeJC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, random, requests, pandas as pd\n",
        "from datetime import datetime\n",
        "from itertools import cycle # Import cycle\n",
        "\n",
        "# Copy raw data\n",
        "metrics = pull_request.copy()\n",
        "repos = repository.copy()\n",
        "\n",
        "# === BASIC CLEANUP ===\n",
        "metrics = metrics.rename(columns={'id': 'pr_id'})\n",
        "repos = repos.rename(columns={'id': 'repo_id'})\n",
        "\n",
        "print(f\"Total PRs before filtering: {len(metrics):,}\")\n",
        "metrics = metrics[metrics['closed_at'].notna()]\n",
        "print(f\"Closed PRs retained: {len(metrics):,}\")\n",
        "\n",
        "metrics['created_at'] = pd.to_datetime(metrics['created_at'], errors='coerce')\n",
        "metrics['closed_at'] = pd.to_datetime(metrics['closed_at'], errors='coerce')\n",
        "\n",
        "print(f\"Total repos before filtering: {len(repos):,}\")\n",
        "repos = repos[repos['repo_id'].isin(metrics['repo_id'])]\n",
        "print(f\"Repos with ‚â•1 PR: {len(repos):,}\")\n",
        "\n",
        "# === ADD: SMALL-SUBSET TEST MODE ===\n",
        "# Toggle to True for testing on limited data before full run\n",
        "TEST_MODE = True\n",
        "if TEST_MODE:\n",
        "    # Select the next 500 repos (from index 500 to 1000)\n",
        "    start_index = 500\n",
        "    end_index = 1000\n",
        "    if end_index > len(repos):\n",
        "        end_index = len(repos)\n",
        "        print(f\"[TEST MODE] Adjusting end index to {end_index} as it exceeds the number of repos.\")\n",
        "\n",
        "    test_repo_ids = repos['repo_id'].iloc[start_index:end_index]\n",
        "\n",
        "    metrics = metrics[metrics['repo_id'].isin(test_repo_ids)]\n",
        "    repos = repos[repos['repo_id'].isin(test_repo_ids)]\n",
        "    print(f\"[TEST MODE] Restricting to {len(repos)} repos and {len(metrics)} PRs (indices {start_index} to {end_index-1})\")\n",
        "\n",
        "\n",
        "# The safe_get function and token setup have been moved to the cell above\n",
        "# to ensure they are defined before being called."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKaF2xpBcvJd",
        "outputId": "e1ebf5bc-663d-4452-af19-61ce5f7ed5f1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total PRs before filtering: 932,791\n",
            "Closed PRs retained: 859,927\n",
            "Total repos before filtering: 116,211\n",
            "Repos with ‚â•1 PR: 91,526\n",
            "[TEST MODE] Restricting to 500 repos and 3239 PRs (indices 500 to 999)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(metrics.head())\n",
        "display(repos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "collapsed": true,
        "id": "ODq_0xOtetTr",
        "outputId": "7d968ee1-13d9-4e0f-e017-477354528481"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         pr_id  number                                              title  \\\n",
              "2   3264042289    1600  Add Evals frontend implementation plan and HTM...   \n",
              "3   3264042318    1601  Add 4 new BfDs components for Evals interface ...   \n",
              "5   3264372403       1  Comprehensive LMSR Markets System with Weekly ...   \n",
              "12  3264552777      54  Issue #47: Complete End-to-End Integration Tes...   \n",
              "18  3264677066    1606  Document codebot networking implementation str...   \n",
              "\n",
              "                                                 body        agent   user_id  \\\n",
              "2   \\nCreate comprehensive implementation plan for...  Claude_Code   6766889   \n",
              "3   \\nPhase 1 component creation for the Evals fro...  Claude_Code   6766889   \n",
              "5   ## Summary\\nüöÄ **Major platform upgrade**: Comp...  Claude_Code  62402155   \n",
              "12  ## Summary\\n\\nThis PR implements comprehensive...  Claude_Code   1257915   \n",
              "18  \\nAdd implementation memo outlining the curren...  Claude_Code    448694   \n",
              "\n",
              "          user   state                created_at                 closed_at  \\\n",
              "2    justicart  closed 2025-07-25 18:26:15+00:00 2025-07-25 23:19:14+00:00   \n",
              "3    justicart  closed 2025-07-25 18:26:16+00:00 2025-07-25 23:19:11+00:00   \n",
              "5   derspotter  closed 2025-07-25 20:59:39+00:00 2025-07-28 12:01:05+00:00   \n",
              "12  datablogin  closed 2025-07-25 22:39:02+00:00 2025-07-25 23:29:29+00:00   \n",
              "18    randallb  closed 2025-07-25 23:52:25+00:00 2025-07-26 00:28:14+00:00   \n",
              "\n",
              "               merged_at       repo_id  \\\n",
              "2                   None  9.267118e+08   \n",
              "3                   None  9.267118e+08   \n",
              "5   2025-07-28T12:01:05Z  9.249577e+08   \n",
              "12  2025-07-25T23:29:29Z  1.022864e+09   \n",
              "18  2025-07-26T00:28:14Z  9.267118e+08   \n",
              "\n",
              "                                             repo_url  \\\n",
              "2   https://api.github.com/repos/bolt-foundry/bolt...   \n",
              "3   https://api.github.com/repos/bolt-foundry/bolt...   \n",
              "5   https://api.github.com/repos/derspotter/intell...   \n",
              "12  https://api.github.com/repos/datablogin/analyt...   \n",
              "18  https://api.github.com/repos/bolt-foundry/bolt...   \n",
              "\n",
              "                                             html_url  \n",
              "2   https://github.com/bolt-foundry/bolt-foundry/p...  \n",
              "3   https://github.com/bolt-foundry/bolt-foundry/p...  \n",
              "5   https://github.com/derspotter/intellacc.com/pu...  \n",
              "12  https://github.com/datablogin/analytics-backen...  \n",
              "18  https://github.com/bolt-foundry/bolt-foundry/p...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d023ab6-d98d-4146-8606-342ec8b9333f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr_id</th>\n",
              "      <th>number</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>agent</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user</th>\n",
              "      <th>state</th>\n",
              "      <th>created_at</th>\n",
              "      <th>closed_at</th>\n",
              "      <th>merged_at</th>\n",
              "      <th>repo_id</th>\n",
              "      <th>repo_url</th>\n",
              "      <th>html_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3264042289</td>\n",
              "      <td>1600</td>\n",
              "      <td>Add Evals frontend implementation plan and HTM...</td>\n",
              "      <td>\\nCreate comprehensive implementation plan for...</td>\n",
              "      <td>Claude_Code</td>\n",
              "      <td>6766889</td>\n",
              "      <td>justicart</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-07-25 18:26:15+00:00</td>\n",
              "      <td>2025-07-25 23:19:14+00:00</td>\n",
              "      <td>None</td>\n",
              "      <td>9.267118e+08</td>\n",
              "      <td>https://api.github.com/repos/bolt-foundry/bolt...</td>\n",
              "      <td>https://github.com/bolt-foundry/bolt-foundry/p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3264042318</td>\n",
              "      <td>1601</td>\n",
              "      <td>Add 4 new BfDs components for Evals interface ...</td>\n",
              "      <td>\\nPhase 1 component creation for the Evals fro...</td>\n",
              "      <td>Claude_Code</td>\n",
              "      <td>6766889</td>\n",
              "      <td>justicart</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-07-25 18:26:16+00:00</td>\n",
              "      <td>2025-07-25 23:19:11+00:00</td>\n",
              "      <td>None</td>\n",
              "      <td>9.267118e+08</td>\n",
              "      <td>https://api.github.com/repos/bolt-foundry/bolt...</td>\n",
              "      <td>https://github.com/bolt-foundry/bolt-foundry/p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3264372403</td>\n",
              "      <td>1</td>\n",
              "      <td>Comprehensive LMSR Markets System with Weekly ...</td>\n",
              "      <td>## Summary\\nüöÄ **Major platform upgrade**: Comp...</td>\n",
              "      <td>Claude_Code</td>\n",
              "      <td>62402155</td>\n",
              "      <td>derspotter</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-07-25 20:59:39+00:00</td>\n",
              "      <td>2025-07-28 12:01:05+00:00</td>\n",
              "      <td>2025-07-28T12:01:05Z</td>\n",
              "      <td>9.249577e+08</td>\n",
              "      <td>https://api.github.com/repos/derspotter/intell...</td>\n",
              "      <td>https://github.com/derspotter/intellacc.com/pu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3264552777</td>\n",
              "      <td>54</td>\n",
              "      <td>Issue #47: Complete End-to-End Integration Tes...</td>\n",
              "      <td>## Summary\\n\\nThis PR implements comprehensive...</td>\n",
              "      <td>Claude_Code</td>\n",
              "      <td>1257915</td>\n",
              "      <td>datablogin</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-07-25 22:39:02+00:00</td>\n",
              "      <td>2025-07-25 23:29:29+00:00</td>\n",
              "      <td>2025-07-25T23:29:29Z</td>\n",
              "      <td>1.022864e+09</td>\n",
              "      <td>https://api.github.com/repos/datablogin/analyt...</td>\n",
              "      <td>https://github.com/datablogin/analytics-backen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3264677066</td>\n",
              "      <td>1606</td>\n",
              "      <td>Document codebot networking implementation str...</td>\n",
              "      <td>\\nAdd implementation memo outlining the curren...</td>\n",
              "      <td>Claude_Code</td>\n",
              "      <td>448694</td>\n",
              "      <td>randallb</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-07-25 23:52:25+00:00</td>\n",
              "      <td>2025-07-26 00:28:14+00:00</td>\n",
              "      <td>2025-07-26T00:28:14Z</td>\n",
              "      <td>9.267118e+08</td>\n",
              "      <td>https://api.github.com/repos/bolt-foundry/bolt...</td>\n",
              "      <td>https://github.com/bolt-foundry/bolt-foundry/p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d023ab6-d98d-4146-8606-342ec8b9333f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5d023ab6-d98d-4146-8606-342ec8b9333f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5d023ab6-d98d-4146-8606-342ec8b9333f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-59581302-8aa6-46c3-9e04-15b3dbdc4e27\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59581302-8aa6-46c3-9e04-15b3dbdc4e27')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-59581302-8aa6-46c3-9e04-15b3dbdc4e27 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        repo_id                                                url  \\\n",
              "550     7346775  https://api.github.com/repos/bendrucker/bendru...   \n",
              "552  1025441210  https://api.github.com/repos/berrylands/modal-...   \n",
              "553   952643099  https://api.github.com/repos/bigdra50/Material...   \n",
              "554   968101794  https://api.github.com/repos/binary-install/bi...   \n",
              "555   875493470           https://api.github.com/repos/bitsacco/os   \n",
              "\n",
              "         license                          full_name    language  forks  stars  \n",
              "550  NOASSERTION           bendrucker/bendrucker.me       Astro    0.0    0.0  \n",
              "552         None  berrylands/modal-meigen-multitalk      Python    0.0    1.0  \n",
              "553          MIT          bigdra50/MaterialCombiner          C#    0.0    0.0  \n",
              "554          MIT          binary-install/binstaller          Go    1.0    4.0  \n",
              "555          MIT                        bitsacco/os  TypeScript    4.0    9.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40516dbe-19f6-430a-9bf2-0d2a62f1ec25\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo_id</th>\n",
              "      <th>url</th>\n",
              "      <th>license</th>\n",
              "      <th>full_name</th>\n",
              "      <th>language</th>\n",
              "      <th>forks</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>550</th>\n",
              "      <td>7346775</td>\n",
              "      <td>https://api.github.com/repos/bendrucker/bendru...</td>\n",
              "      <td>NOASSERTION</td>\n",
              "      <td>bendrucker/bendrucker.me</td>\n",
              "      <td>Astro</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>1025441210</td>\n",
              "      <td>https://api.github.com/repos/berrylands/modal-...</td>\n",
              "      <td>None</td>\n",
              "      <td>berrylands/modal-meigen-multitalk</td>\n",
              "      <td>Python</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>952643099</td>\n",
              "      <td>https://api.github.com/repos/bigdra50/Material...</td>\n",
              "      <td>MIT</td>\n",
              "      <td>bigdra50/MaterialCombiner</td>\n",
              "      <td>C#</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>968101794</td>\n",
              "      <td>https://api.github.com/repos/binary-install/bi...</td>\n",
              "      <td>MIT</td>\n",
              "      <td>binary-install/binstaller</td>\n",
              "      <td>Go</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>875493470</td>\n",
              "      <td>https://api.github.com/repos/bitsacco/os</td>\n",
              "      <td>MIT</td>\n",
              "      <td>bitsacco/os</td>\n",
              "      <td>TypeScript</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40516dbe-19f6-430a-9bf2-0d2a62f1ec25')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-40516dbe-19f6-430a-9bf2-0d2a62f1ec25 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-40516dbe-19f6-430a-9bf2-0d2a62f1ec25');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-39d7d246-fd7e-4859-aeb4-43d0aa23b725\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-39d7d246-fd7e-4859-aeb4-43d0aa23b725')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-39d7d246-fd7e-4859-aeb4-43d0aa23b725 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(repos\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"repo_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 427356370,\n        \"min\": 7346775,\n        \"max\": 1025441210,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1025441210,\n          875493470,\n          952643099\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://api.github.com/repos/berrylands/modal-meigen-multitalk\",\n          \"https://api.github.com/repos/bitsacco/os\",\n          \"https://api.github.com/repos/bigdra50/MaterialCombiner\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"license\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"MIT\",\n          \"NOASSERTION\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"berrylands/modal-meigen-multitalk\",\n          \"bitsacco/os\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Python\",\n          \"TypeScript\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"forks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7320508075688772,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.8340579025361627,\n        \"min\": 0.0,\n        \"max\": 9.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          9.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive to save Dataset for all of our access"
      ],
      "metadata": {
        "id": "jA6SGbhQZhLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save initial processed Dataset (no need to re-run this every time)"
      ],
      "metadata": {
        "id": "sVs4gjRhZ4zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.to_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_cleaned.pkl\")\n",
        "repos.to_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_cleaned.pkl\")"
      ],
      "metadata": {
        "id": "WOunph6PZrOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOR TESTING\n",
        "metrics.to_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_TEST.pkl\")\n",
        "repos.to_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_TEST.pkl\")"
      ],
      "metadata": {
        "id": "qy077Rz2drsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload dataset (this can be ran every time)"
      ],
      "metadata": {
        "id": "YUGl3g1aaApG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "metrics = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_cleaned.pkl\")\n",
        "repos = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_cleaned.pkl\")\n",
        "print(\"‚úÖ Loaded shared cached cleaned dataset\")"
      ],
      "metadata": {
        "id": "KxAKR0tjaA7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOR TESTING\n",
        "import pandas as pd\n",
        "metrics = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_TEST.pkl\")\n",
        "repos = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_TEST.pkl\")\n",
        "print(\"‚úÖ Loaded TEST shared cached cleaned dataset\")"
      ],
      "metadata": {
        "id": "Uz6YPZpJdyTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we decide to update this dataset again or for frequent updates-- use this to prevent overwriting"
      ],
      "metadata": {
        "id": "8VrCLkuWaX_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "tag = date.today().isoformat()\n",
        "metrics.to_pickle(f\"/content/drive/MyDrive/AIDev_shared/metrics_{tag}.pkl\")\n",
        "repos.to_pickle(f\"/content/drive/MyDrive/AIDev_shared/metrics_{tag}.pkl\")"
      ],
      "metadata": {
        "id": "hfQlplV8aXD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Third**, Gather the covariant variables\n",
        "\n",
        "## PR Variables\n",
        "\n",
        "1. **additions:** The # of added LOC by a PR\n",
        "2. **deletions:** The # of deleted LOC by a PR\n",
        "3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)\n",
        "4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', ‚Ä¶).\n",
        "5. **changedFiles:** The # of files changed by a PR\n",
        "6. **commitsTotalCount:** The # of commits involved in a PR\n",
        "7. **bodyLength**: Length of the PR body (in characters).\n",
        "8. **prExperience:** The # of prior PRs that were submitted by the PR author (author‚Äôs prior PR count). Query the author‚Äôs PR history in the same repo and count PRs created before the current one.\n",
        "9. **commentsTotalCount:** The # of comments left on a PR\n",
        "10. **authorComments:** The # of comments left by the PR author\n",
        "11. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion\n",
        "12. **reviewersTotalCount:** The # of developers who participate in the discussion (excluding author).\n",
        "13. **repoAge:** Time interval between the repository creation time and PR creation time in days.\n",
        "14. **isMember:** Whether or not the author is a member or outside collaborator (True/False).\n",
        "15. **state**: State of the pull request (MERGED or CLOSED).\n",
        "16. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).\n",
        "\n",
        "## Project variables\n",
        "\n",
        "17. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue). *[I'm assuming its the top language as there is only one]*\n",
        "18. **forkCount:** The # of forks that a repository has\n",
        "19. **stargazerCount:** The # of stargazers that a repository has.\n",
        "\n",
        "## Treatment variables\n",
        "\n",
        "20. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)\n",
        "\n",
        "## Outcome variables\n",
        "\n",
        "21. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours\n",
        "22. **Is merged (state):** Whether or not a PR is merged (binary)\n",
        "\n"
      ],
      "metadata": {
        "id": "a30SFFhZXgy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PR Variables"
      ],
      "metadata": {
        "id": "xANcFKyCb9Fm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d51076"
      },
      "source": [
        "1. **additions:** The # of added LOC by a PR\n",
        "2. **deletions:** The # of deleted LOC by a PR\n",
        "3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "In the notebook (e.g., CollectCopilot4prs.ipynb), the `additions` and `deletions` values are extracted directly from the GitHub API response for each PR: `pr['node']['additions']` and `pr['node']['deletions']`. The GraphQL query for PRs includes the fields, so the value is as reported by GitHub. `prSize = additions + deletions`\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "In the `pr_commit_details` DataFrame, we use the `additions` and `deletions` fields. We sum them for `prSize`. Alternatively, the dataset also has`changes` which represents prSize but we chose to perform the sum ourselves.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['additions', 'deletions', 'prSize'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding LOC metrics: {len(metrics):,}\")\n",
        "\n",
        "# Get the sums of the columns we are interested in\n",
        "pr_commit_LOC = (pr_commit_details.groupby(['pr_id'])\n",
        "                                  .sum(['additions', 'deletions', 'changes'])\n",
        "                                  .reset_index())\n",
        "\n",
        "# Rename the sum columns to what we want\n",
        "pr_commit_LOC = (pr_commit_LOC.rename(columns={'changes': 'prSize'}))\n",
        "\n",
        "# Drop the extraneous columns\n",
        "pr_commit_LOC = pr_commit_LOC.drop(columns=['commit_stats_total', 'commit_stats_additions', 'commit_stats_deletions'])\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "metrics = pd.merge(metrics, pr_commit_LOC, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage collect the temporary Dataframe\n",
        "pr_commit_LOC = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['additions'] = metrics['additions'].fillna(0).astype(int)\n",
        "metrics['deletions'] = metrics['deletions'].fillna(0).astype(int)\n",
        "metrics['prSize'] = metrics['prSize'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding LOC metrics: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "nj9typbGIcI2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', ‚Ä¶).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "In `CollectCopilot4prs.ipynb`, the code uses `np.select` with conditions based on the PR's title and body content to assign \"Bug\", \"Document\", or \"Feature\" as the purpose. This is a simple rule-based classification:\n",
        "\n",
        "- If the title/body contains keywords for bugs (e.g., \"fix\", \"bug\"), it's labeled \"Bug\".\n",
        "- If it contains documentation keywords (e.g., \"doc\"), it's labeled \"Document\".\n",
        "- Otherwise, it's labeled \"Feature\".\n",
        "\n",
        "\n",
        "**Our approach:**\n",
        "\n",
        "The `title` and `body` columns are part of the initial dataset that was loaded into the pull_request (`all_pull_request.parquet`) DataFrame.\n"
      ],
      "metadata": {
        "id": "_9ULLRD1eVyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['purpose'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before calculating purpose: {len(metrics):,}\")\n",
        "\n",
        "# Combine title and body for keyword search, handling potential None values\n",
        "metrics['title_body'] = metrics['title'].fillna('') + ' ' + metrics['body'].fillna('')\n",
        "\n",
        "# Define conditions and choices for np.select\n",
        "conditions = [\n",
        "    metrics['title_body'].str.contains('fix|bug', case=False, na=False),\n",
        "    metrics['title_body'].str.contains('doc', case=False, na=False)\n",
        "]\n",
        "choices = ['fix', 'doc']\n",
        "\n",
        "# Apply np.select to determine purpose\n",
        "metrics['purpose'] = np.select(conditions, choices, default='feat')\n",
        "\n",
        "# Drop the temporary combined column\n",
        "metrics = metrics.drop(columns=['title_body'])\n",
        "\n",
        "print(f\"Number of PRs after calculating purpose: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "Hc2sYaVwFPdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02810ce"
      },
      "source": [
        "5. **changedFiles:** The # of files changed by a PR\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "The `changedFiles` field is extracted directly from the GitHub API for each pull request. In the code (e.g., `in CollectCopilot4prs.ipynb`), it is accessed as: `pr['node']['changedFiles']`.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "This variable is calculated from the `pr_commit_details` DataFrame, steps include:\n",
        "\n",
        "- Identify the PR ID: The code uses `groupby(['pr_id', 'filename'])` which implicitly identifies each PR by its `pr_id`.\n",
        "- Locate the file-level change records: It operates on the `pr_commit_details` DataFrame, which contains the file-level change records.\n",
        "- Collect all rows belonging to the same PR: The `groupby(['pr_id', 'filename'])` operation groups all rows for a specific PR together.\n",
        "- Count the number of unique filenames for each `pr_id` across all its commits."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['changedFiles'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding changedFiles: {len(metrics):,}\")\n",
        "\n",
        "# Count the number of Files changed and change the column name to what we want\n",
        "pr_files_changed = (pr_commit_details.groupby(['pr_id', 'filename'])\n",
        "                                     .size()\n",
        "                                     .groupby(['pr_id'])\n",
        "                                     .size()\n",
        "                                     .reset_index(name='changedFiles'))\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "metrics = pd.merge(metrics, pr_files_changed, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_files_changed = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['changedFiles'] = metrics['changedFiles'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding changedFiles: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "YwnUjWn9FPY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **commitsTotalCount:** The # of commits involved in a PR\n",
        "\n",
        "\n",
        "**Xiao, 2024:**\n",
        "\n",
        "Fetched from GitHub‚Äôs GraphQL API by querying the PR‚Äôs‚ÄØ`commits { totalCount }` field.\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "The `pr_commit_details` table contains a `sha` column (the commit hash) and a `pr_id` column that links each commit to its pull request. Count every distinct `sha` in the entire table. Group by `pr_id` and count distinct `sha` values for each group.\n"
      ],
      "metadata": {
        "id": "xxZIoTx8fiCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['commitsTotalCount'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding commitsTotalCount: {len(metrics):,}\")\n",
        "\n",
        "# Calculate the number of unique commits for each Pull Request from pr_commit_details\n",
        "# Group by pr_id and count the number of unique sha values\n",
        "pr_commits_count = pr_commit_details.groupby('pr_id')['sha'].nunique().reset_index(name='commitsTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_commits_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_commits_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['commitsTotalCount'] = metrics['commitsTotalCount'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding commitsTotalCount: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "vOyzUxnHFPUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **bodyLength:** The length of a PR description\n",
        "\n",
        "**Xiao, 2024:**\n",
        "\n",
        "Query each PR with `pullRequest { body }`. Take the returned text and compute its character count (e.g., len(body)). Record that count as the value of description length.\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "In the `pull_request` DataFrame, calculate the character length of the `body` column.\n"
      ],
      "metadata": {
        "id": "Ed6fS14afxfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['bodyLength'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding bodyLength: {len(metrics):,}\")\n",
        "\n",
        "# Get the Length of the Body of the Pull Request\n",
        "metrics['bodyLength'] = metrics['body'].str.len()\n",
        "\n",
        "print(f\"Number of PRs after adding bodyLength: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "OZYyz-mVFPPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **prExperience:** The # of prior PRs that were submitted by the PR author (author‚Äôs prior PR count). Query the author‚Äôs PR history in the same repo and count PRs created before the current one.\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "For every pull request the study queries GitHub‚Äôs GraphQL API and extracts the author.login (or author.id) and the repository identifier (repository.id). Using the same API they request all pull requests belonging to the same repository.id whose author.login matches the author of the target PR. Each of these PRs includes its createdAt timestamp. The list is filtered to keep only those PRs whose createdAt value is earlier than the createdAt timestamp of the target PR. The number of remaining PRs is taken as an integer count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "- Extract the author's login from the `user` column.\n",
        "- Sorts the metrics DataFrame by `repo_id`, `author_login`, and the PR creation time (`created_at`).\n",
        "- Groups the sorted DataFrame by both `repo_id` and `author_login`.\n",
        "- Within each group (for each unique author in each unique repository), it uses the `.cumcount()` method. `cumcount()` assigns a sequential number starting from 0 to each row within the group based on the current order (which is sorted by `created_at`).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JQyVdlFCf2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['prExperience'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding prExperience: {len(metrics):,}\")\n",
        "\n",
        "# Extract the author's login from the 'user' column and store it in a new column 'author_login'\n",
        "metrics['author_login'] = metrics['user'].astype(str).str.strip()\n",
        "\n",
        "# Drop rows where 'repo_id' or 'created_at' are missing, as these are needed for sorting and calculation\n",
        "metrics = metrics.dropna(subset=['repo_id', 'created_at'])\n",
        "\n",
        "# Sort the DataFrame by 'repo_id', 'author_login', and 'created_at' in ascending order--This is crucial for correctly calculating the cumulative count of PRs for each author within each repository.\n",
        "metrics = metrics.sort_values(['repo_id', 'author_login', 'created_at'])\n",
        "\n",
        "# Calculate the cumulative count of PRs for each author within each repository.\n",
        "# The `groupby(['repo_id', 'author_login'])` groups the DataFrame by repository and author.\n",
        "# The `cumcount()` method then calculates the number of previous PRs for each row within those groups.\n",
        "metrics['prExperience'] = (\n",
        "    metrics.groupby(['repo_id', 'author_login'])\n",
        "           .cumcount()\n",
        "           .astype('Int64')\n",
        ")\n",
        "\n",
        "print(f\"Number of PRs after adding bodyLength: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "iS8q-30-icdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **commentsTotalCount:** The # of comments left on a PR\n",
        "\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "`commentsTotalCount` is obtained directly from the GitHub GraphQL API. For each pull request it queries the PR‚Äôs `comments` connection and records the `totalCount` field, which is the number of comment objects attached to that PR (including review comments, issue‚Äëstyle comments, and any other discussion entries).\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Group the `pr_comments` DataFrame by `pr_id` (Pull Request ID). Each row in `pr_comments` represents a single comment.\n",
        "Count the number of rows within each group using `.size()`.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ac9ifXkVgLqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics.drop(columns=['commentsTotalCount'], errors='ignore', inplace=True)\n",
        "\n",
        "print(f\"Number of PRs before adding commentsTotalCount: {len(metrics):,}\")\n",
        "\n",
        "# Count the number of Comments for the Pull Request, name the column what we want.\n",
        "pr_comments_count = pr_comments.groupby(['pr_id']).size().reset_index(name='commentsTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['commentsTotalCount'] = metrics['commentsTotalCount'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding commentsTotalCount: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "2BNmsnGIFO9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **authorComments:** The # of comments left by the PR author\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "Query each PR‚Äôs comment data through the GitHub GraphQL API and then filter the comment list to keep only those whose `author.login` matches the PR author‚Äôs login.\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Merges `pr_comments` with author information and counting comments where the comment author matches the PR author."
      ],
      "metadata": {
        "id": "LoBAgwxOgaJn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "322b4128"
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['authorComments'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding authorComments: {len(metrics):,}\")\n",
        "\n",
        "# Filter comments to only include those made by the PR author\n",
        "# Need to merge with metrics to get the author_id for each pr_comment\n",
        "author_comments = pd.merge(pr_comments, metrics[['pr_id', 'user_id']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "author_comments = author_comments[author_comments['user_id_x'] == author_comments['user_id_y']]\n",
        "\n",
        "# Count the number of author comments per pull request\n",
        "author_comments_count = author_comments.groupby(['pr_id']).size().reset_index(name='authorComments')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, author_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "author_comments = None\n",
        "author_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['authorComments'] = metrics['authorComments'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding authorComments: {len(metrics):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion\n",
        "\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "Querying the `comments` edge of each PR via GraphQL (or the REST endpoint `GET /repos/{owner}/{repo}/issues/{pull_number}/comments`). Filtering out comments whose user.login equals the PR author‚Äôs login. Counting the remaining comments ‚Äì that number is the reviewerComments value.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Merges `pr_comments` with author information and counting comments where the comment author does not match the PR author.\n",
        "\n"
      ],
      "metadata": {
        "id": "0cC_Qh-DgvEJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "454479f7"
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewersComments'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding reviewersComments: {len(metrics):,}\")\n",
        "\n",
        "# Filter comments to exclude those made by the PR author\n",
        "# Need to merge with metrics to get the author_id for each pr_comment\n",
        "reviewer_comments = pd.merge(pr_comments, metrics[['pr_id', 'user_id']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "reviewer_comments = reviewer_comments[reviewer_comments['user_id_x'] != reviewer_comments['user_id_y']]\n",
        "\n",
        "# Count the number of reviewer comments per pull request\n",
        "reviewer_comments_count = reviewer_comments.groupby(['pr_id']).size().reset_index(name='reviewersComments')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, reviewer_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "reviewer_comments = None\n",
        "reviewer_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['reviewersComments'] = metrics['reviewersComments'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding reviewersComments: {len(metrics):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **reviewersTotalCount:** The # of developers who participate in the discussion (excluding author).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "Calling the GraphQL endpoint for each PR,\n",
        "Requesting the `reviewers` (or `reviewRequests`) connection,\n",
        "Reading the `totalCount` field,\n",
        "Verifying that the author‚Äôs login is excluded (or simply using the `totalCount` as GitHub already excludes the author in the `reviewers` connection).\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Calculates `reviewersTotalCount` by summing the counts of unique reviewers from reviews (`reviewers`) for each pull request.\n",
        "\n"
      ],
      "metadata": {
        "id": "sDtcHmxxg9MP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b294946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "0835dc3f-6027-40a4-ed58-f7c518c029c6"
      },
      "source": [
        "# Ensure we don't crash if columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewersTotalCount'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding reviewersTotalCount: {len(metrics):,}\")\n",
        "\n",
        "# Create a copy of PR reviews with only the necessary columns\n",
        "pr_reviews_temp = pr_reviews.copy().drop(columns=['id', 'state', 'submitted_at', 'body', 'user_type'], errors='ignore')\n",
        "\n",
        "# Filter reviews to exclude those made by the PR author\n",
        "# Merge with metrics to get the author_id for each pr_comment\n",
        "pr_reviews_temp = pd.merge(pr_reviews_temp, metrics[['pr_id', 'user']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "pr_reviews_temp = pr_reviews_temp[pr_reviews_temp['user_x'] != pr_reviews_temp['user_y']]\n",
        "\n",
        "# Count the number of unique non-author users who left review comments per pull request\n",
        "# Group by 'pr_id' to count per pull request\n",
        "pr_reviews_temp = pr_reviews_temp.groupby(['pr_id'])['user_x'].nunique().reset_index(name='reviewersTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_reviews_temp, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "pr_reviews_temp = None\n",
        "\n",
        "# Fill N/A values with defaults (for PRs with no reviews/comments)\n",
        "metrics['reviewersTotalCount'] = metrics['reviewersTotalCount'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding reviewersTotalCount: {len(metrics):,}\")\n",
        "\n",
        "\n",
        "# Display unique values and their counts for 'reviewersTotalCount'\n",
        "print(\"\\nUnique values and counts for 'reviewersTotalCount':\")\n",
        "display(metrics['reviewersTotalCount'].value_counts(dropna=False).sort_index())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PRs before adding reviewersTotalCount: 3,239\n",
            "Number of PRs after adding reviewersTotalCount: 3,239\n",
            "\n",
            "Unique values and counts for 'reviewersTotalCount':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "reviewersTotalCount\n",
              "0    3002\n",
              "1     111\n",
              "2      61\n",
              "3      57\n",
              "4       8\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewersTotalCount</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **repoAge:** Time interval between the repository creation time and PR creation time in days.\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "- Retrieve the creation timestamp of the repository (the time the repo was first created on GitHub).\n",
        "- Retrieve the creation timestamp of the pull request under study.\n",
        "- Compute the time interval between these two timestamps in days\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "Merging `pr_review_comments` and `pr_reviews` DataFrames with our `metrics` to link comments/reviews to PR authors.\n",
        "Filtering these merged DataFrames to exclude rows where the comment/review user_id matches the PR author's user_id.\n",
        "Grouping the filtered data by `pr_id`.\n",
        "Calculating the count of distinct user_id values (`.nunique()`) within each `pr_id` group for both filtered DataFrames.\n",
        "Merging these unique counts and summing them per `pr_id` to get the total unique non-author participants across review comments and reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bvwr_F_0h3p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['repoAge'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding repoAge: {len(metrics):,}\")\n",
        "\n",
        "# Copy the Repository dataframe\n",
        "repos_temp = repos.copy()\n",
        "\n",
        "# Calculate repo_created_at for the repos_temp DataFrame\n",
        "repos_temp['repo_created_at'] = repos_temp.apply(lambda row: get_repo_created_at(row['url']), axis=1)\n",
        "\n",
        "# Drop repos where repo_created_at could not be retrieved\n",
        "repos_temp = repos_temp.dropna(subset=['repo_created_at'])\n",
        "\n",
        "\n",
        "# Merge the Dataframes using a left join to bring repo_created_at into metrics\n",
        "# Ensure we only merge the necessary columns to avoid duplicates like repo_created_at_x\n",
        "metrics = pd.merge(metrics, repos_temp[['repo_id', 'repo_created_at']], on='repo_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary dataframe\n",
        "repos_temp = None\n",
        "\n",
        "# Drop from Metrics any Repo without a repo created date after the merge\n",
        "# This might be redundant if merged_at is already checked, but good for safety\n",
        "metrics = metrics.dropna(subset=['repo_created_at'])\n",
        "\n",
        "# Calculate the Repo Age in Days (created_at - repo_created_at)\n",
        "# Ensure 'created_at' and 'repo_created_at' are in datetime format before calculation\n",
        "metrics['created_at'] = pd.to_datetime(metrics['created_at'], errors='coerce')\n",
        "metrics['repo_created_at'] = pd.to_datetime(metrics['repo_created_at'], errors='coerce')\n",
        "\n",
        "metrics = metrics.assign(repoAge=lambda x: (x['created_at'] - x['repo_created_at']).dt.days)\n",
        "\n",
        "# Drop the unnecessary Repo Created At column after calculating repoAge\n",
        "metrics = metrics.drop(columns=['repo_created_at'], errors='ignore')\n",
        "\n",
        "# Fill N/A values for repoAge with defaults\n",
        "metrics['repoAge'] = metrics['repoAge'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding repoAge: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "ZBVOYKNaFOcN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. **isMember:** Whether or not the author is a member or outside collaborator (True/False).\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "- For each PR the study calls GitHub‚Äôs GraphQL API and retrieves the author‚Äôs association with the repository (the `authorAssociation` field).\n",
        "- If the returned association is `MEMBER` or `OWNER`, the flag is set to‚ÄØ1; otherwise (e.g., `CONTRIBUTOR`, `NONE`, or an external collaborator) it is set to‚ÄØ0.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Followed the same approach as Xiao 2024."
      ],
      "metadata": {
        "id": "IhSC6qwXf95V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b205b222",
        "collapsed": true
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['isMember'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding isMember: {len(metrics):,}\")\n",
        "\n",
        "def get_author_association(repo_owner: str, repo_name: str, pr_number: int) -> str:\n",
        "    \"\"\"Fetches the authorAssociation for a PR using GitHub GraphQL API, cycling through tokens.\"\"\"\n",
        "    # Use the token_cycle defined in cell 818abba5\n",
        "    current_token = next(token_cycle) # Get the next token in the cycle\n",
        "    query = \"\"\"\n",
        "    query($owner: String!, $repo: String!, $pr: Int!) {\n",
        "      repository(owner: $owner, name: $repo) {\n",
        "        pullRequest(number: $pr) {\n",
        "          authorAssociation\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        "    variables = {\n",
        "        \"owner\": repo_owner,\n",
        "        \"repo\": repo_name,\n",
        "        \"pr\": pr_number\n",
        "    }\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {current_token}\", # Use the current token\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Implement basic rate limit handling and retry\n",
        "    for attempt in range(3): # Retry up to 3 times\n",
        "        try:\n",
        "            response = requests.post(\"https://api.github.com/graphql\", json={'query': query, 'variables': variables}, headers=headers)\n",
        "            response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "\n",
        "            data = response.json()\n",
        "\n",
        "            # Handle GraphQL errors using the centralized function\n",
        "            error_status = handle_graphql_errors(data, context_info=f\"PR {pr_number} in {repo_owner}/{repo_name}\")\n",
        "\n",
        "            if error_status == 'RATE_LIMITED':\n",
        "                # Switch to the next token before retrying due to rate limit\n",
        "                current_token = next(token_cycle)\n",
        "                headers['Authorization'] = f'Bearer {current_token}'\n",
        "                continue # Continue to the next retry attempt\n",
        "\n",
        "            if error_status == 'ERROR':\n",
        "                 # For other GraphQL errors, break the retry loop\n",
        "                 break\n",
        "\n",
        "            # If no GraphQL errors and no HTTP errors, extract the data\n",
        "            association = data['data']['repository']['pullRequest']['authorAssociation']\n",
        "            return association\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request error for PR {pr_number} in {repo_owner}/{repo_name}: {e}\")\n",
        "            time.sleep(5 * (attempt + 1)) # Exponential backoff\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred for PR {pr_number} in {repo_owner}/{repo_name}: {e}\")\n",
        "            break # Don't retry for unexpected errors\n",
        "\n",
        "    return None # Return None if all retries fail\n",
        "\n",
        "\n",
        "# Prepare data: Ensure 'repo_url' and 'number' are available in metrics\n",
        "# You might need to merge with the original pull_request DataFrame if these columns were dropped\n",
        "if 'repo_url' not in metrics.columns or 'number' not in metrics.columns:\n",
        "     print(\"Warning: 'repo_url' or 'number' not found in metrics. Merging with original pull_request data.\")\n",
        "     # Assuming original pull_request is available\n",
        "     metrics = pd.merge(metrics, pull_request[['id', 'repo_url', 'number']], left_on='pr_id', right_on='id', how='left', suffixes=('', '_original'))\n",
        "     metrics = metrics.drop(columns=['id_original'], errors='ignore') # Drop duplicate id column\n",
        "\n",
        "\n",
        "# Extract repo owner and name from repo_url\n",
        "def extract_repo_details(repo_url):\n",
        "    try:\n",
        "        parts = repo_url.split('/')\n",
        "        owner = parts[-2]\n",
        "        name = parts[-1]\n",
        "        return owner, name\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "metrics[['repo_owner', 'repo_name']] = metrics['repo_url'].apply(lambda url: pd.Series(extract_repo_details(url)))\n",
        "\n",
        "\n",
        "# Apply the function to fetch author association for each PR\n",
        "# This can be slow due to API calls. Consider applying to a subset for testing.\n",
        "# Ensure TEST_MODE is handled if you want to limit API calls\n",
        "if 'TEST_MODE' in globals() and TEST_MODE:\n",
        "    print(\"Running in TEST_MODE, applying to a subset.\")\n",
        "    # Apply to the current subset of metrics\n",
        "    metrics['authorAssociation'] = metrics.apply(\n",
        "        lambda row: get_author_association(row['repo_owner'], row['repo_name'], row['number']),\n",
        "        axis=1\n",
        "    )\n",
        "else:\n",
        "     print(\"Running on the full dataset.\")\n",
        "     metrics['authorAssociation'] = metrics.apply(\n",
        "        lambda row: get_author_association(row['repo_owner'], row['repo_name'], row['number']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "\n",
        "# Map authorAssociation to isMember (1 for MEMBER/OWNER, 0 otherwise)\n",
        "metrics['isMember'] = metrics['authorAssociation'].apply(\n",
        "    lambda x: 1 if x in ['MEMBER', 'OWNER'] else (0 if x is not None else None) # Handle None from API errors\n",
        ")\n",
        "\n",
        "# Drop temporary columns used for API calls if they are not needed for further analysis\n",
        "metrics = metrics.drop(columns=['repo_owner', 'repo_name', 'authorAssociation'], errors='ignore')\n",
        "\n",
        "\n",
        "print(f\"Number of PRs after adding isMember: {len(metrics):,}\")\n",
        "\n",
        "print(\"‚úÖ Calculated isMember using GraphQL API.\")\n",
        "display(metrics[['isMember']].head())\n",
        "display(metrics['isMember'].value_counts(dropna=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GraphQL Errors Example for isMember**\n",
        "\n",
        "GraphQL errors for PR ...: [{'type': 'NOT_FOUND', ... 'message': \"Could not resolve to a Repository with the name 'owner/repo'.\"}]: These are error messages from the GitHub GraphQL API. The NOT_FOUND type means that the API could not find the specific resource you were requesting. In these cases, the message \"Could not resolve to a Repository with the name 'owner/repo'\" indicates that the repository specified in the API call (e.g., 'vals-ai/vals-sdk', 'ImSingingInTheRain/gpai-assessment') could not be found on GitHub. This could be because the repository was renamed, deleted, or is private and your token doesn't have access.\n",
        "\n",
        "\n",
        "GraphQL errors for PR ...: [{'type': 'NOT_FOUND', ... 'message': 'Could not resolve to a PullRequest with the number of XX.'}]: Similarly, this error indicates that the specific pull request number within the given repository could not be found. This might happen if the PR was closed and potentially deleted, or the number is incorrect for that repository."
      ],
      "metadata": {
        "id": "LPoWAdik39GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. **state**: State of the pull request (MERGED or CLOSED).\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "In GitHub GraphQL (or REST) API, the response includes a field called state (or, equivalently, a Boolean merged flag). The value of that field can be one of three mutually‚Äëexclusive statuses:\n",
        "- MERGED (or merged = true)\tThe PR was successfully merged into the target branch.\n",
        "- CLOSED (or merged = false‚ÄØ&‚ÄØstate = CLOSED)\tThe PR was closed without being merged.\n",
        "- OPEN (or state = OPEN)\tThe PR was still open at the time the data were collected.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "If the `merged_at` column for a pull request has a value (i.e., it's not `null`), it means the pull request was merged, and the state is set to `MERGED`.\n",
        "If the `merged_at` column is `null`, it means the pull request was closed without being merged, and the state is set to `CLOSED`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qNQdfoY8iNBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['state'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding state: {len(metrics):,}\")\n",
        "\n",
        "# Set the State to MERGED or CLOSED\n",
        "metrics['state'] = metrics['merged_at'].apply(lambda x: 'MERGED' if x is not None else 'CLOSED')\n",
        "\n",
        "print(f\"Number of PRs after adding state: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "5Ftmm7qZlHbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "`reviewtime` (in hours) = (PR Closed Timestamp - PR Creation Timestamp).\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "The difference between the `closed_at` and `created_at` timestamps and converting that duration into hours.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VzgXfZtoCjPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewTime'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding reviewTime: {len(metrics):,}\")\n",
        "\n",
        "# Calculate the Review Time\n",
        "metrics['reviewTime'] = (metrics['closed_at'] - metrics['created_at']).dt.total_seconds() / 3600\n",
        "\n",
        "print(f\"Number of PRs after adding reviewTime: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "UM0zn7ZtCmdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project variables"
      ],
      "metadata": {
        "id": "1unf8guqcK2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "The number of stargazers that a repository has\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Same approach\n",
        "\n",
        "\n",
        "18. **forkCount:** The # of forks that a repository has\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "19. **stargazerCount:** The # of stargazers that a repository has.\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5lSM-5VimJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['repoLanguage', 'forkCount', 'stargazerCount'], errors='ignore')\n",
        "\n",
        "repos_temp = (repos.copy()\n",
        "                   .drop(columns=['license', 'repo_url', 'html_url', 'full_name'], errors='ignore')\n",
        "                   .rename(columns={'language': 'repoLanguage', 'forks': 'forkCount', 'stars': 'stargazerCount'}))\n",
        "\n",
        "# Group by ID and get the First Record\n",
        "repos_temp = repos_temp.groupby(['repo_id']).first().reset_index() # Add reset_index() to make repo_id a column again\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, repos_temp, left_on='repo_id', right_on='repo_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "repos_temp = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['repoLanguage'] = metrics['repoLanguage'].fillna('other')\n",
        "metrics['forkCount'] = metrics['forkCount'].fillna(0).astype(int)\n",
        "metrics['stargazerCount'] = metrics['stargazerCount'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "hEoTBBXjFOVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treatment variables"
      ],
      "metadata": {
        "id": "RV-ykVvwcSGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)\n"
      ],
      "metadata": {
        "id": "zUDj9SFMjShm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics = metrics.drop(columns=['isGeneratedByCopliot'], errors='ignore')\n",
        "\n",
        "metrics['isGeneratedByCopilot'] = metrics['agent'].apply(lambda x: True if x == 'Copilot' else False) # Corrected column name and capitalization"
      ],
      "metadata": {
        "id": "btn7vVCnjSTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outcome variables"
      ],
      "metadata": {
        "id": "iRzFbe-Acwbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours\n"
      ],
      "metadata": {
        "id": "0Nzqcx6Yi6Uo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c5c683",
        "collapsed": true
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics = metrics.drop(columns=['reviewTime'], errors='ignore')\n",
        "\n",
        "# Calculate review time in hours, handling potential NaT values\n",
        "metrics = metrics.assign(reviewTime=lambda x: (x['closed_at'] - x['created_at']).dt.total_seconds() / 3600)\n",
        "\n",
        "# Fill N/A values with defaults (e.g., for open PRs)\n",
        "metrics['reviewTime'] = metrics['reviewTime'].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. **Is merged (state):** Whether or not a PR is merged (binary)\n"
      ],
      "metadata": {
        "id": "-QSkr74njEzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['isMerged'], errors='ignore')\n",
        "\n",
        "# If Merged_At is None, the PR was not merged, otherwise it was\n",
        "metrics['isMerged'] = metrics['merged_at'].apply(lambda x: 0 if x is None else 1)"
      ],
      "metadata": {
        "id": "fvl0rnKHbOwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Order in CSV (treatment_metrics.csv and control_metrics.csc)\n",
        "\n",
        "1. **repoLanguage**\n",
        "2. **forkCount**\n",
        "3. **stargazerCount**\n",
        "4. **repoAge**\n",
        "5. **state**\n",
        "6. **deletions**\n",
        "7. **additions**\n",
        "8. **changedFiles**\n",
        "9. **commentsTotalCount**\n",
        "10. **commitsTotalCount**\n",
        "11. **prExperience**\n",
        "12. **isMember**\n",
        "13. **authorComments**\n",
        "14. **reviewersComments**\n",
        "15. **reviewersTotalCount**\n",
        "16. **bodyLength**\n",
        "17. **prSize**\n",
        "18. **reviewTime**\n",
        "19. **purpose**\n"
      ],
      "metadata": {
        "id": "0-kgS2O2dL6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export to CSV"
      ],
      "metadata": {
        "id": "3z7NwYwGxxiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the Google Drive folder\n",
        "drive_path = \"/content/drive/MyDrive/AIDev_shared/\"\n",
        "\n",
        "# Ensure the directory exists (optional, but good practice)\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Define the desired order of columns for the output CSVs\n",
        "csv_order = ['repoLanguage',\n",
        "'forkCount',\n",
        "'stargazerCount',\n",
        "'repoAge',\n",
        "'state',\n",
        "'deletions',\n",
        "'additions',\n",
        "'changedFiles',\n",
        "'commentsTotalCount',\n",
        "'commitsTotalCount',\n",
        "'prExperience',\n",
        "'isMember',\n",
        "'authorComments',\n",
        "'reviewersComments',\n",
        "'reviewersTotalCount',\n",
        "'bodyLength',\n",
        "'prSize',\n",
        "'reviewTime',\n",
        "'purpose']\n",
        "\n",
        "# Split the metrics DataFrame into treatment (Copilot) and control (non-Copilot) based on the 'isGeneratedByCopilot' column\n",
        "try:\n",
        "    # Use the isGeneratedByCopilot column for splitting\n",
        "    treatment_metrics_full = metrics[metrics['isGeneratedByCopilot'] == True].copy()\n",
        "    control_metrics_full = metrics[metrics['isGeneratedByCopilot'] == False].copy()\n",
        "except KeyError:\n",
        "    print(\"Error: 'isGeneratedByCopilot' column not found in metrics DataFrame. Please ensure it's included in previous steps.\")\n",
        "    raise\n",
        "\n",
        "# Select and reorder columns for the treatment and control DataFrames\n",
        "treatment_metrics = treatment_metrics_full[csv_order].copy()\n",
        "control_metrics = control_metrics_full[csv_order].copy()\n",
        "\n",
        "# Now, drop rows with NaN values from the column-filtered DataFrames\n",
        "print(f\"Number of treatment PRs before dropping NaNs: {len(treatment_metrics):,}\")\n",
        "treatment_metrics.dropna(inplace=True)\n",
        "print(f\"Number of treatment PRs after dropping NaNs: {len(treatment_metrics):,}\")\n",
        "\n",
        "print(f\"Number of control PRs before dropping NaNs: {len(control_metrics):,}\")\n",
        "control_metrics.dropna(inplace=True)\n",
        "print(f\"Number of control PRs after dropping NaNs: {len(control_metrics):,}\")\n",
        "\n",
        "\n",
        "# Define the file paths\n",
        "treatment_file_path = os.path.join(drive_path, \"treatment_metrics.csv\")\n",
        "control_file_path = os.path.join(drive_path, \"control_metrics.csv\")\n",
        "\n",
        "# Export to CSV\n",
        "treatment_metrics.to_csv(treatment_file_path, index=False)\n",
        "control_metrics.to_csv(control_file_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Exported treatment metrics to: {treatment_file_path}\")\n",
        "print(f\"‚úÖ Exported control metrics to: {control_file_path}\")"
      ],
      "metadata": {
        "id": "ov2O_pNGwy-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.head()"
      ],
      "metadata": {
        "id": "lSJa206uCknk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fourth**, Bot detection and filtering employed the methodology of Golzadeh‚ÄØet‚ÄØal. (2022)\n",
        "\n",
        "simple ‚Äúbot‚Äù username suffix check with a comprehensive, manually verified list of 527 bot accounts\n",
        "* groundtruthbots.csv - a list of bots from Golzadeh et al.\n"
      ],
      "metadata": {
        "id": "V8vXCVjGW1aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the Google Drive folder\n",
        "drive_path = \"/content/drive/MyDrive/AIDev_shared/\"\n",
        "\n",
        "# Define the file path for the ground truth bots list (now a CSV)\n",
        "bots_file_path = os.path.join(drive_path, \"groundtruthbots.csv\")\n",
        "\n",
        "# Load the ground truth bots list from CSV\n",
        "try:\n",
        "    # Assuming the CSV has a header and the usernames are in the first column\n",
        "    groundtruth_bots_df = pd.read_csv(bots_file_path)\n",
        "    # Extract the first column and convert to a list, handling potential NaNs\n",
        "    groundtruth_bots = groundtruth_bots_df.iloc[:, 0].dropna().astype(str).tolist()\n",
        "    print(f\"Loaded {len(groundtruth_bots)} bot usernames from {bots_file_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {bots_file_path} not found. Please ensure the file exists and contains bot usernames in the first column.\")\n",
        "    groundtruth_bots = [] # Initialize as empty list to avoid errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading {bots_file_path}: {e}\")\n",
        "    groundtruth_bots = [] # Initialize as empty list\n",
        "\n",
        "\n",
        "# --- Filter bot-submitted PRs from metrics DataFrame ---\n",
        "\n",
        "print(f\"Number of PRs before bot filtering: {len(metrics):,}\")\n",
        "\n",
        "# Filter out PRs where the author's login ends with 'bot' or is in the ground truth list\n",
        "# Ensure 'author_login' column exists and is treated as string\n",
        "if 'author_login' not in metrics.columns:\n",
        "    print(\"Warning: 'author_login' column not found in metrics. Skipping PR bot filtering.\")\n",
        "    # You might want to add a step to create 'author_login' here if it's missing\n",
        "    # For now, we'll skip this filtering step to avoid crashing\n",
        "    metrics_filtered = metrics.copy()\n",
        "else:\n",
        "    metrics_filtered = metrics[\n",
        "        (~metrics['author_login'].astype(str).str.lower().str.endswith('bot', na=False)) &\n",
        "        (~metrics['author_login'].astype(str).isin(groundtruth_bots))\n",
        "    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "print(f\"Number of PRs after bot filtering: {len(metrics_filtered):,}\")\n",
        "\n",
        "\n",
        "# --- Filter bot comments from pr_comments DataFrame ---\n",
        "\n",
        "print(f\"Number of comments before bot filtering: {len(pr_comments):,}\")\n",
        "\n",
        "# Filter out comments where the user's login ends with 'bot' or is in the ground truth list\n",
        "# Ensure 'user' column exists and is treated as string\n",
        "if 'user' not in pr_comments.columns:\n",
        "     print(\"Warning: 'user' column not found in pr_comments. Skipping comment bot filtering.\")\n",
        "     # You might want to add a step to create 'user' here if it's missing\n",
        "     # For now, we'll skip this filtering step to avoid crashing\n",
        "     pr_comments_filtered = pr_comments.copy()\n",
        "else:\n",
        "    pr_comments_filtered = pr_comments[\n",
        "        (~pr_comments['user'].astype(str).str.lower().str.endswith('bot', na=False)) &\n",
        "        (~pr_comments['user'].astype(str).isin(groundtruth_bots))\n",
        "    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "print(f\"Number of comments after bot filtering: {len(pr_comments_filtered):,}\")\n",
        "\n",
        "# You can now replace the original metrics and pr_comments DataFrames with the filtered ones\n",
        "metrics = metrics_filtered\n",
        "pr_comments = pr_comments_filtered\n",
        "\n",
        "print(\"Bot filtering applied to metrics and pr_comments DataFrames.\")"
      ],
      "metadata": {
        "id": "AqXYcUmgxz1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fifth**, Adoption Trend (RQ1)\n",
        "(RQ1) To what extent do developers use Copilot for PRs in the code review process?\n",
        "\n",
        "* Counted occurrences of each marker tag; copilot:summary was the most frequent (13‚ÄØ231 instances).\n",
        "* Visualised cumulative PRs over time (Fig.‚ÄØ3) and proportion of PRs per repository (Fig.‚ÄØ4).\n"
      ],
      "metadata": {
        "id": "g4l6_AqBzYVI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CNLP973ZzYBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sixth**, Causal Inference (RQ2)\n",
        "\n",
        "### Propensity‚ÄëScore Estimation\n",
        "Logistic regression (treatment‚ÄØ=‚ÄØCopilot usage) on the 17 covariates.\n",
        "Estimated each PR‚Äôs probability of receiving the treatment (ps).\n",
        "### Weight Construction\n",
        "Inverse‚Äëprobability weights: 1/ps for treated, 1/(1‚Äëps) for control.\n",
        "### Entropy Balancing\n",
        "Applied the entropy‚Äëbalancing algorithm (equivalent to R‚Äôs ebalance) to adjust the raw weights so that the weighted means of all covariates matched exactly between groups.\n",
        "After balancing, absolute mean differences for every covariate were ‚â§‚ÄØ0.10 (Fig.‚ÄØ2).\n",
        "### Outcome Regression\n",
        "* Review time (continuous): weighted ordinary least squares (lm analogue) with only the treatment indicator. The coefficient gave the Average Treatment Effect on the Treated (ATT) of ‚Äë19.3‚ÄØh (p‚ÄØ‚âà‚ÄØ1.6‚ÄØ√ó‚ÄØ10‚Åª¬π‚Å∑).\n",
        "* Merge outcome (binary): weighted logistic regression (glm with logit link). The exponentiated treatment coefficient yielded an odds ratio of 1.57 (95‚ÄØ%‚ÄØCI‚ÄØ[1.35,‚ÄØ1.84],‚ÄØp‚ÄØ<‚ÄØ0.001).\n",
        "These two models answer RQ2.1 (review‚Äëtime reduction) and RQ2.2 (higher merge likelihood).\n"
      ],
      "metadata": {
        "id": "48T4d5AezoVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The R Scripts\n",
        "The main difference between PMW_merge.R and PMW_review.R is:\n",
        "\n",
        "* PMW_merge.R includes the column isMerged, which indicates whether each pull request was merged (state == \"MERGED\"). This column is added to the modeling data and used in the analysis.\n",
        "* PMW_review.R does not include the isMerged column in its modeling data; it focuses only on review-related metrics.\n",
        "* Otherwise, both scripts process the same input data, use similar covariates, and prepare for causal inference analysis. The inclusion of isMerged in PMW_merge.R allows for analysis related to PR merge status, while PMW_review.R is focused on review characteristics."
      ],
      "metadata": {
        "id": "kNvRCCdDFmIf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GN6LEJdGbVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}