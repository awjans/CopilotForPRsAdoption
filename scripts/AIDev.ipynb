{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awjans/CopilotForPRsAdoption/blob/main/scripts/AIDev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection/Cleaning Overview\n",
        "1. **PR identification**\n",
        "   * Queried GitHub via GraphQL for PRs whose description contained the phrase **“Generated by Copilot”** or any of the marker tags:\n",
        "\n",
        "     * `copilot:summary`\n",
        "     * `copilot:walkthrough`\n",
        "     * `copilot:poem`\n",
        "     * `copilot:all`\n",
        "\n",
        "2. **Scope**\n",
        "   * Collected **18,256 PRs** from **146 early-adopter repositories** during **March 2023 – August 2023**.\n",
        "\n",
        "3. **Control set**\n",
        "   * For the same repositories, gathered **54,188 PRs** that did **not** contain any Copilot marker.\n",
        "   * These served as the **untreated (control) group** for the **RQ2 comparison**.\n",
        "\n",
        "4. **Bot filtering**\n",
        "   * Removed PRs and comments authored by bots using the **high-precision method** of **Golzadeh et al. (2022)**, which included:\n",
        "     * (i) Usernames ending with “bot”\n",
        "     * (ii) A curated list of **527 known bot accounts**\n",
        "\n",
        "5. **Revision extraction (RQ3)**\n",
        "   * From the **18,256 Copilot-generated PRs**, retrieved the full **edit history** of PR descriptions.\n",
        "   * Identified **1,437 revisions** where developers **edited the AI-suggested content**."
      ],
      "metadata": {
        "id": "j4dl0GKtyPQO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eZ03WzKRQ1bS"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "\n",
        "from dateutil import parser\n",
        "from google.colab import userdata\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b7c674"
      },
      "source": [
        "# **First**, We need to define the URLs of the AIDev Parquet Files that we are intersted in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de61a39b"
      },
      "source": [
        "pull_request_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_pull_request.parquet'\n",
        "pr_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_comments.parquet'\n",
        "pr_commits_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commits.parquet'\n",
        "pr_commit_details_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commit_details.parquet'\n",
        "pr_reviews_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_reviews.parquet'\n",
        "pr_review_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_review_comments.parquet'\n",
        "pr_task_type_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_task_type.parquet'\n",
        "repository_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_repository.parquet'\n",
        "user_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/user.parquet'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the Parquet file into a Pandas DataFrame from the file URL.\n",
        "\"\"\"\n",
        "def load_data(url: str):\n",
        "  import pandas as pd # Import pandas inside the function\n",
        "  try:\n",
        "    # For Parquet files:\n",
        "    df = pd.read_parquet(url)\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading data: {e}\")\n",
        "      print(\"Please ensure the URL is correct and the file is publicly accessible.\")\n",
        "      return None # Return None in case of an error"
      ],
      "metadata": {
        "id": "Qcu3bdoFM6UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "818abba5"
      },
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "GH_TOKEN = os.environ.get('GITHUB_TOKEN', userdata.get('GITHUB_TOKEN'))\n",
        "\n",
        "async def get_repo_data(repo_url: str):\n",
        "    # Make the Request\n",
        "    print(f'Requesting: {repo_url}')\n",
        "    response = requests.get(repo_url, headers={'Authorization': f'token {GH_TOKEN}'})\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "    # Process the JSON response\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def get_repo_created_at(repo_url: str):\n",
        "    \"\"\"\n",
        "    Get the Repository Created At timestamp for the Repo from GitHub the API call.\n",
        "\n",
        "    Args:\n",
        "        repo_url: The GitHub API repository URL.\n",
        "\n",
        "    Returns:\n",
        "        The created_at timestamp if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        task = asyncio.create_task(get_repo_data(repo_url))\n",
        "        event_loop = asyncio.get_running_loop()\n",
        "        if event_loop.is_running():\n",
        "          data = event_loop.run_until_complete(task)\n",
        "        else:\n",
        "          data = asyncio.run(task)\n",
        "\n",
        "        # Extract the createdAt value\n",
        "        created_at = data['created_at']\n",
        "        print(f\"Repo: {repo_url}; Created At: {created_at}\")\n",
        "\n",
        "        if created_at:\n",
        "            return pd.to_datetime(created_at)\n",
        "        else:\n",
        "            raise Exception(f\"Error: Could not retrieve createdAt for {repo_url}. Response data: {data}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during GitHub API request for {repo_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Second**, We need to load the data from the URLs"
      ],
      "metadata": {
        "id": "CKy9xnAeb1p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pull_request = load_data(pull_request_file_url)\n",
        "pr_comments = load_data(pr_comments_file_url)\n",
        "pr_commits = load_data(pr_commits_file_url)\n",
        "pr_commit_details = load_data(pr_commit_details_file_url)\n",
        "pr_reviews = load_data(pr_reviews_file_url)\n",
        "pr_review_comments = load_data(pr_review_comments_file_url)\n",
        "pr_task_type = load_data(pr_task_type_file_url)\n",
        "repository = load_data(repository_file_url)\n",
        "user = load_data(user_file_url)"
      ],
      "metadata": {
        "id": "YAPFfpktRJkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Copy of Pull_Requests\n",
        "\n",
        "Remove the Open Pull Requests"
      ],
      "metadata": {
        "id": "67cBtszBeJC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = pull_request.copy()\n",
        "repos = repository.copy()\n",
        "\n",
        "# Rename 'id' to 'pr_id' for joining\n",
        "metrics = metrics.rename(columns={'id': 'pr_id'})\n",
        "# Rename 'id' to 'repo_id' for joining\n",
        "repos = repos.rename(columns={'id': 'repo_id'})\n",
        "\n",
        "# Remove Open Pull Requests (closed_at is None)\n",
        "print(f\"Number of Pull Requests: {len(metrics)}\")\n",
        "metrics = metrics[metrics['closed_at'].notna()]\n",
        "print(f\"Number of Closed Pull Requests: {len(metrics)}\")\n",
        "\n",
        "# Convert Timestamps\n",
        "metrics['created_at'] = pd.to_datetime(metrics['created_at'])\n",
        "metrics['closed_at'] = pd.to_datetime(metrics['closed_at'])\n",
        "\n",
        "# Remove Repositories that do not have a Pull Request\n",
        "print(f\"Number of Repositories: {len(repos)}\")\n",
        "repos = repos[repos['repo_id'].isin(metrics['repo_id'])]\n",
        "print(f\"Number of Repositories with Pull Requests: {len(repos)}\")\n",
        "repos['repo_created_at'] = repos.apply(lambda row: get_repo_created_at(row['url']), axis=1)\n",
        "repos = repos.dropna(subset=['created_at'])\n",
        "print(f\"Number of Repositories with Pull Requests that are Active: {len(repos)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyRRwfJc30t",
        "outputId": "1bc22372-a1af-4f41-a0bf-7620c7f5aca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Pull Requests: 33596\n",
            "Number of Closed Pull Requests: 31284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Third**, Gather the covariant variables\n",
        "\n",
        "## PR Variables\n",
        "\n",
        "1. **additions:** The # of added LOC by a PR\n",
        "2. **deletions:** The # of deleted LOC by a PR\n",
        "3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)\n",
        "4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', …).\n",
        "5. **changedFiles:** The # of files changed by a PR\n",
        "6. **commitsTotalCount:** The # of commits involved in a PR\n",
        "7. **Description length:** The length of a PR description\n",
        "8. **prExperience:** The # of prior PRs that were submitted by the PR author (author’s prior PR count). Query the author’s PR history in the same repo and count PRs created before the current one.\n",
        "9. **isMember:** Whether or not the author is a member or outside collaborator (True/False).\n",
        "10. **commentsTotalCount:** The # of comments left on a PR\n",
        "11. **authorComments:** The # of comments left by the PR author\n",
        "12. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion\n",
        "13. **reviewersTotalCount:** The # of developers who participate in the discussion.\n",
        "14. **repoAge:** Time interval between the repository creation time and PR creation time in days.\n",
        "15. **state**: State of the pull request (MERGED or CLOSED).\n",
        "16. **bodyLength**: Length of the PR body (in characters).\n",
        "17. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).\n",
        "\n",
        "## Project variables\n",
        "\n",
        "18. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue). *[I'm assuming its the top language as there is only one]*\n",
        "19. **forkCount:** The # of forks that a repository has\n",
        "20. **stargazerCount:** The # of stargazers that a repository has.\n",
        "\n",
        "## Treatment variables\n",
        "\n",
        "21. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)\n",
        "\n",
        "## Outcome variables\n",
        "\n",
        "22. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours\n",
        "23. **Is merged (state):** Whether or not a PR is merged (binary)\n",
        "\n"
      ],
      "metadata": {
        "id": "a30SFFhZXgy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PR Variables"
      ],
      "metadata": {
        "id": "xANcFKyCb9Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **additions:** The # of added LOC by a PR\n",
        "2. **deletions:** The # of deleted LOC by a PR\n",
        "3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)\n"
      ],
      "metadata": {
        "id": "zP1sNtj3eIRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['additions', 'deletions', 'prSize'], errors='ignore')\n",
        "\n",
        "# Get the sums of the columns we are interested in\n",
        "pr_commit_LOC = (pr_commit_details.groupby(['pr_id'])\n",
        "                                  .sum(['additions', 'deletions', 'changes'])\n",
        "                                  .reset_index())\n",
        "\n",
        "# Rename the sum columns to what we want\n",
        "pr_commit_LOC = (pr_commit_LOC.rename(columns={'changes': 'prSize'}))\n",
        "\n",
        "# Drop the extraneous columns\n",
        "pr_commit_LOC = pr_commit_LOC.drop(columns=['commit_stats_total', 'commit_stats_additions', 'commit_stats_deletions'])\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "metrics = pd.merge(metrics, pr_commit_LOC, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage collect the temporary Dataframe\n",
        "pr_commit_LOC = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['additions'] = metrics['additions'].fillna(0).astype(int)\n",
        "metrics['deletions'] = metrics['deletions'].fillna(0).astype(int)\n",
        "metrics['prSize'] = metrics['prSize'].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "nj9typbGIcI2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', …).\n"
      ],
      "metadata": {
        "id": "_9ULLRD1eVyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['purpose'], errors='ignore')\n",
        "\n",
        "# Make a copy of the PR Task Type Dataframe and Drop unneeded columns\n",
        "pr_task = (pr_task_type.copy()\n",
        "                       .rename(columns={'id': 'pr_id'})\n",
        "                       .drop(columns=['agent', 'title', 'reason', 'confidence'], errors='ignore'))\n",
        "\n",
        "# Filter the PR Tasks to bug, feature or document\n",
        "pr_task = pr_task[pr_task['type'].isin(['fix', 'feat', 'doc'])]\n",
        "\n",
        "# Group by ID and get the First Record\n",
        "pr_task = pr_task.groupby(['pr_id']).first()\n",
        "\n",
        "# Rename the column to what we want to keep\n",
        "pr_task = pr_task.rename(columns={'type': 'purpose'})\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "metrics = pd.merge(metrics, pr_task, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_task = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['purpose'] = metrics['purpose'].fillna('other')\n",
        "\n",
        "#Check that the purpose = either of the three options; Bug, Feature, Document\n",
        "metrics = metrics[metrics['purpose'].isin(['fix', 'feat', 'doc'])]"
      ],
      "metadata": {
        "id": "Hc2sYaVwFPdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **changedFiles:** The # of files changed by a PR\n"
      ],
      "metadata": {
        "id": "jKdoh7knfFD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['changedFiles'], errors='ignore')\n",
        "\n",
        "# Count the number of Files changed and change the column name to what we want\n",
        "pr_files_changed = (pr_commit_details.groupby(['pr_id', 'filename'])\n",
        "                                     .size()\n",
        "                                     .groupby(['pr_id'])\n",
        "                                     .size()\n",
        "                                     .reset_index(name='changedFiles'))\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "metrics = pd.merge(metrics, pr_files_changed, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_files_changed = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['changedFiles'] = metrics['changedFiles'].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "YwnUjWn9FPY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **commitsTotalCount:** The # of commits involved in a PR\n"
      ],
      "metadata": {
        "id": "xxZIoTx8fiCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['commitsTotalCount'], errors='ignore')\n",
        "\n",
        "# Count the number of Commits for the Pull Request, name the column what we want.\n",
        "pr_commits_count = pr_commits.groupby(['pr_id']).size().reset_index(name='commitsTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_commits_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_commits_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['commitsTotalCount'] = metrics['commitsTotalCount'].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "vOyzUxnHFPUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Description length:** The length of a PR description\n"
      ],
      "metadata": {
        "id": "Ed6fS14afxfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['bodyLength'], errors='ignore')\n",
        "\n",
        "# Get the Length of the Body of the Pull Request\n",
        "metrics['bodyLength'] = metrics['body'].str.len()"
      ],
      "metadata": {
        "id": "OZYyz-mVFPPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **prExperience:** The # of prior PRs that were submitted by the PR author (author’s prior PR count). Query the author’s PR history in the same repo and count PRs created before the current one.\n"
      ],
      "metadata": {
        "id": "JQyVdlFCf2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['prExperience'], errors='ignore')\n",
        "\n",
        "# TODO: Figure out how to do this\n",
        "metrics['prExperience'] = 0\n"
      ],
      "metadata": {
        "id": "37dKywGDFPKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **isMember:** Whether or not the author is a member or outside collaborator (True/False).\n"
      ],
      "metadata": {
        "id": "IhSC6qwXf95V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['isMember'], errors='ignore')\n",
        "\n",
        "# TODO: Figure out how to tell if a user is a member\n",
        "metrics['isMember'] = False\n"
      ],
      "metadata": {
        "id": "DW_ZiHEHFPEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **commentsTotalCount:** The # of comments left on a PR\n"
      ],
      "metadata": {
        "id": "ac9ifXkVgLqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics.drop(columns=['commentsTotalCount'], errors='ignore', inplace=True)\n",
        "\n",
        "# Count the number of Comments for the Pull Request, name the column what we want.\n",
        "pr_comments_count = pr_comments.groupby(['pr_id']).size().reset_index(name='commentsTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['commentsTotalCount'] = metrics['commentsTotalCount'].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "2BNmsnGIFO9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. **authorComments:** The # of comments left by the PR author\n"
      ],
      "metadata": {
        "id": "LoBAgwxOgaJn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "322b4128"
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['authorComments'], errors='ignore')\n",
        "\n",
        "# Filter comments to only include those made by the PR author\n",
        "# Need to merge with metrics to get the author_id for each pr_comment\n",
        "author_comments = pd.merge(pr_comments, metrics[['pr_id', 'user_id']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "author_comments = author_comments[author_comments['user_id_x'] == author_comments['user_id_y']]\n",
        "\n",
        "# Count the number of author comments per pull request\n",
        "author_comments_count = author_comments.groupby(['pr_id']).size().reset_index(name='authorComments')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, author_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "author_comments = None\n",
        "author_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['authorComments'] = metrics['authorComments'].fillna(0).astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion\n"
      ],
      "metadata": {
        "id": "0cC_Qh-DgvEJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "454479f7"
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewersComments'], errors='ignore')\n",
        "\n",
        "# Filter comments to exclude those made by the PR author\n",
        "# Need to merge with metrics to get the author_id for each pr_comment\n",
        "reviewer_comments = pd.merge(pr_comments, metrics[['pr_id', 'user_id']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "reviewer_comments = reviewer_comments[reviewer_comments['user_id_x'] != reviewer_comments['user_id_y']]\n",
        "\n",
        "# Count the number of reviewer comments per pull request\n",
        "reviewer_comments_count = reviewer_comments.groupby(['pr_id']).size().reset_index(name='reviewersComments')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, reviewer_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "reviewer_comments = None\n",
        "reviewer_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['reviewersComments'] = metrics['reviewersComments'].fillna(0).astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **reviewersTotalCount:** The # of developers who participate in the discussion.\n"
      ],
      "metadata": {
        "id": "sDtcHmxxg9MP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b294946"
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewersTotalCount'], errors='ignore')\n",
        "\n",
        "# Extract user_id from the nested 'user' column in pr_review_comments\n",
        "pr_review_comments['user_id_from_user'] = pr_review_comments['user'].apply(lambda x: x.get('id') if isinstance(x, dict) else None)\n",
        "\n",
        "# Extract user_id from the nested 'user' column in pr_reviews\n",
        "pr_reviews['user_id_from_user'] = pr_reviews['user'].apply(lambda x: x.get('id') if isinstance(x, dict) else None)\n",
        "\n",
        "# Get author_id from metrics for merging\n",
        "metrics['author_id_from_author'] = metrics['user'].apply(lambda x: x.get('id') if isinstance(x, dict) else None)\n",
        "\n",
        "\n",
        "# Get unique reviewer IDs from review comments, excluding the author\n",
        "reviewer_comments_users = pd.merge(pr_review_comments, metrics[['pr_id', 'author_id_from_author']], left_on='pull_request_review_id', right_on='pr_id', how='left')\n",
        "reviewer_comments_users = reviewer_comments_users[reviewer_comments_users['user_id_from_user'] != reviewer_comments_users['author_id_from_author']]\n",
        "reviewer_comments_users = reviewer_comments_users.groupby(['pull_request_review_id'])['user_id_from_user'].nunique().reset_index(name='reviewer_commenters')\n",
        "\n",
        "\n",
        "# Get unique reviewer IDs from reviews, excluding the author\n",
        "review_users = pd.merge(pr_reviews, metrics[['pr_id', 'author_id_from_author']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "review_users = review_users[review_users['user_id_from_user'] != review_users['author_id_from_author']]\n",
        "review_users = review_users.groupby(['pr_id'])['user_id_from_user'].nunique().reset_index(name='reviewers')\n",
        "\n",
        "# Merge the two dataframes to get unique users from both sources\n",
        "reviewers_total = pd.merge(reviewer_comments_users, review_users, left_on='pull_request_review_id', right_on='pr_id', how='outer').fillna(0)\n",
        "\n",
        "# Calculate the total number of unique reviewers\n",
        "reviewers_total['reviewersTotalCount'] = reviewers_total['reviewer_commenters'] + reviewers_total['reviewers']\n",
        "reviewers_total = reviewers_total.drop(columns=['reviewer_commenters', 'reviewers'])\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, reviewers_total, left_on='pr_id', right_on='pull_request_review_id', how='left')\n",
        "\n",
        "# Garbage Collect temporary dataframes\n",
        "reviewer_comments_users = None\n",
        "review_users = None\n",
        "reviewers_total = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['reviewersTotalCount'] = metrics['reviewersTotalCount'].fillna(0).astype(int)\n",
        "\n",
        "# Drop the temporary user_id and author_id columns\n",
        "pr_review_comments = pr_review_comments.drop(columns=['user_id_from_user'], errors='ignore')\n",
        "pr_reviews = pr_reviews.drop(columns=['user_id_from_user'], errors='ignore')\n",
        "metrics = metrics.drop(columns=['author_id_from_author', 'pull_request_review_id'], errors='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. **repoAge:** Time interval between the repository creation time and PR creation time in days.\n"
      ],
      "metadata": {
        "id": "Bvwr_F_0h3p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['repoAge', 'repo_created_at', 'repo_created_at_x'], errors='ignore')\n",
        "\n",
        "# Copy the Repository dataframe and remove the unnecessary columns\n",
        "repos_temp = (repos.copy()\n",
        "                   .drop(columns=['license', 'full_name', 'language', 'forks', 'stars', 'url'], errors='ignore'))\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, repos_temp, left_on='repo_id', right_on='repo_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary dataframe\n",
        "repos_temp = None\n",
        "\n",
        "# Drop from Metrics any Repo without a repo created date\n",
        "metrics = metrics.dropna(subset=['repo_created_at'])\n",
        "\n",
        "# Calculate the Repo Age in Days (created_at - repo_created_at), handling potential None values\n",
        "metrics = metrics.assign(repoAge=lambda x: (x['created_at'] - x['repo_created_at']).dt.days)\n",
        "\n",
        "# Drop the unnecessary Repo Created At column\n",
        "metrics = metrics.drop(columns=['repo_created_at'], errors='ignore')\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['repoAge'] = metrics['repoAge'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "ZBVOYKNaFOcN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. **state**: State of the pull request (MERGED or CLOSED).\n",
        "16. **bodyLength**: Length of the PR body (in characters).\n",
        "17. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).\n"
      ],
      "metadata": {
        "id": "qNQdfoY8iNBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['state', 'bodyLength', 'reviewTime'], errors='ignore')\n",
        "\n",
        "# Set the State to MERGED or CLOSED\n",
        "metrics['state'] = metrics['merged_at'].apply(lambda x: 'MERGED' if x is not None else 'CLOSED')\n",
        "\n",
        "# Get the Length of the Body of the Pull Request\n",
        "metrics['bodyLength'] = metrics['body'].str.len()\n",
        "\n",
        "# Calculate the Review Time\n",
        "metrics['reviewTime'] = (metrics['closed_at'] - metrics['created_at']).dt.total_seconds() / 3600"
      ],
      "metadata": {
        "id": "5Ftmm7qZlHbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project variables"
      ],
      "metadata": {
        "id": "1unf8guqcK2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue).\n",
        "*[I'm assuming its the top language as there is only one]*\n",
        "19. **forkCount:** The # of forks that a repository has\n",
        "20. **stargazerCount:** The # of stargazers that a repository has.\n"
      ],
      "metadata": {
        "id": "A5lSM-5VimJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['repoLanguage', 'forkCount', 'stargazerCount'], errors='ignore')\n",
        "\n",
        "repos_temp = (repos.copy()\n",
        "                   .drop(columns=['license', 'repo_url', 'html_url', 'full_name'], errors='ignore')\n",
        "                   .rename(columns={'language': 'repoLanguage', 'forks': 'forkCount', 'stars': 'stargazerCount'}))\n",
        "\n",
        "# Group by ID and get the First Record\n",
        "repos_temp = repos_temp.groupby(['repo_id']).first().reset_index() # Add reset_index() to make repo_id a column again\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, repos_temp, left_on='repo_id', right_on='repo_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "repos_temp = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['repoLanguage'] = metrics['repoLanguage'].fillna('other')\n",
        "metrics['forkCount'] = metrics['forkCount'].fillna(0).astype(int)\n",
        "metrics['stargazerCount'] = metrics['stargazerCount'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "hEoTBBXjFOVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treatment variables"
      ],
      "metadata": {
        "id": "RV-ykVvwcSGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)\n"
      ],
      "metadata": {
        "id": "zUDj9SFMjShm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics = metrics.drop(columns=['reviewTime'], errors='ignore')\n",
        "\n",
        "# Was the PR created by Copilot\n",
        "metrics['With Copilot for PRs'] = metrics['agent'].apply(lambda x: 1 if x == 'copilot' else 0)"
      ],
      "metadata": {
        "id": "btn7vVCnjSTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outcome variables"
      ],
      "metadata": {
        "id": "iRzFbe-Acwbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours\n"
      ],
      "metadata": {
        "id": "0Nzqcx6Yi6Uo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c5c683",
        "collapsed": true
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics = metrics.drop(columns=['reviewTime'], errors='ignore')\n",
        "\n",
        "# Calculate review time in hours, handling potential NaT values\n",
        "metrics = metrics.assign(reviewTime=lambda x: (x['closed_at'] - x['created_at']).dt.total_seconds() / 3600)\n",
        "\n",
        "# Fill N/A values with defaults (e.g., for open PRs)\n",
        "metrics['reviewTime'] = metrics['reviewTime'].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. **Is merged (state):** Whether or not a PR is merged (binary)\n"
      ],
      "metadata": {
        "id": "-QSkr74njEzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['isMerged'], errors='ignore')\n",
        "\n",
        "# If Merged_At is None, the PR was not merged, otherwise it was\n",
        "metrics['isMerged'] = metrics['merged_at'].apply(lambda x: 0 if x is None else 1)"
      ],
      "metadata": {
        "id": "fvl0rnKHbOwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Order in CSV (treatment_metrics.csv and control_metrics.csc)\n",
        "\n",
        "1. **repoLanguage**\n",
        "2. **forkCount**\n",
        "3. **stargazerCount**\n",
        "4. **repoAge**\n",
        "5. **state**\n",
        "6. **deletions**\n",
        "7. **additions**\n",
        "8. **changedFiles**\n",
        "9. **commentsTotalCount**\n",
        "10. **commitsTotalCount**\n",
        "11. **prExperience**\n",
        "12. **isMember**\n",
        "13. **authorComments**\n",
        "14. **reviewersComments**\n",
        "15. **reviewersTotalCount**\n",
        "16. **bodyLength**\n",
        "17. **prSize**\n",
        "18. **reviewTime**\n",
        "19. **purpose**\n"
      ],
      "metadata": {
        "id": "0-kgS2O2dL6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_order = ['repoLanguage',\n",
        "'forkCount',\n",
        "'stargazerCount',\n",
        "'repoAge',\n",
        "'state',\n",
        "'deletions',\n",
        "'additions',\n",
        "'changedFiles',\n",
        "'commentsTotalCount',\n",
        "'commitsTotalCount',\n",
        "'prExperience',\n",
        "'isMember',\n",
        "'authorComments',\n",
        "'reviewersComments',\n",
        "'reviewersTotalCount',\n",
        "'bodyLength',\n",
        "'prSize',\n",
        "'reviewTime',\n",
        "'purpose']\n",
        "\n",
        "# Remove unnecessary columns\n",
        "metrics = metrics.loc[:, csv_order]\n",
        "# Put the Columns in the right order\n",
        "metrics = metrics[csv_order]\n",
        "\n",
        "display(metrics.columns)\n",
        "display(len(pull_request))\n",
        "display(len(metrics))"
      ],
      "metadata": {
        "id": "KNB1iJ34WyMx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ef921d3d-572f-43ae-8d7a-f97783413573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Index(['repoLanguage', 'forkCount', 'stargazerCount', 'repoAge', 'state',\n",
              "       'deletions', 'additions', 'changedFiles', 'commentsTotalCount',\n",
              "       'commitsTotalCount', 'prExperience', 'isMember', 'authorComments',\n",
              "       'reviewersComments', 'reviewersTotalCount', 'bodyLength', 'prSize',\n",
              "       'reviewTime', 'purpose'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "33596"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "20751"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fourth**, Bot detection and filtering employed the methodology of Golzadeh et al. (2022)\n",
        "\n",
        "simple “bot” username suffix check with a comprehensive, manually verified list of 527 bot accounts\n",
        "* groundtruthbots.csv - a list of bots from Golzadeh et al.\n"
      ],
      "metadata": {
        "id": "V8vXCVjGW1aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bots_list = pd.read_csv(\"https://raw.githubusercontent.com/awjans/CopilotForPRsAdoption/main/data/groundtruthbots.csv\", engine='python')"
      ],
      "metadata": {
        "id": "AqXYcUmgxz1z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fifth**, Adoption Trend (RQ1)\n",
        "\n",
        "* Counted occurrences of each marker tag; copilot:summary was the most frequent (13 231 instances).\n",
        "* Visualised cumulative PRs over time (Fig. 3) and proportion of PRs per repository (Fig. 4).\n"
      ],
      "metadata": {
        "id": "g4l6_AqBzYVI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CNLP973ZzYBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sixth**, Causal Inference (RQ2)\n",
        "\n",
        "### Propensity‑Score Estimation\n",
        "Logistic regression (treatment = Copilot usage) on the 17 covariates.\n",
        "Estimated each PR’s probability of receiving the treatment (ps).\n",
        "### Weight Construction\n",
        "Inverse‑probability weights: 1/ps for treated, 1/(1‑ps) for control.\n",
        "### Entropy Balancing\n",
        "Applied the entropy‑balancing algorithm (equivalent to R’s ebalance) to adjust the raw weights so that the weighted means of all covariates matched exactly between groups.\n",
        "After balancing, absolute mean differences for every covariate were ≤ 0.10 (Fig. 2).\n",
        "### Outcome Regression\n",
        "* Review time (continuous): weighted ordinary least squares (lm analogue) with only the treatment indicator. The coefficient gave the Average Treatment Effect on the Treated (ATT) of ‑19.3 h (p ≈ 1.6 × 10⁻¹⁷).\n",
        "* Merge outcome (binary): weighted logistic regression (glm with logit link). The exponentiated treatment coefficient yielded an odds ratio of 1.57 (95 % CI [1.35, 1.84], p < 0.001).\n",
        "These two models answer RQ2.1 (review‑time reduction) and RQ2.2 (higher merge likelihood).\n"
      ],
      "metadata": {
        "id": "48T4d5AezoVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The R Scripts\n",
        "The main difference between PMW_merge.R and PMW_review.R is:\n",
        "\n",
        "* PMW_merge.R includes the column isMerged, which indicates whether each pull request was merged (state == \"MERGED\"). This column is added to the modeling data and used in the analysis.\n",
        "* PMW_review.R does not include the isMerged column in its modeling data; it focuses only on review-related metrics.\n",
        "* Otherwise, both scripts process the same input data, use similar covariates, and prepare for causal inference analysis. The inclusion of isMerged in PMW_merge.R allows for analysis related to PR merge status, while PMW_review.R is focused on review characteristics."
      ],
      "metadata": {
        "id": "kNvRCCdDFmIf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GN6LEJdGbVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}