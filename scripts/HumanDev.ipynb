{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awjans/CopilotForPRsAdoption/blob/main/scripts/HumanDev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection/Cleaning Overview\n",
        "1. **PR identification**\n",
        "   * Queried GitHub via GraphQL for PRs whose description contained the phrase **“Generated by Copilot”** or any of the marker tags:\n",
        "\n",
        "     * `copilot:summary`\n",
        "     * `copilot:walkthrough`\n",
        "     * `copilot:poem`\n",
        "     * `copilot:all`\n",
        "\n",
        "2. **Scope**\n",
        "   * Collected **18,256 PRs** from **146 early-adopter repositories** during **March 2023 – August 2023**.\n",
        "\n",
        "3. **Control set**\n",
        "   * For the same repositories, gathered **54,188 PRs** that did **not** contain any Copilot marker.\n",
        "   * These served as the **untreated (control) group** for the **RQ2 comparison**.\n",
        "\n",
        "4. **Bot filtering**\n",
        "   * Removed PRs and comments authored by bots using the **high-precision method** of **Golzadeh et al. (2022)**, which included:\n",
        "     * (i) Usernames ending with “bot”\n",
        "     * (ii) A curated list of **527 known bot accounts**\n",
        "\n",
        "5. **Revision extraction (RQ3)**\n",
        "   * From the **18,256 Copilot-generated PRs**, retrieved the full **edit history** of PR descriptions.\n",
        "   * Identified **1,437 revisions** where developers **edited the AI-suggested content**."
      ],
      "metadata": {
        "id": "j4dl0GKtyPQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXf5zM0UZf20",
        "outputId": "0a9b11f3-97f7-415d-f590-80211cd77a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ03WzKRQ1bS"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from dateutil import parser\n",
        "from google.colab import userdata\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "import random\n",
        "from itertools import cycle\n",
        "from google.colab import userdata\n",
        "from dateutil import parser\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b7c674"
      },
      "source": [
        "# **First**, Define the URLs of the AIDev Parquet Files that we are intersted in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de61a39b"
      },
      "source": [
        "pull_request_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_pull_request.parquet'\n",
        "pr_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_comments.parquet'\n",
        "pr_commits_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commits.parquet'\n",
        "pr_commit_details_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commit_details.parquet'\n",
        "pr_reviews_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_reviews.parquet'\n",
        "pr_review_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_review_comments.parquet'\n",
        "pr_task_type_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_task_type.parquet'\n",
        "repository_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_repository.parquet'\n",
        "user_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/user.parquet'\n",
        "human_pull_request_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/human_pull_request.parquet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Second**, We need to load the data from the URLs (15s)"
      ],
      "metadata": {
        "id": "CKy9xnAeb1p6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Parquet Files"
      ],
      "metadata": {
        "id": "eU0V6v6thrEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the Parquet file into a Pandas DataFrame from the file URL.\n",
        "\"\"\"\n",
        "def load_data(url: str):\n",
        "  import pandas as pd # Import pandas inside the function\n",
        "  try:\n",
        "    # For Parquet files:\n",
        "    df = pd.read_parquet(url)\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading data: {e}\")\n",
        "      print(\"Please ensure the URL is correct and the file is publicly accessible.\")\n",
        "      return None # Return None in case of an error"
      ],
      "metadata": {
        "id": "Qcu3bdoFM6UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pull_request = load_data(pull_request_file_url)\n",
        "pr_comments = load_data(pr_comments_file_url)\n",
        "pr_commits = load_data(pr_commits_file_url)\n",
        "pr_commit_details = load_data(pr_commit_details_file_url)\n",
        "pr_reviews = load_data(pr_reviews_file_url)\n",
        "pr_review_comments = load_data(pr_review_comments_file_url)\n",
        "pr_task_type = load_data(pr_task_type_file_url)\n",
        "repository = load_data(repository_file_url)\n",
        "user = load_data(user_file_url)\n",
        "human_pull_request = load_data(human_pull_request_file_url)"
      ],
      "metadata": {
        "id": "YAPFfpktRJkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Github token cycling & safe_get functions"
      ],
      "metadata": {
        "id": "K8VxcWjshui0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "818abba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "6cb1db81-71e0-4c87-efa9-6453cd056a1c"
      },
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "GITHUB_TOKENS = [\n",
        "    os.environ.get(\"GITHUB_TOKEN\"),\n",
        "    userdata.get('GITHUB_TOKEN_2'),\n",
        "    userdata.get('GITHUB_TOKEN_3'),\n",
        "    userdata.get('GITHUB_TOKEN_4'),\n",
        "    userdata.get('GITHUB_TOKEN_5'),\n",
        "    userdata.get('GITHUB_TOKEN_6'),\n",
        "    userdata.get('GITHUB_TOKEN_7'),\n",
        "]\n",
        "# Filter out any None values if a token is not set\n",
        "GITHUB_TOKENS = [token for token in GITHUB_TOKENS if token is not None]\n",
        "\n",
        "if not GITHUB_TOKENS:\n",
        "    raise RuntimeError(\"❌ No GitHub tokens found. Please add your tokens to Colab Secrets.\")\n",
        "\n",
        "# Create a cycle iterator for the tokens\n",
        "token_cycle = cycle(GITHUB_TOKENS)\n",
        "\n",
        "def safe_get(url, sleep_base=1.0, retries=3):\n",
        "    \"\"\"GET with retry, error handling, and rate-limit backoff, cycling through tokens.\"\"\"\n",
        "    current_token = next(token_cycle) # Get the next token in the cycle\n",
        "    headers = {'Authorization': f'token {current_token}', # Use the current token\n",
        "               'Accept': 'application/vnd.github+json'}\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            r = requests.get(url, headers=headers)\n",
        "            # --- Rate limit exceeded ---\n",
        "            if r.status_code == 403 and 'X-RateLimit-Remaining' in r.headers and r.headers['X-RateLimit-Remaining'] == '0':\n",
        "                reset_time = int(r.headers.get('X-RateLimit-Reset', time.time()+60))\n",
        "                sleep_for = max(reset_time - time.time(), 30)\n",
        "                print(f\"[RATE-LIMIT] Sleeping {sleep_for/60:.1f} min until reset …\")\n",
        "                time.sleep(sleep_for + 1)\n",
        "                current_token = next(token_cycle) # Switch to the next token\n",
        "                headers['Authorization'] = f'token {current_token}' # Update header with new token\n",
        "                continue\n",
        "\n",
        "            # --- Success / known cases ---\n",
        "            if r.status_code in (200, 204, 404):\n",
        "                return r\n",
        "            else:\n",
        "                print(f\"[WARN] {r.status_code} on {url}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"[ERROR] Request failed: {e}\")\n",
        "        # --- backoff ---\n",
        "        time.sleep(sleep_base * (2 ** attempt) + random.uniform(0, 1))\n",
        "    return None\n",
        "\n",
        "def handle_graphql_errors(data, context_info=None):\n",
        "    \"\"\"Handles GraphQL errors returned in the response body, including rate limits.\"\"\"\n",
        "    if 'errors' in data:\n",
        "        error_message = f\"GraphQL errors for {context_info or 'request'}: {data['errors']}\"\n",
        "        print(f\"[GRAPHQL ERROR] {error_message}\")\n",
        "        for error in data['errors']:\n",
        "            if error.get('type') == 'RATE_LIMITED':\n",
        "                 print(\"[RATE-LIMITED] Retrying after a delay...\")\n",
        "                 time.sleep(60) # Wait for a minute before retrying\n",
        "                 # Note: Token cycling for GraphQL is handled in the calling function (e.g., get_author_association)\n",
        "                 return 'RATE_LIMITED' # Indicate rate limit error\n",
        "        return 'ERROR' # Indicate other GraphQL error\n",
        "    return None # No GraphQL errors\n",
        "\n",
        "async def get_repo_data(repo_url: str):\n",
        "    # Make the Request using safe_get\n",
        "    print(f'Requesting: {repo_url}')\n",
        "    # Use safe_get which includes token cycling and rate limit handling\n",
        "    response = safe_get(repo_url)\n",
        "\n",
        "    if response is None:\n",
        "        print(f\"Error: Failed to retrieve data for {repo_url} after multiple retries.\")\n",
        "        return None\n",
        "\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def get_repo_created_at(repo_url: str):\n",
        "    try:\n",
        "        task = asyncio.create_task(get_repo_data(repo_url))\n",
        "        event_loop = asyncio.get_running_loop()\n",
        "        if event_loop.is_running():\n",
        "          data = event_loop.run_until_complete(task)\n",
        "        else:\n",
        "          data = asyncio.run(task)\n",
        "        if data is None:\n",
        "            return None\n",
        "\n",
        "        created_at = data['created_at']\n",
        "        print(f\"Repo: {repo_url}; Created At: {created_at}\")\n",
        "\n",
        "        if created_at:\n",
        "            return pd.to_datetime(created_at)\n",
        "        else:\n",
        "            raise Exception(f\"Error: Could not retrieve createdAt for {repo_url}. Response data: {data}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during GitHub API request for {repo_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GITHUB_TOKEN_2 does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1875095686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m GITHUB_TOKENS = [\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GITHUB_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GITHUB_TOKEN_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GITHUB_TOKEN_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GITHUB_TOKEN_4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GITHUB_TOKEN_2 does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Copy of Pull_Requests & Data Cleaning\n",
        "\n",
        "1. **Copies DataFrames:** It creates copies of the pull_request and repository DataFrames and assigns them to metrics and repos respectively. This is a good practice to avoid modifying the original loaded data.\n",
        "2. **Renames Columns:** It renames the 'id' column to 'pr_id' in the metrics DataFrame and to 'repo_id' in the repos DataFrame. This is done to prepare for merging these DataFrames later.\n",
        "3. **Filters Open Pull Requests:** It removes pull requests that are still open by filtering out rows where the 'closed_at' column has a missing value (NaN).\n",
        "4. **Converts Timestamps:** It converts the 'created_at' and 'closed_at' columns in the metrics DataFrame to datetime objects. This allows for easier time-based calculations.\n",
        "5. **Filters Repositories:** It removes repositories from the repos DataFrame that do not have any closed pull requests in the metrics DataFrame.\n",
        "6. **Gets Repository Creation Dates:** For the remaining repositories, it calls the get_repo_created_at function (defined in a previous cell) to fetch the creation date of each repository from the GitHub API and stores it in a new column 'repo_created_at'.\n",
        "7. **Filters Repositories with Creation Dates:** It removes repositories where the 'repo_created_at' could not be retrieved."
      ],
      "metadata": {
        "id": "67cBtszBeJC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, random, requests, pandas as pd\n",
        "from datetime import datetime\n",
        "from itertools import cycle # Import cycle\n",
        "\n",
        "# Copy raw data\n",
        "metrics = human_pull_request.copy()\n",
        "repos = repository.copy()\n",
        "\n",
        "# === BASIC CLEANUP ===\n",
        "metrics = metrics.rename(columns={'id': 'pr_id'})\n",
        "repos = repos.rename(columns={'id': 'repo_id'})\n",
        "\n",
        "print(f\"Total PRs before filtering: {len(metrics):,}\")\n",
        "metrics = metrics[metrics['closed_at'].notna()]\n",
        "print(f\"Closed PRs retained: {len(metrics):,}\")\n",
        "\n",
        "metrics['created_at'] = pd.to_datetime(metrics['created_at'], errors='coerce')\n",
        "metrics['closed_at'] = pd.to_datetime(metrics['closed_at'], errors='coerce')\n",
        "\n",
        "print(f\"Total repos before filtering: {len(repos):,}\")\n",
        "repos = repos[repos['repo_id'].isin(metrics['repo_id'])]\n",
        "print(f\"Repos with ≥1 PR: {len(repos):,}\")\n",
        "\n",
        "# === ADD: SMALL-SUBSET TEST MODE ===\n",
        "# Toggle to True for testing on limited data before full run\n",
        "TEST_MODE = True\n",
        "if TEST_MODE:\n",
        "    # Select the next 500 repos (from index 500 to 1000)\n",
        "    start_index = 500\n",
        "    end_index = 1000\n",
        "    if end_index > len(repos):\n",
        "        end_index = len(repos)\n",
        "        print(f\"[TEST MODE] Adjusting end index to {end_index} as it exceeds the number of repos.\")\n",
        "\n",
        "    test_repo_ids = repos['repo_id'].iloc[start_index:end_index]\n",
        "\n",
        "    metrics = metrics[metrics['repo_id'].isin(test_repo_ids)]\n",
        "    repos = repos[repos['repo_id'].isin(test_repo_ids)]\n",
        "    print(f\"[TEST MODE] Restricting to {len(repos)} repos and {len(metrics)} PRs (indices {start_index} to {end_index-1})\")\n",
        "\n",
        "\n",
        "# The safe_get function and token setup have been moved to the cell above\n",
        "# to ensure they are defined before being called."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "VKaF2xpBcvJd",
        "outputId": "f251d1c5-841e-43c4-dd7e-e95d64214a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total PRs before filtering: 6,618\n",
            "Closed PRs retained: 6,149\n",
            "Total repos before filtering: 116,211\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'repo_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'repo_id'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1585782110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total repos before filtering: {len(repos):,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repo_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repo_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Repos with ≥1 PR: {len(repos):,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'repo_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(metrics.head())\n",
        "display(repos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "collapsed": true,
        "id": "ODq_0xOtetTr",
        "outputId": "0b88bda0-217e-44ef-ee5c-3aeeb488cc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        pr_id  number                                              title  \\\n",
              "0  2336888723   85268              feat(aci): add automations index page   \n",
              "1  2447123365   89131  ref(insights): Make use of `<FeatureBadge>` fo...   \n",
              "2  2438086945   88748  :bug: fix: update how we fetch workflow_id and...   \n",
              "3  2265431531   83085         fix(org-stats): Require project membership   \n",
              "4  2332333882   85102  ref(consumers): Rename parallel -> batched-par...   \n",
              "\n",
              "            user   user_id   state                created_at  \\\n",
              "0      ameliahsu  55610339  closed 2025-02-14 19:04:59+00:00   \n",
              "1        ryan953    187460  closed 2025-04-08 23:29:50+00:00   \n",
              "2    iamrajjoshi  33237075  closed 2025-04-03 21:36:59+00:00   \n",
              "3    ArthurKnaus   7033940  closed 2025-01-08 07:47:13+00:00   \n",
              "4  evanpurkhiser   1421724  closed 2025-02-12 21:24:17+00:00   \n",
              "\n",
              "                  closed_at             merged_at  \\\n",
              "0 2025-02-18 22:42:20+00:00  2025-02-18T22:42:19Z   \n",
              "1 2025-04-09 15:56:55+00:00  2025-04-09T15:56:54Z   \n",
              "2 2025-04-04 15:10:57+00:00  2025-04-04T15:10:57Z   \n",
              "3 2025-01-08 08:49:40+00:00  2025-01-08T08:49:40Z   \n",
              "4 2025-02-12 22:20:33+00:00  2025-02-12T22:20:33Z   \n",
              "\n",
              "                                        repo_url  \\\n",
              "0  https://api.github.com/repos/getsentry/sentry   \n",
              "1  https://api.github.com/repos/getsentry/sentry   \n",
              "2  https://api.github.com/repos/getsentry/sentry   \n",
              "3  https://api.github.com/repos/getsentry/sentry   \n",
              "4  https://api.github.com/repos/getsentry/sentry   \n",
              "\n",
              "                                         html_url  \\\n",
              "0  https://github.com/getsentry/sentry/pull/85268   \n",
              "1  https://github.com/getsentry/sentry/pull/89131   \n",
              "2  https://github.com/getsentry/sentry/pull/88748   \n",
              "3  https://github.com/getsentry/sentry/pull/83085   \n",
              "4  https://github.com/getsentry/sentry/pull/85102   \n",
              "\n",
              "                                                body  agent  \n",
              "0  https://sentry-j41gpomr5.sentry.dev/automation...  Human  \n",
              "1  Using the premade component reduces an import ...  Human  \n",
              "2  i realized i made a mistake for how i fetch th...  Human  \n",
              "3  ### Problem\\r\\n\\r\\nIf the user is not member o...  Human  \n",
              "4  Both crons and uptime consumers have a paralle...  Human  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-555372b5-a7fb-4b66-b63b-c8a175a5dba0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr_id</th>\n",
              "      <th>number</th>\n",
              "      <th>title</th>\n",
              "      <th>user</th>\n",
              "      <th>user_id</th>\n",
              "      <th>state</th>\n",
              "      <th>created_at</th>\n",
              "      <th>closed_at</th>\n",
              "      <th>merged_at</th>\n",
              "      <th>repo_url</th>\n",
              "      <th>html_url</th>\n",
              "      <th>body</th>\n",
              "      <th>agent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2336888723</td>\n",
              "      <td>85268</td>\n",
              "      <td>feat(aci): add automations index page</td>\n",
              "      <td>ameliahsu</td>\n",
              "      <td>55610339</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-02-14 19:04:59+00:00</td>\n",
              "      <td>2025-02-18 22:42:20+00:00</td>\n",
              "      <td>2025-02-18T22:42:19Z</td>\n",
              "      <td>https://api.github.com/repos/getsentry/sentry</td>\n",
              "      <td>https://github.com/getsentry/sentry/pull/85268</td>\n",
              "      <td>https://sentry-j41gpomr5.sentry.dev/automation...</td>\n",
              "      <td>Human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2447123365</td>\n",
              "      <td>89131</td>\n",
              "      <td>ref(insights): Make use of `&lt;FeatureBadge&gt;` fo...</td>\n",
              "      <td>ryan953</td>\n",
              "      <td>187460</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-04-08 23:29:50+00:00</td>\n",
              "      <td>2025-04-09 15:56:55+00:00</td>\n",
              "      <td>2025-04-09T15:56:54Z</td>\n",
              "      <td>https://api.github.com/repos/getsentry/sentry</td>\n",
              "      <td>https://github.com/getsentry/sentry/pull/89131</td>\n",
              "      <td>Using the premade component reduces an import ...</td>\n",
              "      <td>Human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2438086945</td>\n",
              "      <td>88748</td>\n",
              "      <td>:bug: fix: update how we fetch workflow_id and...</td>\n",
              "      <td>iamrajjoshi</td>\n",
              "      <td>33237075</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-04-03 21:36:59+00:00</td>\n",
              "      <td>2025-04-04 15:10:57+00:00</td>\n",
              "      <td>2025-04-04T15:10:57Z</td>\n",
              "      <td>https://api.github.com/repos/getsentry/sentry</td>\n",
              "      <td>https://github.com/getsentry/sentry/pull/88748</td>\n",
              "      <td>i realized i made a mistake for how i fetch th...</td>\n",
              "      <td>Human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2265431531</td>\n",
              "      <td>83085</td>\n",
              "      <td>fix(org-stats): Require project membership</td>\n",
              "      <td>ArthurKnaus</td>\n",
              "      <td>7033940</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-01-08 07:47:13+00:00</td>\n",
              "      <td>2025-01-08 08:49:40+00:00</td>\n",
              "      <td>2025-01-08T08:49:40Z</td>\n",
              "      <td>https://api.github.com/repos/getsentry/sentry</td>\n",
              "      <td>https://github.com/getsentry/sentry/pull/83085</td>\n",
              "      <td>### Problem\\r\\n\\r\\nIf the user is not member o...</td>\n",
              "      <td>Human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2332333882</td>\n",
              "      <td>85102</td>\n",
              "      <td>ref(consumers): Rename parallel -&gt; batched-par...</td>\n",
              "      <td>evanpurkhiser</td>\n",
              "      <td>1421724</td>\n",
              "      <td>closed</td>\n",
              "      <td>2025-02-12 21:24:17+00:00</td>\n",
              "      <td>2025-02-12 22:20:33+00:00</td>\n",
              "      <td>2025-02-12T22:20:33Z</td>\n",
              "      <td>https://api.github.com/repos/getsentry/sentry</td>\n",
              "      <td>https://github.com/getsentry/sentry/pull/85102</td>\n",
              "      <td>Both crons and uptime consumers have a paralle...</td>\n",
              "      <td>Human</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-555372b5-a7fb-4b66-b63b-c8a175a5dba0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-555372b5-a7fb-4b66-b63b-c8a175a5dba0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-555372b5-a7fb-4b66-b63b-c8a175a5dba0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1d213e0a-bbf9-42dd-9c60-2f7a647761c4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d213e0a-bbf9-42dd-9c60-2f7a647761c4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1d213e0a-bbf9-42dd-9c60-2f7a647761c4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(repos\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"pr_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 77220258,\n        \"min\": 2265431531,\n        \"max\": 2447123365,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2447123365,\n          2332333882,\n          2438086945\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2590,\n        \"min\": 83085,\n        \"max\": 89131,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          89131,\n          85102,\n          88748\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ref(insights): Make use of `<FeatureBadge>` for new Insight page tabs\",\n          \"ref(consumers): Rename parallel -> batched-parallel\",\n          \":bug: fix: update how we fetch workflow_id and legacy_rule_id from ru\\u2026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ryan953\",\n          \"evanpurkhiser\",\n          \"iamrajjoshi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24227384,\n        \"min\": 187460,\n        \"max\": 55610339,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          187460,\n          1421724,\n          33237075\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"closed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-01-08 07:47:13+00:00\",\n        \"max\": \"2025-04-08 23:29:50+00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-04-08 23:29:50+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"closed_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-01-08 08:49:40+00:00\",\n        \"max\": \"2025-04-09 15:56:55+00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-04-09 15:56:55+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"merged_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-04-09T15:56:54Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"repo_url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://api.github.com/repos/getsentry/sentry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"html_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://github.com/getsentry/sentry/pull/89131\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Using the premade component reduces an import here, and gives us a tooltip for free!\\r\\nAlso we've now got consistent strings to translate, in this case it's lowercase `\\\"new\\\"`.\\r\\n\\r\\n![SCR-20250408-ombk](https://github.com/user-attachments/assets/10554630-7558-4150-80a0-31b9f2caa010)\\r\\n\\r\\n---\\r\\n\\r\\nI realized that the Session Health tab is only out for EA orgs right now, so the \\\"beta\\\" label and tooltip is probably more important. But I think \\\"new\\\" looks better with the \\\"Mobile Vitals (new)\\\" tab in there... and just seems like the better choice.\\r\\n\\r\\n![SCR-20250408-omxg](https://github.com/user-attachments/assets/762c680d-db03-4479-8178-b2aa40879df3)\\r\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Human\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      repo_id                                                url license  \\\n",
              "0   987641962  https://api.github.com/repos/1010-dev/senjudev...    None   \n",
              "1   990249393  https://api.github.com/repos/106-/HellSinkerWa...    None   \n",
              "2  1009549206  https://api.github.com/repos/1genadam/tileshop...    None   \n",
              "3   983546765  https://api.github.com/repos/1kimnet/ETL-pipeline    None   \n",
              "4  1024190983  https://api.github.com/repos/20m61/lightningta...    None   \n",
              "\n",
              "                    full_name    language  forks  stars  \n",
              "0      1010-dev/senjudev-site  TypeScript    1.0    0.0  \n",
              "1    106-/HellSinkerWallPaper        Java    0.0    0.0  \n",
              "2       1genadam/tileshop-rag      Python    0.0    0.0  \n",
              "3        1kimnet/ETL-pipeline      Python    0.0    0.0  \n",
              "4  20m61/lightningtalk-circle  JavaScript    0.0    0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71ba0d74-a911-43fc-a8e6-893cf1c4429b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo_id</th>\n",
              "      <th>url</th>\n",
              "      <th>license</th>\n",
              "      <th>full_name</th>\n",
              "      <th>language</th>\n",
              "      <th>forks</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>987641962</td>\n",
              "      <td>https://api.github.com/repos/1010-dev/senjudev...</td>\n",
              "      <td>None</td>\n",
              "      <td>1010-dev/senjudev-site</td>\n",
              "      <td>TypeScript</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>990249393</td>\n",
              "      <td>https://api.github.com/repos/106-/HellSinkerWa...</td>\n",
              "      <td>None</td>\n",
              "      <td>106-/HellSinkerWallPaper</td>\n",
              "      <td>Java</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1009549206</td>\n",
              "      <td>https://api.github.com/repos/1genadam/tileshop...</td>\n",
              "      <td>None</td>\n",
              "      <td>1genadam/tileshop-rag</td>\n",
              "      <td>Python</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>983546765</td>\n",
              "      <td>https://api.github.com/repos/1kimnet/ETL-pipeline</td>\n",
              "      <td>None</td>\n",
              "      <td>1kimnet/ETL-pipeline</td>\n",
              "      <td>Python</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1024190983</td>\n",
              "      <td>https://api.github.com/repos/20m61/lightningta...</td>\n",
              "      <td>None</td>\n",
              "      <td>20m61/lightningtalk-circle</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71ba0d74-a911-43fc-a8e6-893cf1c4429b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71ba0d74-a911-43fc-a8e6-893cf1c4429b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71ba0d74-a911-43fc-a8e6-893cf1c4429b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-853231bc-c765-42e1-9fe7-21740ad7e8f6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-853231bc-c765-42e1-9fe7-21740ad7e8f6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-853231bc-c765-42e1-9fe7-21740ad7e8f6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive to save Dataset for all of our access"
      ],
      "metadata": {
        "id": "jA6SGbhQZhLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save initial processed Dataset (no need to re-run this every time)"
      ],
      "metadata": {
        "id": "sVs4gjRhZ4zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.to_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_cleaned.pkl\")\n",
        "repos.to_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_cleaned.pkl\")"
      ],
      "metadata": {
        "id": "WOunph6PZrOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOR TESTING\n",
        "metrics.to_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_TEST.pkl\")\n",
        "repos.to_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_TEST.pkl\")"
      ],
      "metadata": {
        "id": "qy077Rz2drsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload dataset (this can be ran every time)"
      ],
      "metadata": {
        "id": "YUGl3g1aaApG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "metrics = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_cleaned.pkl\")\n",
        "repos = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_cleaned.pkl\")\n",
        "print(\"✅ Loaded shared cached cleaned dataset\")"
      ],
      "metadata": {
        "id": "KxAKR0tjaA7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOR TESTING\n",
        "import pandas as pd\n",
        "metrics = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/metrics_TEST.pkl\")\n",
        "repos = pd.read_pickle(\"/content/drive/MyDrive/AIDev_shared/repos_TEST.pkl\")\n",
        "print(\"✅ Loaded TEST shared cached cleaned dataset\")"
      ],
      "metadata": {
        "id": "Uz6YPZpJdyTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we decide to update this dataset again or for frequent updates-- use this to prevent overwriting"
      ],
      "metadata": {
        "id": "8VrCLkuWaX_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "tag = date.today().isoformat()\n",
        "metrics.to_pickle(f\"/content/drive/MyDrive/AIDev_shared/metrics_{tag}.pkl\")\n",
        "repos.to_pickle(f\"/content/drive/MyDrive/AIDev_shared/metrics_{tag}.pkl\")"
      ],
      "metadata": {
        "id": "hfQlplV8aXD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Third**, Gather the covariant variables\n",
        "\n",
        "## PR Variables\n",
        "\n",
        "1. **additions:** The # of added LOC by a PR\n",
        "2. **deletions:** The # of deleted LOC by a PR\n",
        "3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)\n",
        "4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', …).\n",
        "5. **changedFiles:** The # of files changed by a PR\n",
        "6. **commitsTotalCount:** The # of commits involved in a PR\n",
        "7. **bodyLength**: Length of the PR body (in characters).\n",
        "8. **prExperience:** The # of prior PRs that were submitted by the PR author (author’s prior PR count). Query the author’s PR history in the same repo and count PRs created before the current one.\n",
        "9. **commentsTotalCount:** The # of comments left on a PR\n",
        "10. **authorComments:** The # of comments left by the PR author\n",
        "11. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion\n",
        "12. **reviewersTotalCount:** The # of developers who participate in the discussion (excluding author).\n",
        "13. **repoAge:** Time interval between the repository creation time and PR creation time in days.\n",
        "14. **isMember:** Whether or not the author is a member or outside collaborator (True/False).\n",
        "15. **state**: State of the pull request (MERGED or CLOSED).\n",
        "16. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).\n",
        "\n",
        "## Project variables\n",
        "\n",
        "17. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue). *[I'm assuming its the top language as there is only one]*\n",
        "18. **forkCount:** The # of forks that a repository has\n",
        "19. **stargazerCount:** The # of stargazers that a repository has.\n",
        "\n",
        "## Treatment variables\n",
        "\n",
        "20. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)\n",
        "\n",
        "## Outcome variables\n",
        "\n",
        "21. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours\n",
        "22. **Is merged (state):** Whether or not a PR is merged (binary)\n",
        "\n"
      ],
      "metadata": {
        "id": "a30SFFhZXgy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PR Variables"
      ],
      "metadata": {
        "id": "xANcFKyCb9Fm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d51076"
      },
      "source": [
        "1. **additions:** The # of added LOC by a PR\n",
        "2. **deletions:** The # of deleted LOC by a PR\n",
        "3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "In the notebook (e.g., CollectCopilot4prs.ipynb), the `additions` and `deletions` values are extracted directly from the GitHub API response for each PR: `pr['node']['additions']` and `pr['node']['deletions']`. The GraphQL query for PRs includes the fields, so the value is as reported by GitHub. `prSize = additions + deletions`\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "In the `pr_commit_details` DataFrame, we use the `additions` and `deletions` fields. We sum them for `prSize`. Alternatively, the dataset also has`changes` which represents prSize but we chose to perform the sum ourselves.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['additions', 'deletions', 'prSize'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding LOC metrics: {len(metrics):,}\")\n",
        "\n",
        "# Get the sums of the columns we are interested in\n",
        "pr_commit_LOC = (pr_commit_details.groupby(['pr_id'])\n",
        "                                  .sum(['additions', 'deletions', 'changes'])\n",
        "                                  .reset_index())\n",
        "\n",
        "# Rename the sum columns to what we want\n",
        "pr_commit_LOC = (pr_commit_LOC.rename(columns={'changes': 'prSize'}))\n",
        "\n",
        "# Drop the extraneous columns\n",
        "pr_commit_LOC = pr_commit_LOC.drop(columns=['commit_stats_total', 'commit_stats_additions', 'commit_stats_deletions'])\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "metrics = pd.merge(metrics, pr_commit_LOC, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage collect the temporary Dataframe\n",
        "pr_commit_LOC = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['additions'] = metrics['additions'].fillna(0).astype(int)\n",
        "metrics['deletions'] = metrics['deletions'].fillna(0).astype(int)\n",
        "metrics['prSize'] = metrics['prSize'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding LOC metrics: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "nj9typbGIcI2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', …).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "In `CollectCopilot4prs.ipynb`, the code uses `np.select` with conditions based on the PR's title and body content to assign \"Bug\", \"Document\", or \"Feature\" as the purpose. This is a simple rule-based classification:\n",
        "\n",
        "- If the title/body contains keywords for bugs (e.g., \"fix\", \"bug\"), it's labeled \"Bug\".\n",
        "- If it contains documentation keywords (e.g., \"doc\"), it's labeled \"Document\".\n",
        "- Otherwise, it's labeled \"Feature\".\n",
        "\n",
        "\n",
        "**Our approach:**\n",
        "\n",
        "The `title` and `body` columns are part of the initial dataset that was loaded into the pull_request (`all_pull_request.parquet`) DataFrame.\n"
      ],
      "metadata": {
        "id": "_9ULLRD1eVyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['purpose'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before calculating purpose: {len(metrics):,}\")\n",
        "\n",
        "# Combine title and body for keyword search, handling potential None values\n",
        "metrics['title_body'] = metrics['title'].fillna('') + ' ' + metrics['body'].fillna('')\n",
        "\n",
        "# Define conditions and choices for np.select\n",
        "conditions = [\n",
        "    metrics['title_body'].str.contains('fix|bug', case=False, na=False),\n",
        "    metrics['title_body'].str.contains('doc', case=False, na=False)\n",
        "]\n",
        "choices = ['fix', 'doc']\n",
        "\n",
        "# Apply np.select to determine purpose\n",
        "metrics['purpose'] = np.select(conditions, choices, default='feat')\n",
        "\n",
        "# Drop the temporary combined column\n",
        "metrics = metrics.drop(columns=['title_body'])\n",
        "\n",
        "print(f\"Number of PRs after calculating purpose: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "Hc2sYaVwFPdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02810ce"
      },
      "source": [
        "5. **changedFiles:** The # of files changed by a PR\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "The `changedFiles` field is extracted directly from the GitHub API for each pull request. In the code (e.g., `in CollectCopilot4prs.ipynb`), it is accessed as: `pr['node']['changedFiles']`.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "This variable is calculated from the `pr_commit_details` DataFrame, steps include:\n",
        "\n",
        "- Identify the PR ID: The code uses `groupby(['pr_id', 'filename'])` which implicitly identifies each PR by its `pr_id`.\n",
        "- Locate the file-level change records: It operates on the `pr_commit_details` DataFrame, which contains the file-level change records.\n",
        "- Collect all rows belonging to the same PR: The `groupby(['pr_id', 'filename'])` operation groups all rows for a specific PR together.\n",
        "- Count the number of unique filenames for each `pr_id` across all its commits."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['changedFiles'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding changedFiles: {len(metrics):,}\")\n",
        "\n",
        "# Count the number of Files changed and change the column name to what we want\n",
        "pr_files_changed = (pr_commit_details.groupby(['pr_id', 'filename'])\n",
        "                                     .size()\n",
        "                                     .groupby(['pr_id'])\n",
        "                                     .size()\n",
        "                                     .reset_index(name='changedFiles'))\n",
        "\n",
        "# Merge the Dataframes with a left join\n",
        "metrics = pd.merge(metrics, pr_files_changed, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_files_changed = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['changedFiles'] = metrics['changedFiles'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding changedFiles: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "YwnUjWn9FPY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **commitsTotalCount:** The # of commits involved in a PR\n",
        "\n",
        "\n",
        "**Xiao, 2024:**\n",
        "\n",
        "Fetched from GitHub’s GraphQL API by querying the PR’s `commits { totalCount }` field.\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "The `pr_commit_details` table contains a `sha` column (the commit hash) and a `pr_id` column that links each commit to its pull request. Count every distinct `sha` in the entire table. Group by `pr_id` and count distinct `sha` values for each group.\n"
      ],
      "metadata": {
        "id": "xxZIoTx8fiCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['commitsTotalCount'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding commitsTotalCount: {len(metrics):,}\")\n",
        "\n",
        "# Calculate the number of unique commits for each Pull Request from pr_commit_details\n",
        "# Group by pr_id and count the number of unique sha values\n",
        "pr_commits_count = pr_commit_details.groupby('pr_id')['sha'].nunique().reset_index(name='commitsTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_commits_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_commits_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['commitsTotalCount'] = metrics['commitsTotalCount'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding commitsTotalCount: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "vOyzUxnHFPUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **bodyLength:** The length of a PR description\n",
        "\n",
        "**Xiao, 2024:**\n",
        "\n",
        "Query each PR with `pullRequest { body }`. Take the returned text and compute its character count (e.g., len(body)). Record that count as the value of description length.\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "In the `pull_request` DataFrame, calculate the character length of the `body` column.\n"
      ],
      "metadata": {
        "id": "Ed6fS14afxfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['bodyLength'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding bodyLength: {len(metrics):,}\")\n",
        "\n",
        "# Get the Length of the Body of the Pull Request\n",
        "metrics['bodyLength'] = metrics['body'].str.len()\n",
        "\n",
        "print(f\"Number of PRs after adding bodyLength: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "OZYyz-mVFPPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **prExperience:** The # of prior PRs that were submitted by the PR author (author’s prior PR count). Query the author’s PR history in the same repo and count PRs created before the current one.\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "For every pull request the study queries GitHub’s GraphQL API and extracts the author.login (or author.id) and the repository identifier (repository.id). Using the same API they request all pull requests belonging to the same repository.id whose author.login matches the author of the target PR. Each of these PRs includes its createdAt timestamp. The list is filtered to keep only those PRs whose createdAt value is earlier than the createdAt timestamp of the target PR. The number of remaining PRs is taken as an integer count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "- Extract the author's login from the `user` column.\n",
        "- Sorts the metrics DataFrame by `repo_id`, `author_login`, and the PR creation time (`created_at`).\n",
        "- Groups the sorted DataFrame by both `repo_id` and `author_login`.\n",
        "- Within each group (for each unique author in each unique repository), it uses the `.cumcount()` method. `cumcount()` assigns a sequential number starting from 0 to each row within the group based on the current order (which is sorted by `created_at`).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JQyVdlFCf2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['prExperience'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding prExperience: {len(metrics):,}\")\n",
        "\n",
        "# Extract the author's login from the 'user' column and store it in a new column 'author_login'\n",
        "metrics['author_login'] = metrics['user'].astype(str).str.strip()\n",
        "\n",
        "# Drop rows where 'repo_id' or 'created_at' are missing, as these are needed for sorting and calculation\n",
        "metrics = metrics.dropna(subset=['repo_id', 'created_at'])\n",
        "\n",
        "# Sort the DataFrame by 'repo_id', 'author_login', and 'created_at' in ascending order--This is crucial for correctly calculating the cumulative count of PRs for each author within each repository.\n",
        "metrics = metrics.sort_values(['repo_id', 'author_login', 'created_at'])\n",
        "\n",
        "# Calculate the cumulative count of PRs for each author within each repository.\n",
        "# The `groupby(['repo_id', 'author_login'])` groups the DataFrame by repository and author.\n",
        "# The `cumcount()` method then calculates the number of previous PRs for each row within those groups.\n",
        "metrics['prExperience'] = (\n",
        "    metrics.groupby(['repo_id', 'author_login'])\n",
        "           .cumcount()\n",
        "           .astype('Int64')\n",
        ")\n",
        "\n",
        "print(f\"Number of PRs after adding bodyLength: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "iS8q-30-icdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **commentsTotalCount:** The # of comments left on a PR\n",
        "\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "`commentsTotalCount` is obtained directly from the GitHub GraphQL API. For each pull request it queries the PR’s `comments` connection and records the `totalCount` field, which is the number of comment objects attached to that PR (including review comments, issue‑style comments, and any other discussion entries).\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Group the `pr_comments` DataFrame by `pr_id` (Pull Request ID). Each row in `pr_comments` represents a single comment.\n",
        "Count the number of rows within each group using `.size()`.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ac9ifXkVgLqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics.drop(columns=['commentsTotalCount'], errors='ignore', inplace=True)\n",
        "\n",
        "print(f\"Number of PRs before adding commentsTotalCount: {len(metrics):,}\")\n",
        "\n",
        "# Count the number of Comments for the Pull Request, name the column what we want.\n",
        "pr_comments_count = pr_comments.groupby(['pr_id']).size().reset_index(name='commentsTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "pr_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['commentsTotalCount'] = metrics['commentsTotalCount'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding commentsTotalCount: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "2BNmsnGIFO9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **authorComments:** The # of comments left by the PR author\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "Query each PR’s comment data through the GitHub GraphQL API and then filter the comment list to keep only those whose `author.login` matches the PR author’s login.\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Merges `pr_comments` with author information and counting comments where the comment author matches the PR author."
      ],
      "metadata": {
        "id": "LoBAgwxOgaJn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "322b4128"
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['authorComments'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding authorComments: {len(metrics):,}\")\n",
        "\n",
        "# Filter comments to only include those made by the PR author\n",
        "# Need to merge with metrics to get the author_id for each pr_comment\n",
        "author_comments = pd.merge(pr_comments, metrics[['pr_id', 'user_id']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "author_comments = author_comments[author_comments['user_id_x'] == author_comments['user_id_y']]\n",
        "\n",
        "# Count the number of author comments per pull request\n",
        "author_comments_count = author_comments.groupby(['pr_id']).size().reset_index(name='authorComments')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, author_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "author_comments = None\n",
        "author_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['authorComments'] = metrics['authorComments'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding authorComments: {len(metrics):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion\n",
        "\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "Querying the `comments` edge of each PR via GraphQL (or the REST endpoint `GET /repos/{owner}/{repo}/issues/{pull_number}/comments`). Filtering out comments whose user.login equals the PR author’s login. Counting the remaining comments – that number is the reviewerComments value.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Merges `pr_comments` with author information and counting comments where the comment author does not match the PR author.\n",
        "\n"
      ],
      "metadata": {
        "id": "0cC_Qh-DgvEJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "454479f7"
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewersComments'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding reviewersComments: {len(metrics):,}\")\n",
        "\n",
        "# Filter comments to exclude those made by the PR author\n",
        "# Need to merge with metrics to get the author_id for each pr_comment\n",
        "reviewer_comments = pd.merge(pr_comments, metrics[['pr_id', 'user_id']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "reviewer_comments = reviewer_comments[reviewer_comments['user_id_x'] != reviewer_comments['user_id_y']]\n",
        "\n",
        "# Count the number of reviewer comments per pull request\n",
        "reviewer_comments_count = reviewer_comments.groupby(['pr_id']).size().reset_index(name='reviewersComments')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, reviewer_comments_count, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "reviewer_comments = None\n",
        "reviewer_comments_count = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['reviewersComments'] = metrics['reviewersComments'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding reviewersComments: {len(metrics):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **reviewersTotalCount:** The # of developers who participate in the discussion (excluding author).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "Calling the GraphQL endpoint for each PR,\n",
        "Requesting the `reviewers` (or `reviewRequests`) connection,\n",
        "Reading the `totalCount` field,\n",
        "Verifying that the author’s login is excluded (or simply using the `totalCount` as GitHub already excludes the author in the `reviewers` connection).\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Calculates `reviewersTotalCount` by summing the counts of unique reviewers from reviews (`reviewers`) for each pull request.\n",
        "\n"
      ],
      "metadata": {
        "id": "sDtcHmxxg9MP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b294946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "0835dc3f-6027-40a4-ed58-f7c518c029c6"
      },
      "source": [
        "# Ensure we don't crash if columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewersTotalCount'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding reviewersTotalCount: {len(metrics):,}\")\n",
        "\n",
        "# Create a copy of PR reviews with only the necessary columns\n",
        "pr_reviews_temp = pr_reviews.copy().drop(columns=['id', 'state', 'submitted_at', 'body', 'user_type'], errors='ignore')\n",
        "\n",
        "# Filter reviews to exclude those made by the PR author\n",
        "# Merge with metrics to get the author_id for each pr_comment\n",
        "pr_reviews_temp = pd.merge(pr_reviews_temp, metrics[['pr_id', 'user']], left_on='pr_id', right_on='pr_id', how='left')\n",
        "pr_reviews_temp = pr_reviews_temp[pr_reviews_temp['user_x'] != pr_reviews_temp['user_y']]\n",
        "\n",
        "# Count the number of unique non-author users who left review comments per pull request\n",
        "# Group by 'pr_id' to count per pull request\n",
        "pr_reviews_temp = pr_reviews_temp.groupby(['pr_id'])['user_x'].nunique().reset_index(name='reviewersTotalCount')\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, pr_reviews_temp, left_on='pr_id', right_on='pr_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframes\n",
        "pr_reviews_temp = None\n",
        "\n",
        "# Fill N/A values with defaults (for PRs with no reviews/comments)\n",
        "metrics['reviewersTotalCount'] = metrics['reviewersTotalCount'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding reviewersTotalCount: {len(metrics):,}\")\n",
        "\n",
        "\n",
        "# Display unique values and their counts for 'reviewersTotalCount'\n",
        "print(\"\\nUnique values and counts for 'reviewersTotalCount':\")\n",
        "display(metrics['reviewersTotalCount'].value_counts(dropna=False).sort_index())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PRs before adding reviewersTotalCount: 3,239\n",
            "Number of PRs after adding reviewersTotalCount: 3,239\n",
            "\n",
            "Unique values and counts for 'reviewersTotalCount':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "reviewersTotalCount\n",
              "0    3002\n",
              "1     111\n",
              "2      61\n",
              "3      57\n",
              "4       8\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewersTotalCount</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **repoAge:** Time interval between the repository creation time and PR creation time in days.\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "- Retrieve the creation timestamp of the repository (the time the repo was first created on GitHub).\n",
        "- Retrieve the creation timestamp of the pull request under study.\n",
        "- Compute the time interval between these two timestamps in days\n",
        "\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "Merging `pr_review_comments` and `pr_reviews` DataFrames with our `metrics` to link comments/reviews to PR authors.\n",
        "Filtering these merged DataFrames to exclude rows where the comment/review user_id matches the PR author's user_id.\n",
        "Grouping the filtered data by `pr_id`.\n",
        "Calculating the count of distinct user_id values (`.nunique()`) within each `pr_id` group for both filtered DataFrames.\n",
        "Merging these unique counts and summing them per `pr_id` to get the total unique non-author participants across review comments and reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bvwr_F_0h3p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['repoAge'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding repoAge: {len(metrics):,}\")\n",
        "\n",
        "# Copy the Repository dataframe\n",
        "repos_temp = repos.copy()\n",
        "\n",
        "# Calculate repo_created_at for the repos_temp DataFrame\n",
        "repos_temp['repo_created_at'] = repos_temp.apply(lambda row: get_repo_created_at(row['url']), axis=1)\n",
        "\n",
        "# Drop repos where repo_created_at could not be retrieved\n",
        "repos_temp = repos_temp.dropna(subset=['repo_created_at'])\n",
        "\n",
        "\n",
        "# Merge the Dataframes using a left join to bring repo_created_at into metrics\n",
        "# Ensure we only merge the necessary columns to avoid duplicates like repo_created_at_x\n",
        "metrics = pd.merge(metrics, repos_temp[['repo_id', 'repo_created_at']], on='repo_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary dataframe\n",
        "repos_temp = None\n",
        "\n",
        "# Drop from Metrics any Repo without a repo created date after the merge\n",
        "# This might be redundant if merged_at is already checked, but good for safety\n",
        "metrics = metrics.dropna(subset=['repo_created_at'])\n",
        "\n",
        "# Calculate the Repo Age in Days (created_at - repo_created_at)\n",
        "# Ensure 'created_at' and 'repo_created_at' are in datetime format before calculation\n",
        "metrics['created_at'] = pd.to_datetime(metrics['created_at'], errors='coerce')\n",
        "metrics['repo_created_at'] = pd.to_datetime(metrics['repo_created_at'], errors='coerce')\n",
        "\n",
        "metrics = metrics.assign(repoAge=lambda x: (x['created_at'] - x['repo_created_at']).dt.days)\n",
        "\n",
        "# Drop the unnecessary Repo Created At column after calculating repoAge\n",
        "metrics = metrics.drop(columns=['repo_created_at'], errors='ignore')\n",
        "\n",
        "# Fill N/A values for repoAge with defaults\n",
        "metrics['repoAge'] = metrics['repoAge'].fillna(0).astype(int)\n",
        "\n",
        "print(f\"Number of PRs after adding repoAge: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "ZBVOYKNaFOcN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. **isMember:** Whether or not the author is a member or outside collaborator (True/False).\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "- For each PR the study calls GitHub’s GraphQL API and retrieves the author’s association with the repository (the `authorAssociation` field).\n",
        "- If the returned association is `MEMBER` or `OWNER`, the flag is set to 1; otherwise (e.g., `CONTRIBUTOR`, `NONE`, or an external collaborator) it is set to 0.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Followed the same approach as Xiao 2024."
      ],
      "metadata": {
        "id": "IhSC6qwXf95V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b205b222",
        "collapsed": true
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['isMember'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding isMember: {len(metrics):,}\")\n",
        "\n",
        "def get_author_association(repo_owner: str, repo_name: str, pr_number: int) -> str:\n",
        "    \"\"\"Fetches the authorAssociation for a PR using GitHub GraphQL API, cycling through tokens.\"\"\"\n",
        "    # Use the token_cycle defined in cell 818abba5\n",
        "    current_token = next(token_cycle) # Get the next token in the cycle\n",
        "    query = \"\"\"\n",
        "    query($owner: String!, $repo: String!, $pr: Int!) {\n",
        "      repository(owner: $owner, name: $repo) {\n",
        "        pullRequest(number: $pr) {\n",
        "          authorAssociation\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        "    variables = {\n",
        "        \"owner\": repo_owner,\n",
        "        \"repo\": repo_name,\n",
        "        \"pr\": pr_number\n",
        "    }\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {current_token}\", # Use the current token\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Implement basic rate limit handling and retry\n",
        "    for attempt in range(3): # Retry up to 3 times\n",
        "        try:\n",
        "            response = requests.post(\"https://api.github.com/graphql\", json={'query': query, 'variables': variables}, headers=headers)\n",
        "            response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "\n",
        "            data = response.json()\n",
        "\n",
        "            # Handle GraphQL errors using the centralized function\n",
        "            error_status = handle_graphql_errors(data, context_info=f\"PR {pr_number} in {repo_owner}/{repo_name}\")\n",
        "\n",
        "            if error_status == 'RATE_LIMITED':\n",
        "                # Switch to the next token before retrying due to rate limit\n",
        "                current_token = next(token_cycle)\n",
        "                headers['Authorization'] = f'Bearer {current_token}'\n",
        "                continue # Continue to the next retry attempt\n",
        "\n",
        "            if error_status == 'ERROR':\n",
        "                 # For other GraphQL errors, break the retry loop\n",
        "                 break\n",
        "\n",
        "            # If no GraphQL errors and no HTTP errors, extract the data\n",
        "            association = data['data']['repository']['pullRequest']['authorAssociation']\n",
        "            return association\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request error for PR {pr_number} in {repo_owner}/{repo_name}: {e}\")\n",
        "            time.sleep(5 * (attempt + 1)) # Exponential backoff\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred for PR {pr_number} in {repo_owner}/{repo_name}: {e}\")\n",
        "            break # Don't retry for unexpected errors\n",
        "\n",
        "    return None # Return None if all retries fail\n",
        "\n",
        "\n",
        "# Prepare data: Ensure 'repo_url' and 'number' are available in metrics\n",
        "# You might need to merge with the original pull_request DataFrame if these columns were dropped\n",
        "if 'repo_url' not in metrics.columns or 'number' not in metrics.columns:\n",
        "     print(\"Warning: 'repo_url' or 'number' not found in metrics. Merging with original pull_request data.\")\n",
        "     # Assuming original pull_request is available\n",
        "     metrics = pd.merge(metrics, pull_request[['id', 'repo_url', 'number']], left_on='pr_id', right_on='id', how='left', suffixes=('', '_original'))\n",
        "     metrics = metrics.drop(columns=['id_original'], errors='ignore') # Drop duplicate id column\n",
        "\n",
        "\n",
        "# Extract repo owner and name from repo_url\n",
        "def extract_repo_details(repo_url):\n",
        "    try:\n",
        "        parts = repo_url.split('/')\n",
        "        owner = parts[-2]\n",
        "        name = parts[-1]\n",
        "        return owner, name\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "metrics[['repo_owner', 'repo_name']] = metrics['repo_url'].apply(lambda url: pd.Series(extract_repo_details(url)))\n",
        "\n",
        "\n",
        "# Apply the function to fetch author association for each PR\n",
        "# This can be slow due to API calls. Consider applying to a subset for testing.\n",
        "# Ensure TEST_MODE is handled if you want to limit API calls\n",
        "if 'TEST_MODE' in globals() and TEST_MODE:\n",
        "    print(\"Running in TEST_MODE, applying to a subset.\")\n",
        "    # Apply to the current subset of metrics\n",
        "    metrics['authorAssociation'] = metrics.apply(\n",
        "        lambda row: get_author_association(row['repo_owner'], row['repo_name'], row['number']),\n",
        "        axis=1\n",
        "    )\n",
        "else:\n",
        "     print(\"Running on the full dataset.\")\n",
        "     metrics['authorAssociation'] = metrics.apply(\n",
        "        lambda row: get_author_association(row['repo_owner'], row['repo_name'], row['number']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "\n",
        "# Map authorAssociation to isMember (1 for MEMBER/OWNER, 0 otherwise)\n",
        "metrics['isMember'] = metrics['authorAssociation'].apply(\n",
        "    lambda x: 1 if x in ['MEMBER', 'OWNER'] else (0 if x is not None else None) # Handle None from API errors\n",
        ")\n",
        "\n",
        "# Drop temporary columns used for API calls if they are not needed for further analysis\n",
        "metrics = metrics.drop(columns=['repo_owner', 'repo_name', 'authorAssociation'], errors='ignore')\n",
        "\n",
        "\n",
        "print(f\"Number of PRs after adding isMember: {len(metrics):,}\")\n",
        "\n",
        "print(\"✅ Calculated isMember using GraphQL API.\")\n",
        "display(metrics[['isMember']].head())\n",
        "display(metrics['isMember'].value_counts(dropna=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GraphQL Errors Example for isMember**\n",
        "\n",
        "GraphQL errors for PR ...: [{'type': 'NOT_FOUND', ... 'message': \"Could not resolve to a Repository with the name 'owner/repo'.\"}]: These are error messages from the GitHub GraphQL API. The NOT_FOUND type means that the API could not find the specific resource you were requesting. In these cases, the message \"Could not resolve to a Repository with the name 'owner/repo'\" indicates that the repository specified in the API call (e.g., 'vals-ai/vals-sdk', 'ImSingingInTheRain/gpai-assessment') could not be found on GitHub. This could be because the repository was renamed, deleted, or is private and your token doesn't have access.\n",
        "\n",
        "\n",
        "GraphQL errors for PR ...: [{'type': 'NOT_FOUND', ... 'message': 'Could not resolve to a PullRequest with the number of XX.'}]: Similarly, this error indicates that the specific pull request number within the given repository could not be found. This might happen if the PR was closed and potentially deleted, or the number is incorrect for that repository."
      ],
      "metadata": {
        "id": "LPoWAdik39GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. **state**: State of the pull request (MERGED or CLOSED).\n",
        "\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "In GitHub GraphQL (or REST) API, the response includes a field called state (or, equivalently, a Boolean merged flag). The value of that field can be one of three mutually‑exclusive statuses:\n",
        "- MERGED (or merged = true)\tThe PR was successfully merged into the target branch.\n",
        "- CLOSED (or merged = false & state = CLOSED)\tThe PR was closed without being merged.\n",
        "- OPEN (or state = OPEN)\tThe PR was still open at the time the data were collected.\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "If the `merged_at` column for a pull request has a value (i.e., it's not `null`), it means the pull request was merged, and the state is set to `MERGED`.\n",
        "If the `merged_at` column is `null`, it means the pull request was closed without being merged, and the state is set to `CLOSED`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qNQdfoY8iNBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['state'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding state: {len(metrics):,}\")\n",
        "\n",
        "# Set the State to MERGED or CLOSED\n",
        "metrics['state'] = metrics['merged_at'].apply(lambda x: 'MERGED' if x is not None else 'CLOSED')\n",
        "\n",
        "print(f\"Number of PRs after adding state: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "5Ftmm7qZlHbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "`reviewtime` (in hours) = (PR Closed Timestamp - PR Creation Timestamp).\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "The difference between the `closed_at` and `created_at` timestamps and converting that duration into hours.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VzgXfZtoCjPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['reviewTime'], errors='ignore')\n",
        "\n",
        "print(f\"Number of PRs before adding reviewTime: {len(metrics):,}\")\n",
        "\n",
        "# Calculate the Review Time\n",
        "metrics['reviewTime'] = (metrics['closed_at'] - metrics['created_at']).dt.total_seconds() / 3600\n",
        "\n",
        "print(f\"Number of PRs after adding reviewTime: {len(metrics):,}\")"
      ],
      "metadata": {
        "id": "UM0zn7ZtCmdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project variables"
      ],
      "metadata": {
        "id": "1unf8guqcK2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue).\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "The number of stargazers that a repository has\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "Same approach\n",
        "\n",
        "\n",
        "18. **forkCount:** The # of forks that a repository has\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "19. **stargazerCount:** The # of stargazers that a repository has.\n",
        "\n",
        "**Xiao 2024:**\n",
        "\n",
        "\n",
        "**Our Approach:**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5lSM-5VimJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['repoLanguage', 'forkCount', 'stargazerCount'], errors='ignore')\n",
        "\n",
        "repos_temp = (repos.copy()\n",
        "                   .drop(columns=['license', 'repo_url', 'html_url', 'full_name'], errors='ignore')\n",
        "                   .rename(columns={'language': 'repoLanguage', 'forks': 'forkCount', 'stars': 'stargazerCount'}))\n",
        "\n",
        "# Group by ID and get the First Record\n",
        "repos_temp = repos_temp.groupby(['repo_id']).first().reset_index() # Add reset_index() to make repo_id a column again\n",
        "\n",
        "# Merge the Dataframes using a left join\n",
        "metrics = pd.merge(metrics, repos_temp, left_on='repo_id', right_on='repo_id', how='left')\n",
        "\n",
        "# Garbage Collect the temporary Dataframe\n",
        "repos_temp = None\n",
        "\n",
        "# Fill N/A values with defaults\n",
        "metrics['repoLanguage'] = metrics['repoLanguage'].fillna('other')\n",
        "metrics['forkCount'] = metrics['forkCount'].fillna(0).astype(int)\n",
        "metrics['stargazerCount'] = metrics['stargazerCount'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "hEoTBBXjFOVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treatment variables"
      ],
      "metadata": {
        "id": "RV-ykVvwcSGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)\n"
      ],
      "metadata": {
        "id": "zUDj9SFMjShm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics = metrics.drop(columns=['isGeneratedByCopliot'], errors='ignore')\n",
        "\n",
        "metrics['isGeneratedByCopilot'] = metrics['agent'].apply(lambda x: True if x == 'Copilot' else False) # Corrected column name and capitalization"
      ],
      "metadata": {
        "id": "btn7vVCnjSTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outcome variables"
      ],
      "metadata": {
        "id": "iRzFbe-Acwbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours\n"
      ],
      "metadata": {
        "id": "0Nzqcx6Yi6Uo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c5c683",
        "collapsed": true
      },
      "source": [
        "# Make sure we don't crash because the columns already exist (rentrant code)\n",
        "metrics = metrics.drop(columns=['reviewTime'], errors='ignore')\n",
        "\n",
        "# Calculate review time in hours, handling potential NaT values\n",
        "metrics = metrics.assign(reviewTime=lambda x: (x['closed_at'] - x['created_at']).dt.total_seconds() / 3600)\n",
        "\n",
        "# Fill N/A values with defaults (e.g., for open PRs)\n",
        "metrics['reviewTime'] = metrics['reviewTime'].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. **Is merged (state):** Whether or not a PR is merged (binary)\n"
      ],
      "metadata": {
        "id": "-QSkr74njEzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't crash because the columns already exist (reentrant code)\n",
        "metrics = metrics.drop(columns=['isMerged'], errors='ignore')\n",
        "\n",
        "# If Merged_At is None, the PR was not merged, otherwise it was\n",
        "metrics['isMerged'] = metrics['merged_at'].apply(lambda x: 0 if x is None else 1)"
      ],
      "metadata": {
        "id": "fvl0rnKHbOwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Order in CSV (treatment_metrics.csv and control_metrics.csc)\n",
        "\n",
        "1. **repoLanguage**\n",
        "2. **forkCount**\n",
        "3. **stargazerCount**\n",
        "4. **repoAge**\n",
        "5. **state**\n",
        "6. **deletions**\n",
        "7. **additions**\n",
        "8. **changedFiles**\n",
        "9. **commentsTotalCount**\n",
        "10. **commitsTotalCount**\n",
        "11. **prExperience**\n",
        "12. **isMember**\n",
        "13. **authorComments**\n",
        "14. **reviewersComments**\n",
        "15. **reviewersTotalCount**\n",
        "16. **bodyLength**\n",
        "17. **prSize**\n",
        "18. **reviewTime**\n",
        "19. **purpose**\n"
      ],
      "metadata": {
        "id": "0-kgS2O2dL6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export to CSV"
      ],
      "metadata": {
        "id": "3z7NwYwGxxiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the Google Drive folder\n",
        "drive_path = \"/content/drive/MyDrive/AIDev_shared/\"\n",
        "\n",
        "# Ensure the directory exists (optional, but good practice)\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Define the desired order of columns for the output CSVs\n",
        "csv_order = ['repoLanguage',\n",
        "'forkCount',\n",
        "'stargazerCount',\n",
        "'repoAge',\n",
        "'state',\n",
        "'deletions',\n",
        "'additions',\n",
        "'changedFiles',\n",
        "'commentsTotalCount',\n",
        "'commitsTotalCount',\n",
        "'prExperience',\n",
        "'isMember',\n",
        "'authorComments',\n",
        "'reviewersComments',\n",
        "'reviewersTotalCount',\n",
        "'bodyLength',\n",
        "'prSize',\n",
        "'reviewTime',\n",
        "'purpose']\n",
        "\n",
        "# Split the metrics DataFrame into treatment (Copilot) and control (non-Copilot) based on the 'isGeneratedByCopilot' column\n",
        "try:\n",
        "    # Use the isGeneratedByCopilot column for splitting\n",
        "    treatment_metrics_full = metrics[metrics['isGeneratedByCopilot'] == True].copy()\n",
        "    control_metrics_full = metrics[metrics['isGeneratedByCopilot'] == False].copy()\n",
        "except KeyError:\n",
        "    print(\"Error: 'isGeneratedByCopilot' column not found in metrics DataFrame. Please ensure it's included in previous steps.\")\n",
        "    raise\n",
        "\n",
        "# Select and reorder columns for the treatment and control DataFrames\n",
        "treatment_metrics = treatment_metrics_full[csv_order].copy()\n",
        "control_metrics = control_metrics_full[csv_order].copy()\n",
        "\n",
        "# Now, drop rows with NaN values from the column-filtered DataFrames\n",
        "print(f\"Number of treatment PRs before dropping NaNs: {len(treatment_metrics):,}\")\n",
        "treatment_metrics.dropna(inplace=True)\n",
        "print(f\"Number of treatment PRs after dropping NaNs: {len(treatment_metrics):,}\")\n",
        "\n",
        "print(f\"Number of control PRs before dropping NaNs: {len(control_metrics):,}\")\n",
        "control_metrics.dropna(inplace=True)\n",
        "print(f\"Number of control PRs after dropping NaNs: {len(control_metrics):,}\")\n",
        "\n",
        "\n",
        "# Define the file paths\n",
        "treatment_file_path = os.path.join(drive_path, \"treatment_metrics.csv\")\n",
        "control_file_path = os.path.join(drive_path, \"control_metrics.csv\")\n",
        "\n",
        "# Export to CSV\n",
        "treatment_metrics.to_csv(treatment_file_path, index=False)\n",
        "control_metrics.to_csv(control_file_path, index=False)\n",
        "\n",
        "print(f\"✅ Exported treatment metrics to: {treatment_file_path}\")\n",
        "print(f\"✅ Exported control metrics to: {control_file_path}\")"
      ],
      "metadata": {
        "id": "ov2O_pNGwy-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.head()"
      ],
      "metadata": {
        "id": "lSJa206uCknk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fourth**, Bot detection and filtering employed the methodology of Golzadeh et al. (2022)\n",
        "\n",
        "simple “bot” username suffix check with a comprehensive, manually verified list of 527 bot accounts\n",
        "* groundtruthbots.csv - a list of bots from Golzadeh et al.\n"
      ],
      "metadata": {
        "id": "V8vXCVjGW1aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the Google Drive folder\n",
        "drive_path = \"/content/drive/MyDrive/AIDev_shared/\"\n",
        "\n",
        "# Define the file path for the ground truth bots list (now a CSV)\n",
        "bots_file_path = os.path.join(drive_path, \"groundtruthbots.csv\")\n",
        "\n",
        "# Load the ground truth bots list from CSV\n",
        "try:\n",
        "    # Assuming the CSV has a header and the usernames are in the first column\n",
        "    groundtruth_bots_df = pd.read_csv(bots_file_path)\n",
        "    # Extract the first column and convert to a list, handling potential NaNs\n",
        "    groundtruth_bots = groundtruth_bots_df.iloc[:, 0].dropna().astype(str).tolist()\n",
        "    print(f\"Loaded {len(groundtruth_bots)} bot usernames from {bots_file_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {bots_file_path} not found. Please ensure the file exists and contains bot usernames in the first column.\")\n",
        "    groundtruth_bots = [] # Initialize as empty list to avoid errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading {bots_file_path}: {e}\")\n",
        "    groundtruth_bots = [] # Initialize as empty list\n",
        "\n",
        "\n",
        "# --- Filter bot-submitted PRs from metrics DataFrame ---\n",
        "\n",
        "print(f\"Number of PRs before bot filtering: {len(metrics):,}\")\n",
        "\n",
        "# Filter out PRs where the author's login ends with 'bot' or is in the ground truth list\n",
        "# Ensure 'author_login' column exists and is treated as string\n",
        "if 'author_login' not in metrics.columns:\n",
        "    print(\"Warning: 'author_login' column not found in metrics. Skipping PR bot filtering.\")\n",
        "    # You might want to add a step to create 'author_login' here if it's missing\n",
        "    # For now, we'll skip this filtering step to avoid crashing\n",
        "    metrics_filtered = metrics.copy()\n",
        "else:\n",
        "    metrics_filtered = metrics[\n",
        "        (~metrics['author_login'].astype(str).str.lower().str.endswith('bot', na=False)) &\n",
        "        (~metrics['author_login'].astype(str).isin(groundtruth_bots))\n",
        "    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "print(f\"Number of PRs after bot filtering: {len(metrics_filtered):,}\")\n",
        "\n",
        "\n",
        "# --- Filter bot comments from pr_comments DataFrame ---\n",
        "\n",
        "print(f\"Number of comments before bot filtering: {len(pr_comments):,}\")\n",
        "\n",
        "# Filter out comments where the user's login ends with 'bot' or is in the ground truth list\n",
        "# Ensure 'user' column exists and is treated as string\n",
        "if 'user' not in pr_comments.columns:\n",
        "     print(\"Warning: 'user' column not found in pr_comments. Skipping comment bot filtering.\")\n",
        "     # You might want to add a step to create 'user' here if it's missing\n",
        "     # For now, we'll skip this filtering step to avoid crashing\n",
        "     pr_comments_filtered = pr_comments.copy()\n",
        "else:\n",
        "    pr_comments_filtered = pr_comments[\n",
        "        (~pr_comments['user'].astype(str).str.lower().str.endswith('bot', na=False)) &\n",
        "        (~pr_comments['user'].astype(str).isin(groundtruth_bots))\n",
        "    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "print(f\"Number of comments after bot filtering: {len(pr_comments_filtered):,}\")\n",
        "\n",
        "# You can now replace the original metrics and pr_comments DataFrames with the filtered ones\n",
        "metrics = metrics_filtered\n",
        "pr_comments = pr_comments_filtered\n",
        "\n",
        "print(\"Bot filtering applied to metrics and pr_comments DataFrames.\")"
      ],
      "metadata": {
        "id": "AqXYcUmgxz1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fifth**, Adoption Trend (RQ1)\n",
        "(RQ1) To what extent do developers use Copilot for PRs in the code review process?\n",
        "\n",
        "* Counted occurrences of each marker tag; copilot:summary was the most frequent (13 231 instances).\n",
        "* Visualised cumulative PRs over time (Fig. 3) and proportion of PRs per repository (Fig. 4).\n"
      ],
      "metadata": {
        "id": "g4l6_AqBzYVI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CNLP973ZzYBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sixth**, Causal Inference (RQ2)\n",
        "\n",
        "### Propensity‑Score Estimation\n",
        "Logistic regression (treatment = Copilot usage) on the 17 covariates.\n",
        "Estimated each PR’s probability of receiving the treatment (ps).\n",
        "### Weight Construction\n",
        "Inverse‑probability weights: 1/ps for treated, 1/(1‑ps) for control.\n",
        "### Entropy Balancing\n",
        "Applied the entropy‑balancing algorithm (equivalent to R’s ebalance) to adjust the raw weights so that the weighted means of all covariates matched exactly between groups.\n",
        "After balancing, absolute mean differences for every covariate were ≤ 0.10 (Fig. 2).\n",
        "### Outcome Regression\n",
        "* Review time (continuous): weighted ordinary least squares (lm analogue) with only the treatment indicator. The coefficient gave the Average Treatment Effect on the Treated (ATT) of ‑19.3 h (p ≈ 1.6 × 10⁻¹⁷).\n",
        "* Merge outcome (binary): weighted logistic regression (glm with logit link). The exponentiated treatment coefficient yielded an odds ratio of 1.57 (95 % CI [1.35, 1.84], p < 0.001).\n",
        "These two models answer RQ2.1 (review‑time reduction) and RQ2.2 (higher merge likelihood).\n"
      ],
      "metadata": {
        "id": "48T4d5AezoVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The R Scripts\n",
        "The main difference between PMW_merge.R and PMW_review.R is:\n",
        "\n",
        "* PMW_merge.R includes the column isMerged, which indicates whether each pull request was merged (state == \"MERGED\"). This column is added to the modeling data and used in the analysis.\n",
        "* PMW_review.R does not include the isMerged column in its modeling data; it focuses only on review-related metrics.\n",
        "* Otherwise, both scripts process the same input data, use similar covariates, and prepare for causal inference analysis. The inclusion of isMerged in PMW_merge.R allows for analysis related to PR merge status, while PMW_review.R is focused on review characteristics."
      ],
      "metadata": {
        "id": "kNvRCCdDFmIf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GN6LEJdGbVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}